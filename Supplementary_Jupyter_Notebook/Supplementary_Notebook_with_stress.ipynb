{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supplementary Notebook for \"Three-Dimensional Fault Morphology Obtained from Unsupervised Machine Learning of Clusters of Aftershocks.\" \n",
    "\n",
    "Authors: Brennan Brunsvik, Gabriele Morra, Gabriele Cambiotti, Lauro Chiaraluce, Raffaele Di Stefano,\n",
    "Pasquale De Gori, David A. Yuen.\n",
    "\n",
    "This Jupyter Notebook combines the paper and code to improve transparency\n",
    "in our work and allow the reader to run their own tests. The code can be ran interactively in the cloud thanks to Binder.\n",
    "\n",
    "This notebook was designed to be simple to use while still giving control. The majority of code has been placed into seperate files. Almost all code used for the paper is contained within the combination of these notebooks. The figures are present, though they may look different than in the main text. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, set up the notebook by importing everything and designating folders for use. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-05T20:36:15.722973Z",
     "start_time": "2019-05-05T20:36:06.620142Z"
    }
   },
   "outputs": [],
   "source": [
    "# THIS CELL CHOOSES HOW TO RUN THE NOTEBOOK. THE OTHER CELLS CAN BE LEFT ALONE.\n",
    "\n",
    "runAllFigures = False\n",
    "\n",
    "\n",
    "# The purpose of this code block is to decide whether to plot 3d interactive figures within the notebook\n",
    "# or to save the plots as a file (temp-plot.html) that can be seen in the Supplementary_Jupyter_Notebook\n",
    "# folder. The former option takes more memory and time, but automatically reloads. \n",
    "# The second option requires you to manually reload the file, but looks nicer. \n",
    "plotlyInNotebook = True\n",
    "if plotlyInNotebook:\n",
    "    from plotly.offline import iplot as plot\n",
    "else:\n",
    "    from plotly.offline import plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-05T20:36:15.747788Z",
     "start_time": "2019-05-05T20:36:15.728776Z"
    }
   },
   "outputs": [],
   "source": [
    "# For loading saved values. \n",
    "variable_folder = \"stored_variables/\"\n",
    "slip_dist_folder = 'slip_dist/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-05T20:36:23.830078Z",
     "start_time": "2019-05-05T20:36:15.755023Z"
    }
   },
   "outputs": [],
   "source": [
    "#Import builtin\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "from datetime import datetime, date, time\n",
    "import copy\n",
    "import json\n",
    "\n",
    "#sci and num py\n",
    "import numpy as np\n",
    "import scipy.optimize as optimize\n",
    "from scipy.optimize import least_squares\n",
    "from scipy.interpolate import Rbf\n",
    "from scipy import interpolate\n",
    "import scipy\n",
    "import scipy.io as sio\n",
    "from numpy.linalg import norm\n",
    "from numpy.linalg import inv\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "#okada wrapper is crucial\n",
    "try: \n",
    "    from okada_wrapper import dc3d0wrapper, dc3dwrapper \n",
    "except: \n",
    "    print('Please install the Okada Wrapper module.\\\n",
    "            https://github.com/tbenthompson/okada_wrapper') \n",
    "\n",
    "    \n",
    "## visualization\n",
    "# matplotlib\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Ellipse\n",
    "import matplotlib.patches as patches\n",
    "from matplotlib.ticker import AutoMinorLocator\n",
    "import matplotlib.mlab as mlab\n",
    "import matplotlib\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from scipy.spatial import ConvexHull\n",
    "import matplotlib.path as mpltPath\n",
    "from matplotlib.patches import FancyArrowPatch\n",
    "from mpl_toolkits.mplot3d import proj3d\n",
    "\n",
    "# Import plotly related stuff\n",
    "#import chart_studio.plotly as py# replaced: import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode\n",
    "init_notebook_mode(connected=True)\n",
    "from scipy.spatial import Delaunay\n",
    "import matplotlib.cm as cm\n",
    "from functools import reduce\n",
    "\n",
    "from IPython.display import Image\n",
    "##\n",
    "\n",
    "# For clustering\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import sklearn.mixture\n",
    "import multiprocessing #!# may not need multiprocessing\n",
    "import sklearn\n",
    "\n",
    "# pyproj handles UTM projection\n",
    "import pyproj\n",
    "\n",
    "# import TDDispl\n",
    "# import TDStrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-05T20:36:24.581231Z",
     "start_time": "2019-05-05T20:36:23.836953Z"
    }
   },
   "outputs": [],
   "source": [
    "# Functions for loading data. \n",
    "%run ./Load_Data.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-05T20:36:25.060893Z",
     "start_time": "2019-05-05T20:36:24.588521Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get functions that are used for many mathematical calculations. Open that file for details. \n",
    "%run ./Calculation_Functions.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-05T20:36:25.258631Z",
     "start_time": "2019-05-05T20:36:25.065129Z"
    }
   },
   "outputs": [],
   "source": [
    "# Functions for fault splitting \n",
    "%run ./Fault_Splitting.ipynb #!# remove this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-05T20:36:25.531812Z",
     "start_time": "2019-05-05T20:36:25.264590Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get some fundamental functions for making figures. Open that file for details. \n",
    "%run ./Figure_Making_Functions.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-05T20:36:25.997884Z",
     "start_time": "2019-05-05T20:36:25.535999Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get clustering info\n",
    "%run ./clustering_base_file.ipynb\n",
    "print('Using old clustering file for now')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-05T20:36:26.844960Z",
     "start_time": "2019-05-05T20:36:26.002088Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get some stress related stuff\n",
    "%run ./stress_base_file.ipynb \n",
    "# ! This shouldn't be needed, 01/04/2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A lack of knowledge of the 3D structure of faults at depth results in several challenges. We propose a data-analytical method to unveil 3D rupture morphology of faults using unsupervised clustering techniques on earthquakes in seismic sequences. We apply this method to the 2009 L’Aquila sequence using about 50,000 relocated hypocenters. Using our 3D fault model, we invert slip distribution models using co-seismic displacement measured at 77 GPS stations and compare these results with a similarly inverted planar fault model as well as a high resolution planar joint inversion. We then use these models to find the induced changes in Coulomb stress (∆CFF) projected on 3415 aftershock focal mechanisms. We find that our slip model on a 3D surface, compared to the analogous planar model, finds fewer events with +∆CFF on focal mechanisms. However, it finds more events with +∆CFF when all aftershocks are used and assumed to be optimally oriented for failure. Ultimately, we find that ∆CFF is different between planar and 3D fault models only in proximity to the main fault. This new method shows promise as a step toward robustly and quickly obtaining 3D rupture morphologies where earthquake sequences have been monitored. Our technique also provides complimentary information to typical co-seismic data which will help to choose from non-unique, nonlinear mainshock inversion solutions.\f",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Introduction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 3D morphology of faults and earthquakes is generally unknown at depth due to the lack of information which could constrain it. We present a new approach where we find the 3D morphology of faults and earthquake rupture surfaces based on the spatial distribution of aftershocks in seismic sequences. We apply this method to the 2009 L’Aquila seismic sequence in the central Apennines. Understanding 3D fault morphology can be useful in many ways. The new method gives insight into fault morphology, earthquake triggering, and can potentially be used to better assess seismic hazard.\n",
    "\n",
    "We specifically apply our 3D fault model to study earthquake triggering. Understanding the earthquake source process and how earthquake sequences evolve requires accurate calculations of how stress changes due to an earthquake. Models of stress change induced by earthquakes can be performed using solutions that assume homogeneous slip (Brune, 1970; Okada, 1985, 1992). To mitigate the error from this assumption, faults are usually divided into many smaller patches with different slip (e.g. Harris and Segall, 1987; King, Stein, and Lin, 1994; Jiang et al., 2014). Due to the difficulty in knowing the 3D morphology of blind faults, planar fault geometry is usually assumed (for clarity, “fault plane” in this text does not refer to an active faults rupture surface and orientation. We use “plane” in a literal sense; for models of faults and/or rupture which have no curvature. This contrasts to the 3D morphology of faults and/or rupture, which can have a non-planar shape). \n",
    "\n",
    "Planar mainshock models may be problematic for modeling stress transfer. Static stress transfer and its earthquake implications are not fully understood. Mainshock originating ∆CFF (a measurement of how a mainshock promotes failure on faults) on aftershocks is often only slightly biased towards positive, suggesting that static stress only partially explains aftershocks. Because mainshock fault morphology is usually modelled as planar, the capability for fault morphology inaccuracy to interfere with the results should be considered, and methods to reduce this inaccuracy should be developed. We use our 3D rupture morphology approach, combined with GPS based co-seismic slip distribution inversion, to address this problem in the L’Aquila sequence. \n",
    "\n",
    "Chiaraluce (2012) provided a review of the seismic activity in the central-northern Apennines with a focus on the 2009 L’Aquila sequence (Figure 1). The L’Aquila sequence activated a south-west dipping normal fault system about 50 km long. Paleo-seismological trenching of the Paganica fault zone, considered to be the geological source responsible for the L’Aquila main shock with MW 6.1 (Scognamiglio et al., 2010), suggests a surface rupture recurrence interval between 700 and 1250 years with the possibility of previous earthquakes with MW>6.5 (Cinti et al., 2011), larger than the 2009 mainshock. The L’Aquila sequence is part of a larger extensional fault system that has experienced several recent earthquakes. The 1979 Umbria sequence with a MS 5.9 mainshock (Deschamps, Iannaccone, and Scarpa, 1984) occurred between the 2009 L’Aquila and 1997 Colfiorito earthquake sequences (Chiaraluce et al., 2003). The 1997 Colfiorito sequence with a MW 6.0 mainshock occurred approximately 60 km north-northwest of the L’Aquila sequence and had several 5<MW<6 earthquakes (Chiaraluce et al., 2003; Nostro et al., 2005). In October 2016, a seismic sequence occurred near Amatrice, northwest of the L'Aquila seismic sequence, with a maximum magnitude of 6.5 (Chiaraluce et al., 2017). \n",
    "\n",
    "The combination of nearby seismic sequences shows rupturing on related fault systems extending for more than 150 km. These sequences are interconnected in that each modifies the stress throughout the fault system, promoting future seismic sequences in some areas and inhibiting them in other areas. Verdecchia et al. (2018) showed that ∆CFF and viscoelastic stress transfer have linked several of these sequences together. This is true in the short and long term. They showed that the L’Aquila sequence experienced ~1 bar of ∆CFF beginning with the 1915 MW 6.9 Fucino earthquake up until rupture in 2009. This suggests the importance of modelling stress transfer between sequences as accurately as possible for scientific and hazard assessment purposes. \n",
    "\n",
    "Modification of fluid properties can result in rupture and is potentially an important component of seismic sequences. If fluid pressure is increased, it counteracts the normal stress that keeps a fault stable. This can happen in the dynamic sense due to passing seismic waves (Hill et al., 1993), or due to static stress transfer (Cocco and Rice, 2002). It is also possible for high pressure fluids to migrate into fault zones and cause seismicity. The 1997 Umbria-Marche sequence in the northern Apennines was proposed to be controlled by high-pressure CO2 entering and releasing faults (Miller et al., 2004). Fluids have also been proposed to be very important throughout the L’Aquila seismic sequence (Di Luccio et al., 2010; Lucente et al., 2010; Malagnini et al., 2012). This is supported in part based on temporal changes in vp/vs ratios, and northward migration of aftershocks at rates comparable to that of fluid flow. \n",
    "\n",
    "Earthquakes associated with the L’Aquila sequence started at the end of 2008. Events continued for about 4 months and included a MW 4.0 earthquake on March 30th before the mainshock ruptured on April 6th (Chiaraluce et al., 2011). Before the mainshock, events were focused along ~10 km of the fault segment in the deepest part of the L’Aquila fault. These have been extensively studied and described (e.g. Di Luccio et al., 2010; Lucente et al., 2010; Chiaraluce, 2012; Valoroso et al., 2013). The aftershocks occurred on three primary SE striking and SW dipping fault systems: L’Aquila, Campotosto, and Cittareale (Figures 1 and 2). The mainshock and a large portion of aftershocks occurred on the L’Aquila fault. The Cittareale fault is NW of the L’Aquila fault, and the Campotosto fault is shifted east from the L’Aquila and Cittareale faults. The aftershocks included several MW>5 events (Chiaraluce, 2012). Southeast of the mainshock, a MW 5.4 event occurred on the L’Aquila fault. Campotosto fault seismicity included a MW 5.0 event on April 6th, and MW 5.0 and 5.2 events on April 9th. These events occurred on the shallower, steeply dipping portion of the Campotosto fault. The deeper, steeply dipping section of the Campotosto fault appears kinked at its base where seismicity abruptly changes to low dip. This kink may have partially controlled the development of aftershocks; a MW 4.4 event occurred later on June 22nd beneath the kink, on the gently dipping portion of the Campotosto fault. On the 25th of June, a 3.9 MW earthquake occurred on a normal fault near Cittareale toward the northernmost portion of the L’Aquila seismic sequence. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Figure 1\n",
    "Figure 1. Modified from Chiaraluce et al. (2017). Map view of local seismicity in the central Apennines. The inset map shows specifically the L’Aquila sequence that we study. Dark blue crosses show the 2009 L’Aquila seismic sequence. Light blue crosses show the 1997 Colfiorito seismic sequence. Black dots show the Amatrice-Visso-Norcia sequence which started in 2016. Black squares show historical earthquakes. Green stars are events with MW≥ 5.0. Red stars are events with MW ≥ 6.0. Focal mechanisms are for events with MW≥ 5.0. Yellow stars and associated focal mechanisms are for events on January 18, 2017 with 5.0≤ML≤5.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-05T20:36:27.107584Z",
     "start_time": "2019-05-05T20:36:26.851508Z"
    }
   },
   "outputs": [],
   "source": [
    "Image(filename='Figure_Map_corrected.jpg') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Figure 2\n",
    "Figure 2. This is a simplified schematic of the fault system involved in the L’Aquila sequence. Seismic activity occurred dominantly on the L’Aquila, Campotosto, and Cittareale faults. The L’Aquila fault hosted the mainshock. The faults strike approximately SE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-05T20:36:28.333159Z",
     "start_time": "2019-05-05T20:36:27.114607Z"
    }
   },
   "outputs": [],
   "source": [
    "matplotlib.rcParams.update({'font.size': 16})\n",
    "\n",
    "\n",
    "# approximate fault locations, sizes, dips\n",
    "p1 = np.array([11.15, -15.17, -7.2])\n",
    "p2 = np.array([-10.58, 10.88, -8.98])\n",
    "p = p1-p2\n",
    "laqstrikewidth = np.linalg.norm(p)\n",
    "laqdipwidth = .4 * laqstrikewidth\n",
    "\n",
    "camstrikewidth = 2/3 * laqstrikewidth\n",
    "camdipwidth = .4 * camstrikewidth \n",
    "\n",
    "citstrikewidth = np.array([10])\n",
    "citdipwidth = .4 * citstrikewidth\n",
    "\n",
    "cit = [-14.8,  25.5, -5.09, citstrikewidth, citdipwidth, - 5 * np.pi/4, 50 * np.pi/180]\n",
    "cam = [-2.83,  17.3, -8.55, camstrikewidth, camdipwidth, - 5 * np.pi/4, 50 * np.pi/180]\n",
    "laq = [ 4.31, -3.64, -6.78, laqstrikewidth, laqdipwidth, - 5 * np.pi/4, 50 * np.pi/180]\n",
    "allFaults = [cit, cam, laq]\n",
    "\n",
    "def makeSchemSurfPoints(x, y, z, xWidth, yWidth, s, d, reps = 2):\n",
    "    # return points on at outside of fault based on inner point of fault\n",
    "    __, strikeUnit = faultUnitVectors(s, d, 0       )\n",
    "    __, dipUnit    = faultUnitVectors(s, d, -np.pi/2)\n",
    "    \n",
    "    points = np.zeros((reps, reps, 3))\n",
    "    points[:, :] = np.array([x, y, z])\n",
    "    \n",
    "#     mults = np.array([-.5, .5])\n",
    "    mults = np.linspace(-.5, .5, reps)\n",
    "    for i, m1 in enumerate(mults):\n",
    "        for j, m2 in enumerate(mults):\n",
    "            points[i, j] += m1 * strikeUnit * xWidth + m2 * dipUnit * yWidth\n",
    "            \n",
    "    xs = points[:, :, 0].ravel()\n",
    "    ys = points[:, :, 1].ravel()\n",
    "    zs = points[:, :, 2].ravel()\n",
    "    \n",
    "    return xs, ys, zs\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "fig=plt.figure(figsize=(12, 12))\n",
    "ax=fig.add_subplot(111,projection='3d')\n",
    "\n",
    "for fault in allFaults:\n",
    "    reps = 2\n",
    "    x, y, z = makeSchemSurfPoints(*fault, reps = reps)\n",
    "    x=x.reshape((reps, reps))\n",
    "    y=y.reshape((reps, reps))\n",
    "    z=z.reshape((reps, reps))\n",
    "    ax.plot_surface(x, y, z, linewidth=1, edgecolors='k', alpha = .7)\n",
    "   \n",
    "arrow = [np.array([-20, -20]),\n",
    "         np.array([0, 10]),\n",
    "         np.array([0, 0]) ]\n",
    "class Arrow3D(FancyArrowPatch):\n",
    "    # arrows in 3D\n",
    "    def __init__(self, xs, ys, zs, *args, **kwargs):\n",
    "        FancyArrowPatch.__init__(self, (0,0), (0,0), *args, **kwargs)\n",
    "        self._verts3d = xs, ys, zs\n",
    "\n",
    "    def draw(self, renderer):\n",
    "        xs3d, ys3d, zs3d = self._verts3d\n",
    "        xs, ys, zs = proj3d.proj_transform(xs3d, ys3d, zs3d, renderer.M)\n",
    "        self.set_positions((xs[0],ys[0]),(xs[1],ys[1]))\n",
    "        FancyArrowPatch.draw(self, renderer)\n",
    "a = Arrow3D(*arrow, mutation_scale=20, \n",
    "                lw=3, arrowstyle=\"-|>\")\n",
    "ax.add_artist(a)\n",
    "ax.text3D(arrow[0][1], arrow[1][1], arrow[2][1], 'N')\n",
    "\n",
    "arrowUp = [arrow[0],\n",
    "         np.array([0, 0]),\n",
    "         np.array([0, 10])]\n",
    "a = Arrow3D(*arrowUp, mutation_scale=20, \n",
    "                lw=3, arrowstyle=\"-|>\")\n",
    "ax.add_artist(a)\n",
    "ax.text3D(arrowUp[0][1], arrowUp[1][1], arrowUp[2][1], 'UP')\n",
    "\n",
    "\n",
    "scaleBar = [np.array([-15,  -15]),\n",
    "            np.array([0 ,  10]),\n",
    "            np.array([0, 0]) ]\n",
    "b = Arrow3D(*scaleBar, mutation_scale=20, lw=3, arrowstyle = '-')\n",
    "ax.add_artist(b)\n",
    "ax.text3D(scaleBar[0][1]+1, scaleBar[1][1] - 5, scaleBar[2][1], '10 Km')\n",
    "\n",
    "labels = ['Cittareale', 'Campotosto', 'L\\'Aquila']\n",
    "for i, fault in enumerate(allFaults):\n",
    "    ax.text3D(fault[0], fault[1], fault[2], labels[i])\n",
    "    \n",
    "l = ax.get_xlim()\n",
    "ax.set_ylim(l)\n",
    "ax.set_zlim(l)\n",
    "\n",
    "groundrange = np.array(l, l)\n",
    "xg, yg = np.meshgrid(groundrange,groundrange)\n",
    "zg = np.zeros(xg.shape)\n",
    "ax.plot_surface(xg, yg, zg, alpha = .1)\n",
    "\n",
    "ax.set_xlabel('East (Km)')\n",
    "ax.set_ylabel('North (Km)')\n",
    "ax.set_zlabel('Vertical (Km)')\n",
    "\n",
    "\n",
    "ax.xaxis.pane.set_edgecolor('black')\n",
    "ax.yaxis.pane.set_edgecolor('black')\n",
    "ax.zaxis.pane.set_edgecolor('black')\n",
    "ax.grid(False)\n",
    "\n",
    "# ax.view_init(elev = 20, azim = -10)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Rupture Morphology and Clustering of Earthquakes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At a large scale, for instance through the Wadati Benioff Zone, the 3D morphology of subducting slabs plates has been determined (Gudmundsson and Sambridge, 1998; Syracuse and Abers, 2006; Hayes, Wald, and Johnson, 2012). For megathrusts, for instance the 7.8 MW 2015 Gorkha earthquake, fault morphology (Hubbard et al., 2016) has been recently incorporated into stress studies (Qiu et al., 2016; Landry and Barbot, 2018) providing important information on locked zones (Avouac, 2008; Avouac et al., 2015). Locked zones are crucial to understand as they can be a source of intense seismicity. For the major subduction zones of the Earth, it has been shown that curvature on a fault is an important control for earthquake rupture area and magnitude (Bletery et al., 2016), suggesting that understanding fault morphology is important for assessing seismic hazard. For strike-slip faults, 3D fault morphology is sometimes estimated by projecting mapped surface ruptures into the ground. For moderate sized dip-slip earthquakes, 3D morphology is more difficult to obtain.\n",
    "\n",
    "The lack of 3D fault model detail is problematic as it is not always reasonable to assume that a fault lies on a plane. This is shown in field work (e.g. Cinti et al., 2011) and seismic imaging (e.g. Lohr et al., 2008). For example, in extensional systems, normal faults may change their morphology dramatically along strike or with depth. An example of this is given within our study area by the Campotosto fault showing an abrupt change in dip toward depth from high angle to low angle (Chiaraluce, 2012). However, rupture can also occur on several smaller faults that are not necessarily coplanar but instead link together through complex fractures (Kim, Peacock, and Sanderson, 2004; Childs et al., 2009). Further, it is known that 3D fault roughness is an important component regarding earthquake magnitude and stress generation (Zielke, Galis, and Mai, 2017). Inaccuracies in fault morphology can strongly interfere with earthquake source inversions resulting in unrealistic slip distributions (Ragon, Sladen, and Simons, 2018) and unrealistic ∆CFF calculations. This illustrates the importance of developing and improving methods to retrieve and incorporate accurate 3D morphology of faults in stress calculations. \n",
    "\n",
    "With 3D seismic imaging, understanding deep fault morphology is not a new task. Because fault damage zones have low seismic velocity, they can create head waves and trapped waves in response to passing seismic waves. Modelling of such waves gives a way to see fault structures (Ben-Zion et al., 2003). Sorting through active source image data to locate points lying on faults can allow software guided interpolation to reconstruct fault surfaces (e.g. Lohr et al., 2008; Røe, Georgsen, and Abrahamsen, 2014). For instance, the 3D morphology of the low angle Altotiberina normal fault in northern Italy, obtained from merging seismic imaging, borehole analysis, geologic maps, and field surveys, was used to model stresses associated with creep and to estimate a deterministic velocity model (Mirabella et al., 2011 and references therein; Anderlini et al., 2016; Latorre et al., 2017). Thanks to a recent triangular fault patch slip solution (Nikkhoo and Walter, 2015), instead of the more traditional Okada (1985, 1992) model which uses rectangular fault patches, there have been numerous stress experiments with 3D morphologies (e.g. Qiu et al., 2016; Landry and Barbot, 2018). Dynamic stress simulations have also incorporated 3D morphologies (e.g. Pelties et al., 2012; Galvez et al., 2014; Zhang, Zhang, and Chen, 2014). \n",
    "\n",
    "Unfortunately, the costs of seismic imaging make this a difficult option for finding the shape of relatively small active faults. This suggests the need for different methods for finding the morphology of a mainshock. The 2009 L’Aquila earthquake sequence provides an excellent opportunity to reconstruct the fault morphology and also to apply slip inversion methods to test the importance of incorporating the fault curvature in stress calculations. This is because the seismic sequence was particularly well monitored, and a great deal of data including hypocenter locations, focal mechanisms, and GPS data are available. With this large amount of data, we attempt to reconstruct the complex curvature of the fault that hosted the mainshock by interpolating for the best fit fault surface within clusters of aftershocks. With the fault morphology reconstructed, we attempt to gain further insight into earthquake interaction by calculating ∆CFF induced by this 3D model as well as planar mainshock models. \n",
    "\n",
    "We employ unsupervised machine learning clustering for our methods (Pedregosa et al., 2011). Clusters of similar data (e.g. location, time of occurrence, magnitude, etc.) can be found using these techniques. See Kong et al. (2018) and Bergen et al. (2019) for an overview of machine learning in seismology and Earth sciences. Clustering of earthquakes has been used extensively for hypocenter relocation. Initial estimates of hypocenter locations are generally poor, but several techniques can improve the accuracy of these already located earthquakes. In the collapsing method (Jones and Stewart, 1997), the centroid of a cluster of nearby events is used to relocate hypocenters by shifting them toward the position of the cluster’s centroid. Many relocation techniques take advantage of the correlation in travel path of the waves originating from clustered earthquakes. Jordan and Sverdrup (1981) developed the hypocentroid decomposition theorem, which has been used in other cluster based relocation techniques (e.g. Bergman and Solomon, 1990; Karasözen et al., 2016). Cross-correlation of waves for similar clustered events has been used for relocation (e.g. Poupinet, Ellsworth, and Frechet, 1984; Dodge, Beroza, and Ellsworth, 1995; Shearer, 1997). For the L’Aquila sequence, the double difference relocation method (Waldhauser and Ellsworth, 2000) was used for the relocation of aftershocks (Chiaraluce et al., 2011), and extension of the aftershock catalog (Valoroso et al., 2013). Generally, machine learning methods of clustering are not relevant to these mentioned studies. Recently, Hierarchical clustering, a machine learning algorithm, was used in GrowClust (Trugman and Shearer, 2017) for earthquake relocation. Clustering has also been used on various earthquake features to investigate earthquake processes and precursors (Dzwinel et al., 2003; Dzwinel et al., 2005; Yuen et al., 2009). \n",
    "\n",
    "Spatial clustering of earthquakes with applications outside of hypocenter relocation have been done as well. Leśniak and Isakow (2009) applied clustering to assessing seismic safety in mines. After locating earthquakes, they clustered the hypocenters using several algorithms based on location and time of occurrence. They then assessed seismic hazard associated with the most dangerous clusters. Unlike our methods, they did not attempt to cluster with the assumption that earthquakes lie on the same faults because this was not relevant to their study. Rietbrock et al. (1996) used waveform clustering to find about 20 different clusters of earthquakes in the Gulf of Corinth. The purpose was to remove ambiguity regarding certain focal mechanisms to determine if low angle normal faulting had been observed. They found an RMS least-error plane to fit the cluster of earthquakes and determined that these events did display low angle normal faulting. However, fault morphology detail was not part of the study. Moment tensor based clustering has also been used (Cesca, Şen, and Dahm, 2013).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Summary of Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The L’Aquila earthquake sequence was particularly well monitored, allowing for relocation of over 50,000 aftershocks throughout 2009 (Valoroso et al., 2013). Using the aftershock locations, we cluster them into different faults. This allows us to interpolate for a fault surface that best fits a clustered set of hypocenters. We distribute small segments of fault cells oriented according to the interpolated surface and we calculate stress as the sum of stresses generated by these cells. GPS coverage of the mainshock was exceptional with 77 3-component co-seismic displacements available (Serpelloni et al., 2012). We use this to invert for and compare the best fit slip distribution for planar faults and for the 3D fault. We calculate ∆CFF that resulted from these fault models onto ~3400 aftershocks. We compare our results to the planar joint inversion of Cirella et al. (2012). The Python code is written in Jupyter Notebooks (Oliphant, 2006; Kluyver et al., 2016; Morra, 2018). It is available in GitHub (https://github.com/brennanbrunsvik/Fault-morphology-clustering) where it can be ran through the cloud service Binder. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The availability of such a large dataset of L’Aquila sequence earthquake locations allows for high precision analysis of the anatomy of the fault system (Valoroso et al., 2013). In order to interpolate a fault surface based on hypocenters, we first associated aftershocks with their respective faults. To distinguish between different faults in the hypocenter data, we perform clustering on the hypocenters based on their spatial locations. This presents many challenges. Where faults intersect, hypocenters of different faults can be very close to each other and clustering algorithms can be blind to this. Also, aftershock location inaccuracy is inevitably a problem due to imperfect knowledge of crustal 3D velocities and elastic properties. Thus, hypocenters tend to erroneously appear between multiple faults, and which fault these hypocenters belong to is unclear. To mitigate this, clustering is done in two iterations; first with a spectral method, and second with the DBSCAN method. Many overviews of clustering algorithms are available (e.g. Omran, Engelbrecht, and Salman, 2007; Joshi and Kaur, 2013).\n",
    "\n",
    "During the first clustering iteration, we used the spectral clustering algorithm from Scikit-Learn (Pedregosa et al., 2011). Spectral clustering is good at distinguishing clusters when data are connected but a point’s distance from the center of the cluster is irrelevant, and when the clusters are non-convex. This algorithm computes the Gaussian affinities (the quantitative similarity between points) based on the distances between points. It then creates a normalized Laplacian matrix and clusters based on the eigenvectors. See Von Luxburg (2007) for more details on spectral clustering. We used only the 343 earthquakes with ML >= 2.5 during the first clustering attempt. This helps to remove small aftershocks that blur the distinction between different faults. It also makes the memory demanding spectral clustering algorithm more stable and manageable. This step separated the hypocenters into three dominant clusters. \n",
    "\n",
    "The second clustering iteration requires a modified data set to enhance fault like trends. For the cluster containing the mainshock, we found a least squares distance best fit plane to the hypocenters. The positions of all hypocenters were then stretched by a factor of 5 in the direction normal to the best fit plane. The stretching factor is subjectively chosen for ideal performance on the L’Aquila fault. The purpose is that trends along the fault surface are left alone, but deviations from this surface are amplified. With planar like trends more pronounced, clustering algorithms are much more successful at clustering long faults and keeping them separate from neighboring faults. This also reduces the problem that some hypocenter groups may lie along the same fault, but there are so few hypocenters between these groups that clustering algorithms normally find them as separate clusters. This spatial modification allows for the use of less computationally expensive clustering methods. \n",
    "\n",
    "For the second clustering iteration on the modified hypocenters, we used DBSCAN (Density-Based Spatial Clustering of Applications with Noise; Ester et al., 1996), which is available through Scikit-Learn (Pedregosa et al., 2011). This works by first finding a sample that has a minimum number of other samples, MinPts, within a minimum distance, Eps, and establishes this as a core sample of a cluster. Nearby core points become part of the same clusters. Points that do not satisfy the MinPts condition but are near another core point are considered part of the same cluster as the core point, but they are not themselves core points. Points that (i) have distance > Eps from all core points, and (ii) cannot become the core point for a new cluster are considered noise and are excluded from all clusters. We chose Eps=2.0 km and MinPts=80 samples. The selection of these values is somewhat arbitrary and choosing values that work well in one location may not work well in other locations. MinDist and Eps were chosen to perform best for the L’Aquila cluster with some expense to performance of the other clusters.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-05T20:36:30.427342Z",
     "start_time": "2019-05-05T20:36:28.337994Z"
    }
   },
   "outputs": [],
   "source": [
    "# object with info for all hypocenters. \n",
    "allHypo = hypoData(minMag=None)\n",
    "\n",
    "allHypo.labels=np.zeros(allHypo.mL.size)\n",
    "\n",
    "if False:\n",
    "    allHypo.plot3dclusters(pointSize=1, save=False)\n",
    "\n",
    "# Spectral clustering\n",
    "sp = spClust()\n",
    "sp.minMag=2.5\n",
    "sp.getHypocenters()\n",
    "\n",
    "sp.assign_labels=\"discretize\"\n",
    "sp.n_init=10\n",
    "sp.beta=1\n",
    "sp.n_clusters=3 # choose this before hand based on the 3 main faults\n",
    "\n",
    "sp.cluster()\n",
    "# spectral clustering is completed \n",
    "# sp.labels indicates cluster identities\n",
    "if False:\n",
    "    sp.plot3dclusters(pointSize=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-05T20:36:36.021074Z",
     "start_time": "2019-05-05T20:36:30.431969Z"
    }
   },
   "outputs": [],
   "source": [
    "# Make a best fit plane to the cluster containing the mainshock\n",
    "\n",
    "laqFault = sp.labels==2 # make bool for hypocenters in the L'Aquila fault cluster\n",
    "\n",
    "x = sp.x[laqFault]\n",
    "y = sp.y[laqFault]\n",
    "z = sp.z[laqFault]\n",
    "mL = sp.mL[laqFault]\n",
    "strikeHyp = sp.st1[laqFault]\n",
    "dipHyp = sp.dp1[laqFault]\n",
    "strike2Hyp = sp.st2[laqFault]\n",
    "dip2Hyp = sp.dp2[laqFault]\n",
    "\n",
    "focalBool = (~( (sp.st1==0) * (sp.st2==0) * (sp.dp1==0)\n",
    "               * (sp.dp2==0) * (sp.rk1==0) * (sp.rk2==0) ))[laqFault]\n",
    "\n",
    "# maxMag = 4\n",
    "# magBool = (mL<=4)\n",
    "\n",
    "\n",
    "splineNodesLine = 7\n",
    "\n",
    "# exclude points over 10km from plane. Do rms fit. \n",
    "laqPlane = interp(x, y, z, \n",
    "               strikeHyp=strikeHyp, dipHyp=dipHyp, strike2Hyp=strike2Hyp, dip2Hyp=dip2Hyp,focalBool=focalBool,\n",
    "               eps=.1, magnitudeWeight=mL, minPlaneDist = 10000,exp=2,\n",
    "               splineNodesStrike = splineNodesLine, splineNodesDip = splineNodesLine)\n",
    "\n",
    "# Prepare for second clustering attempt by \"stretching\" hypocenters.\n",
    "db2 = dbClust() #!# rename db2 to db\n",
    "db2.minMag=None # use all earthquakes\n",
    "db2.getHypocenters()\n",
    "db2.geogToMet()\n",
    "\n",
    "# p is vector of points.\n",
    "db2.p = rotateUniVec(db2.p-np.array([laqPlane.xC,laqPlane.yC, laqPlane.zC]),\n",
    "    laqPlane.strikeHat, laqPlane.dipHat, laqPlane.normalHat)\n",
    "\n",
    "# stretch points away from plane, but not along plane. \n",
    "db2.p[:,2]*=5\n",
    "\n",
    "# distances from plane in new coordinate system\n",
    "distEach = np.abs(distFromPlane(db2.x, db2.y, db2.z, \n",
    "                                laqPlane.xC, laqPlane.yC, laqPlane.zC, \n",
    "                                laqPlane.normalHat[0], laqPlane.normalHat[1], laqPlane.normalHat[2]))\n",
    "\n",
    "# We get a warning regarding matrix deficiency, but this is not relevant to the aquired results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-05T20:36:40.397534Z",
     "start_time": "2019-05-05T20:36:36.026020Z"
    }
   },
   "outputs": [],
   "source": [
    "db2.eps=2.0* 1e3\n",
    "db2.min_samples=80\n",
    "db2.fitdbscan(distEach) # cluster in modified axis system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Figure 3\n",
    "Figure 3. Also available interactively online at https://plot.ly/~BrennanBrunsvik/16/. This shows the final results from clustering hypocenters from the L’Aquila seismic sequence. Spectral clustering was first used to find three primary clusters. DBSCAN was then used on all hypocenter location data. The darkest blue dots correspond to noise. Other dark blue dots correspond to the Paganica fault, light blue to the Campotosto fault, and red to the Cittareale fault. Other clusters were found which do not necessarily relate to known faults."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-05T20:36:45.866696Z",
     "start_time": "2019-05-05T20:36:40.402378Z"
    }
   },
   "outputs": [],
   "source": [
    "if False or runAllFigures:\n",
    "    db2.plot3dclusters(pointSize=1.6, save = False, lineWidthR=1e-10)\n",
    "    \n",
    "# if True:\n",
    "#     db2.plot3dclusters(pointSize=.8, save = True, lineWidthR=1e-10, name = 'Figure S3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final clusters, the results of DBSCAN, can be seen in Figure 2. The reader is also referred to an online interactive 3D plot (Figure S1). Seven clusters were recognized of which some can be visually identified as major faults. The separation between major faults and scattered single aftershocks (referred to as noise and rejected by the DBSCAN algorithm) appears successful. However, sets of faults antithetic to and intersecting the mainshock cluster contained a high density of aftershocks, and these were not distinguishable from the main fault through clustering (Figure 4b). Clustering also failed to separate the Campotosto fault from a synthetic fault lying between the L’Aquila and Campotosto faults. This, however, did not affect the mainshocks cluster, which is the main topic of this study.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Fault Surface Construction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hypocenters in the clustered fault which contained the mainshock were spatially interpolated to make a best fit 3D fault surface. The following coordinate system is used: $x_1’$ faces along dip to the southeast, $x_2’$ faces up dip, and $x_3’$ faces the hanging wall, normal to the plane. The bi-cubic splines of Scientific Python (Jones et al., 2014) provide an excellent tool to interpolate the hypocenters. On the plane that best fits the aftershock locations of the L’Aquila fault cluster, we created a 7x7 grid of knots (nodes on which spline parameters are solved) extending from the minimum to the maximum hypocenter locations in $x_1’$ and $x_2’$ such that all of the cluster is contained within the knot grid. Knot parameters are inverted so that the surface provides a least square error distance to the hypocenters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-05T20:36:47.116953Z",
     "start_time": "2019-05-05T20:36:45.875333Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create boolean showing which quakes are in L'Aquila cluster\n",
    "\n",
    "# Then create boolean of those for which focal mechanisms are available\n",
    "booLaqdb = db2.labels==0\n",
    "\n",
    "allPoints = hypoData(minMag=None)\n",
    "allPoints.getHypocenters()\n",
    "\n",
    "x2 = allPoints.x[booLaqdb]\n",
    "y2 = allPoints.y[booLaqdb]\n",
    "z2 = allPoints.z[booLaqdb]\n",
    "st1 = allPoints.st1[booLaqdb]\n",
    "st2 = allPoints.st2[booLaqdb]\n",
    "dp1 = allPoints.dp1[booLaqdb]\n",
    "dp2 = allPoints.dp2[booLaqdb]\n",
    "focalbooLaqdb = (~( (allPoints.st1==0 ) * (allPoints.st2==0 ) * (allPoints.dp1==0 )\n",
    "  * (allPoints.dp2==0 ) * (allPoints.rk1==0 ) * (allPoints.rk2==0 ) ))[booLaqdb]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # changing on 02012020\n",
    "# def rotateUniVec(p, e1, e2, e3):\n",
    "#     \"\"\"returns pPrime\n",
    "#     p is array of vectors in original coordinates of shape (samples, 3)\n",
    "#     e1, e2, and e3 are the new unit vectors in the old axis, each of shape (3)\"\"\"\n",
    "#     points = p[:,0].size\n",
    "    \n",
    "#     x = np.zeros(points)\n",
    "#     y = np.zeros(points)\n",
    "#     z = np.zeros(points)\n",
    "\n",
    "#     for i in np.arange(points):\n",
    "#         x[i] = np.dot( p[i], e1 ) # !!\n",
    "#         y[i] = np.dot( p[i], e2 ) # !! \n",
    "#         z[i] = np.dot( p[i], e3 ) \n",
    "        \n",
    "#     pPrime = np.array([x, y, z]).T\n",
    "    \n",
    "#     print(x[:3])\n",
    "#     print(y[:3])\n",
    "#     return\n",
    "        \n",
    "#     return pPrime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# surface.spline.get_knots()[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-05T20:38:26.019890Z",
     "start_time": "2019-05-05T20:38:11.138420Z"
    }
   },
   "outputs": [],
   "source": [
    "# %%capture \n",
    "# get rid of a unimportant warning\n",
    "\n",
    "splineNodesLine = 7 # changing from 7, 02/15/2020\n",
    "cutPoints = True # convex hull. 02/15/2020\n",
    "\n",
    "# note that surface is object for spline interpolation\n",
    "# surf is object for our 3D mainshock model\n",
    "surface = interp(x2, y2, z2,\n",
    "               strikeHyp=st1, dipHyp=dp1, strike2Hyp=st2, dip2Hyp=dp2, focalBool=focalBool,\n",
    "               eps=.01, minPlaneDist = 10000,exp=2,\n",
    "               splineNodesStrike = splineNodesLine, splineNodesDip = splineNodesLine, \n",
    "               cutPoints = cutPoints, smooth = 1e4)\n",
    "# eps = .01, smooth = ??\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "surface.focalAvailable=focalbooLaqdb\n",
    "\n",
    "# if True:\n",
    "#     print('this is a test! remove it!')\n",
    "#     surface.pS *= -1\n",
    "#     surface.pD *= -1 \n",
    "#     print(\"It doesn't work!\")\n",
    "\n",
    "# plotSurface(surface)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### testing problems on 02032020\n",
    "# different: surface.pS\n",
    "# approximately negative? : surface.e1Prime\n",
    "# approximately negative? : surface.pPrime\n",
    "# slightly different... surface.xC Maybe just some different points were included. \n",
    "# negative: surface.strikeHat\n",
    "# negative: surface.dipHat\n",
    "# Same!: surface.normalHat\n",
    "# StrikeHat and dipHat are different. strikeHat faces the non right hand rule direction. \n",
    "# Where was strikeHat made? \n",
    "\n",
    "# even if I make strikeHat and dipHat negative, figure 5 gets close (it isn't flipped)\n",
    "# BUT, there is a new huge bump in the fault and there is some new aftershocks that weren't there before.\n",
    "\n",
    "# There are the same number of earthquakes in both notebooks. \n",
    "# However, the fault shape was made from 29357 earthquakes and now it's 29993\n",
    "\n",
    "# I thus thing the clustering has changed...\n",
    "\n",
    "# Somehow, the spectral clustering now finds one more earthquake. \n",
    "\n",
    "# the laqPlane is much different... the center points are about 10% changed. \n",
    "\n",
    "# !!! the dip of laqPlane is now only 25 degrees, it was previously 45!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-05T20:38:26.190208Z",
     "start_time": "2019-05-05T20:38:26.025480Z"
    }
   },
   "outputs": [],
   "source": [
    "# Here, I save info from surface object to upload into a mainshock model.\n",
    "# It has to be ran now, but is not relevant until the slip distribution inversion.\n",
    "\n",
    "surface.gridNodesStrike = 40\n",
    "surface.gridNodesDip = 24\n",
    "\n",
    "#the outermost cell center be inset from the outermost part of surface:\n",
    "shiftS = (np.amax(surface.pS) - np.amin(surface.pS)) / surface.gridNodesStrike * .5\n",
    "shiftD = (np.amax(surface.pD) - np.amin(surface.pD)) / surface.gridNodesDip * .5\n",
    "# Generate points on surface\n",
    "surface.splineInterp(cutPoints=False, sEdgeShift = shiftS, dEdgeShift = shiftD)\n",
    "\n",
    "# lat lon and depth of okada cells\n",
    "latSurf, lonSurf = allPoints.metToGeog(surface.interpX, surface.interpY)\n",
    "depthSurf = surface.interpZ\n",
    "\n",
    "# Surface geometry (strike dip, etc.)\n",
    "surface.surfaceNormal(surface.interpS, surface.interpD)\n",
    "stSurf, dpSurf = normalToStrikeDip(surface.normalAprox)\n",
    "surface.simpleCellWidths()\n",
    "strikeWidth = surface.strikeWidth\n",
    "dipWidth = surface.dipWidth\n",
    "\n",
    "# 2d position for spline interpolation\n",
    "interpS = surface.interpS\n",
    "interpD = surface.interpD\n",
    "\n",
    "# I save this as an array and load it later into a different class.\n",
    "saveArrays([latSurf, lonSurf, depthSurf, stSurf, dpSurf, interpS, interpD, strikeWidth, dipWidth],\n",
    "          fName = 'surfaceArraysCoarse', path = variable_folder)\n",
    "##########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-05T20:38:26.347598Z",
     "start_time": "2019-05-05T20:38:26.195463Z"
    }
   },
   "outputs": [],
   "source": [
    "###########\n",
    "# TRIANGULAR DISLOCATION VERSION\n",
    "# this is the same as above, but for the triangular dislocation.\n",
    "# the difference is that these points cover the entire surface,\n",
    "# while the rectangle points do not touch the edge of the surface\n",
    "# the variables that I save are different as well\n",
    "\n",
    "surface.gridNodesStrike = 40\n",
    "surface.gridNodesDip = 24\n",
    "\n",
    "#the outermost cell center be inset from the outermost part of surface:\n",
    "shiftS = 0\n",
    "shiftD = 0 \n",
    "# Generate points on surface\n",
    "surface.splineInterp(cutPoints=False, sEdgeShift = shiftS, dEdgeShift = shiftD)\n",
    "\n",
    "# lat lon and depth of okada cells\n",
    "latSurf, lonSurf = allPoints.metToGeog(surface.interpX, surface.interpY)\n",
    "depthSurf = surface.interpZ\n",
    "\n",
    "# Surface geometry (strike dip, etc.)\n",
    "surface.surfaceNormal(surface.interpS, surface.interpD)\n",
    "stSurf, dpSurf = normalToStrikeDip(surface.normalAprox)\n",
    "surface.simpleCellWidths()\n",
    "strikeWidth = surface.strikeWidth\n",
    "dipWidth = surface.dipWidth\n",
    "\n",
    "# 2d position for spline interpolation\n",
    "interpSTD = surface.interpS\n",
    "interpDTD = surface.interpD\n",
    "\n",
    "# I save this as an array and load it later into a different class.\n",
    "saveArrays([latSurf, lonSurf, depthSurf, stSurf, dpSurf, interpSTD, interpDTD, strikeWidth, dipWidth],\n",
    "          fName = 'surfaceArraysCoarseTD', path = variable_folder)\n",
    "########"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 3D fault surface that best fits the clustered hypocenters can be seen in Figures 4 and 5. We first describe the surface. It extends 34.0 km in $x_1’$ and 15.4 km in $x_2’$. Visually speaking, the surface is smooth and does not deviate greatly from a plane. The basic plane from which the surface is built, where $x_3’=0$, has a strike and dip of 143° and 42.7°, respectively. At the south-east of the fault surface, the dip suddenly decreases going from bottom to top at a depth of about 3 km. The fault surface wavers here as it interacts with the more complex distribution of hypocenters. At the north-west of the fault surface, it is very flat where hypocenter availability was low. \n",
    "\n",
    "We note specifically two locations on the surface where issues in the interpolation might have arose; (i) near the south-east edge of the fault where there are conjugate faults intersecting the L’Aquila fault (Figure 5b), and (ii) at the north-west edge of the fault where there were few aftershocks for interpolation (Figure 4).\n",
    "\n",
    "Regarding problem (i), because the clustering did not distinguish between some conjugate planes that were closely connected to the mainshock plane, the surface interpolation is slightly shifted by these conjugate fault hypocenters (Figure 5). Because the hypocenters were widely distributed near the fault in the direction normal to the fault (the aftershocks deviate from the 3D surface by an average magnitude of ~600 m), small variation in the fault surface does not conflict with the hypocenter distribution. It is likely thus not a problem that the surface is shifted slightly in this location. \n",
    "\n",
    "Regarding problem (ii), although the splines were inverted with regularly distributed knots, the hypocenters which were used to find the knot parameters were distributed unevenly. Particularly at the upper north-west of the fault, interpolation was done where few hypocenters were available. The spline parameters in this location are thus underconstrained. If the morphology of the fault were more complex, this problem would have to be addressed. Because the splines and fault are generally simple and nearly planar, the few available hypocenters in this region appear to be enough for our spline inversion. Further, the result of the slip distribution inversion on this fault surface (discussed later and shown in Figure 8) shows mainshock slip in this under constrained region, suggesting that although few aftershocks are found here, this part of the fault is important and cannot be removed. Further, because we ultimately compare the slip distribution on this fault surface to that of other slip models with rectangular boundaries, the comparison would be inconsistent if we removed portions of our 3D surface (Figure 8). \n",
    "\n",
    "To evaluate the accuracy of our 3D fault model, we compared its orientation to the orientation of earthquake focal mechanisms. This comparison is quantified as the angle between the normal vectors at the fault surface model and the normal vectors of the aftershock focal mechanisms (Figure 6). For the focal mechanisms, both the fault planes and auxiliary planes are used. For both our 3D fault and for the planar jointly inverted model (Cirella et al., 2012), there is alignment at two primary peaks of 20° and 90° which correspond to the fault and auxiliary planes of the focal mechanisms, respectively (Figure 6b). Agreement improves when the energy associated to each event $\\left( \\sim 10^M\\right)$ is used as a weight. Note that we do not include the focal mechanism of the mainshock in this analysis because it has too much weight."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Figure 4\n",
    "Figure 4. (a) 2D representation of the aftershocks that were part of the Paganica fault cluster, and (b) the surface that interpolates them. Position is shown on the best-fitting plane to the hypocenters. Color represents distance in meters from the best fitting plane."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-05T20:38:31.897227Z",
     "start_time": "2019-05-05T20:38:26.353915Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "###! The aftershock image has been transposed! 020102020\n",
    "def plotSurface(surface):\n",
    "    surface.gridNodesStrike = 100\n",
    "    surface.gridNodesDip = 100\n",
    "    surface.splineInterp(cutPoints=True)\n",
    "    cbarmin = -1e3\n",
    "    cbarmax = 1e3\n",
    "\n",
    "    matplotlib.rcParams.update({'font.size': 12})\n",
    "    plt.figure(figsize=(15,6))\n",
    "\n",
    "    xplot = (surface.pS-np.amin(surface.pS))/1000\n",
    "    yplot = (surface.pD-np.amin(surface.pD))/1000\n",
    "    #pS and pD are point (hypocenter) position in coordinate system of (strike, dip, normal)\n",
    "    plt.scatter(xplot, yplot, \n",
    "                c = surface.pN, vmin = cbarmin, vmax = cbarmax, \n",
    "                s = 10, linewidths = .1, edgecolor = 'k') \n",
    "    plt.title('Aftershock Position')\n",
    "    plt.xlabel('Strike distance (km)')\n",
    "    plt.ylabel('Dip distance (km)')\n",
    "    cbar = plt.colorbar()\n",
    "    cbar.set_label('Distance from plane (m)')\n",
    "    plt.axes().set_aspect('equal')\n",
    "\n",
    "    shiftylabel = -2\n",
    "    plt.text(np.amin(xplot), np.amin(yplot)+shiftylabel, 'NW Base', \n",
    "             verticalalignment = 'top',\n",
    "             horizontalalignment = 'right')\n",
    "    plt.text(np.amax(xplot), np.amin(yplot)+shiftylabel, 'SE Base', \n",
    "             verticalalignment = 'top',\n",
    "             horizontalalignment = 'left')\n",
    "\n",
    "    ax = plt.gca()\n",
    "    xlim = ax.get_xlim()\n",
    "    ylim = ax.get_ylim()\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(15,6))\n",
    "    ax = plt.gca()\n",
    "    ax.set(xlim = xlim, ylim = ylim)\n",
    "    xmin = 0\n",
    "    ymin = 0\n",
    "    xmax = (np.amax(surface.interpD) - np.amin(surface.interpD))/1000\n",
    "    ymax = (np.amax(surface.interpS) - np.amin(surface.interpS))/1000\n",
    "\n",
    "\n",
    "    xplot = (surface.interpS-np.amin(surface.interpS))/1000\n",
    "    yplot = (surface.interpD-np.amin(surface.interpD))/1000\n",
    "    zplot = surface.interpN\n",
    "    tri = plt.tripcolor(xplot, yplot, zplot)\n",
    "    tri.set_clim(cbarmin, cbarmax)\n",
    "    cbar = plt.colorbar()\n",
    "    cbar.set_label('Distance from plane (m)')\n",
    "    plt.title('Surface Position')\n",
    "    plt.xlabel('Strike distance (km)')\n",
    "    plt.ylabel('Dip distance (km)')\n",
    "\n",
    "    plt.text(ymin, xmin+shiftylabel, 'NW Base', \n",
    "             verticalalignment = 'top',\n",
    "             horizontalalignment = 'center')\n",
    "    plt.text(ymax, xmin+shiftylabel, 'SE Base', \n",
    "             verticalalignment = 'top',\n",
    "             horizontalalignment = 'center')\n",
    "\n",
    "    ax = plt.gca()\n",
    "    # ax.set(xlim = [xmin, xmax], ylim = [ymin, ymax])\n",
    "\n",
    "\n",
    "    plt.show()\n",
    "plotSurface(surface)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Figure 5\n",
    "Figure 5. Also available interactively online at https://plot.ly/~BrennanBrunsvik/12/. This shows the interpolated fault surface compared to locations of hypocenters. The images face approximately north-west. Green represents the hypocenters that were part of the cluster that contained the mainshock. Red shows all other hypocenters. (b) Shows two challenges for the fault surface interpolation. First, there are about three conjugate fault like trends in the aftershocks which were clustered as the L’Aquila fault. Second, the aftershocks and surface interpolation have low dip near the vertical top of the south-east section of the Paganica fault. This could be a separate and intersecting fault."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-05T20:38:42.004976Z",
     "start_time": "2019-05-05T20:38:31.905012Z"
    }
   },
   "outputs": [],
   "source": [
    "if True or runAllFigures:\n",
    "    surface.figure3DSurface(otherx=db2.x[~booLaqdb], othery=db2.y[~booLaqdb], otherz=db2.z[~booLaqdb],\n",
    "                          pointSize=.7 * 2, pLineWidthR = 1/10) \n",
    "#                             ,save = True, saveName = 'Figure 5')\n",
    "### This 3D figure looks the same as when things worked, 020102020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-05T20:38:43.673802Z",
     "start_time": "2019-05-05T20:38:42.013470Z"
    }
   },
   "outputs": [],
   "source": [
    "# Make copy of old surface object so it doesn't get broken\n",
    "# use it for comparing focal mechanisms. One for our surface + one for Cirella et al. (2012)\n",
    "\n",
    "# Compare focal to a the planar fault plane of Cirella\n",
    "planeVsSurface = copy.deepcopy(surface)\n",
    "# input strike, dip, rake, get normals, use these for comparison\n",
    "altSurfaceNormal, __=faultUnitVectors(np.array([2.32128791]),\n",
    "                 np.array([0.9424778]), \n",
    "                 np.array([0])\n",
    "                )\n",
    "planeVsSurface.compareFocal(altSurfaceNormal=altSurfaceNormal)\n",
    "\n",
    "\n",
    "\n",
    "# Compare focal to our 3D surface\n",
    "unmodified = copy.deepcopy(surface)\n",
    "unmodified.compareFocal()\n",
    "\n",
    "# the dot products has been completed.\n",
    "\n",
    "# create weights for Figure 5\n",
    "mag = db2.mL[db2.labels==0][unmodified.focalAvailable]\n",
    "weights = 10 ** mag \n",
    "bN = weights!=np.amax(weights)  # Boolean which excludes mainshock because it otherwise has too much weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Figure 6\n",
    "Figure 6. The angle between the normal of the interpolated fault surface and the normal of the available aftershock focal mechanisms. (a) Orientation compared to location of hypocenters in the cluster that corresponds to the L’Aquila fault. Location is distance along strike and along dip on the fault surface. Only the best matching angle for each aftershock is used for (a). (b) Histograms of the angles from the normal of the fault surface to the normals of the aftershock planes. The histograms are normalized so that their integrals are 1. We show both our 3D fault model and the planar model of Cirella et al. (2012). Both potential planes of each focal mechanism were included. This results in alignment peaks near 90° that correspond with the auxiliary planes of focal mechanisms. For the fault planes of the focal mechanisms, alignment peaks are closer to 20°. We show histograms both as unweighted and as weighted by 10M. Orientation match is increased when this is done. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-05T20:38:45.412936Z",
     "start_time": "2019-05-05T20:38:43.678072Z"
    }
   },
   "outputs": [],
   "source": [
    "# Figure 6a\n",
    "### ! the dip distance has flipped since previous submission: 02012020, as well as the distance being multiplied by -1\n",
    "matplotlib.rcParams.update({'font.size': 12})\n",
    "boo = unmodified.focalAvailable\n",
    "plt.figure(figsize=(15,7))\n",
    "plt.axes().set_aspect('equal')\n",
    "\n",
    "x = (unmodified.pS[boo]-np.amin(unmodified.pS) )/1000\n",
    "y = (unmodified.pD[boo]-np.amin(unmodified.pD) )/1000\n",
    "\n",
    "plt.scatter(x, y, c=unmodified.thetaLeast*180/np.pi,\n",
    "        vmin = 0, vmax = 90, linewidths=.3, edgecolors='k',\n",
    "           s = 15)#3**mag)\n",
    "\n",
    "shiftylabel = -2\n",
    "plt.text(np.amin(x), np.amin(y)+shiftylabel, 'NW Base', \n",
    "         verticalalignment = 'top',\n",
    "         horizontalalignment = 'right')\n",
    "plt.text(np.amax(x), np.amin(y)+shiftylabel, 'SE Base', \n",
    "         verticalalignment = 'top',\n",
    "         horizontalalignment = 'left')\n",
    "\n",
    "cbar = plt.colorbar()\n",
    "cbar.set_label('Angle $\\degree$ from 3D surface')\n",
    "plt.xlabel('Strike distance (km)')\n",
    "plt.ylabel('Dip distance (km)')\n",
    "plt.title('Orientation Mismatch')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-05T20:38:49.036835Z",
     "start_time": "2019-05-05T20:38:45.423962Z"
    }
   },
   "outputs": [],
   "source": [
    "matplotlib.rcParams.update({'font.size': 11})\n",
    "fig, axs = plt.subplots(2, 2, figsize=(12,7))\n",
    "\n",
    "axs[0, 0].hist(\n",
    "    np.array([unmodified.theta1[bN], unmodified.theta2[bN]]).ravel()*180/np.pi, \n",
    "         bins = 50, range=(0, 180), density = True # Unmodified has the wrong theat 02012020\n",
    "        )\n",
    "axs[0, 1].hist(np.array([planeVsSurface.theta1[bN], planeVsSurface.theta2[bN]]).ravel()*180/np.pi, \n",
    "         bins = 50, range=(0, 180), density = True\n",
    "        )\n",
    "axs[1,0].hist(np.array([unmodified.theta1[bN], unmodified.theta2[bN]]).ravel()*180/np.pi, \n",
    "         bins = 50, range=(0, 180), density=True,\n",
    "         weights=np.array([weights[bN], weights[bN]]).ravel()\n",
    "         )\n",
    "axs[1,1].hist(np.array([planeVsSurface.theta1[bN], planeVsSurface.theta2[bN]]).ravel()*180/np.pi, \n",
    "         bins = 50, range=(0, 180), density = True,\n",
    "         weights=np.array([weights[bN], weights[bN]]).ravel()\n",
    "        )\n",
    "\n",
    "rangeTopFig = (0, .015)\n",
    "rangeBotFit = (0, .05)\n",
    "for i in range(2):\n",
    "    axs[0, i].set_ylim(rangeTopFig)\n",
    "for i in range(2):\n",
    "    axs[1, i].set_ylim(rangeBotFit)\n",
    "\n",
    "for i in range(2): \n",
    "    for j in range(2):\n",
    "        axs[i, j].tick_params(labelleft=False) \n",
    "\n",
    "axs[0,0].set_title('3D Model', y = 1)\n",
    "axs[0,1].set_title('Planar Model', y = 1)\n",
    "\n",
    "for i in range(2):\n",
    "    axs[1, i].set_xlabel('Angle')\n",
    "    \n",
    "axs[0, 0].set_ylabel('Mechanisms, Unweighted')\n",
    "axs[1, 0].set_ylabel('Mechanisms, Weighted $M^{10}$')\n",
    "\n",
    "for i in range(axs.shape[0]):\n",
    "    for j in range(axs.shape[1]):\n",
    "        axs[i, j].set_xticklabels(['{:.1f}$\\degree$'.format(x) for x in plt.gca().get_xticks()])\n",
    "\n",
    "plt.tight_layout(pad = 1.3)\n",
    "fig.suptitle(\"Angle from Focal Mechanism to Mainshock Model\", fontsize=\"x-large\", y = 1.05)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#02012020 this test indicates that the normal vectors are about right for the 3d surface. \n",
    "# I think the problem is just with the positions of points. \n",
    "altSurfaceNormal\n",
    "\n",
    "n = unmodified.surfaceNormal()\n",
    "np.average(n, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cl = unmodified\n",
    "# t1keep = cl.theta1 < cl.theta2\n",
    "# t = cl.theta2\n",
    "# t[t1keep] = cl.theta1[t1keep]\n",
    "# plt.hist(t * 180 / np.pi, bins = 90)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Slip Inversion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We invert two co-seismic slip distribution models for the MW 6.1 L’Aquila mainshock: one slip distribution on our reconstructed 3D fault model, and another slip distribution on a 2D planar fault model. This allows for a direct evaluation of how the 3D fault morphology affects Coulomb stress. Slip distributions are inverted only with co-seismic GPS deformation measurements. GPS data includes 77 3-component GPS stations, both survey and continuous (Serpelloni et al., 2012). To calculate ground displacement, we use a Poissonian medium with the Lamé parameters equal to 26 GPa as in Serpelloni et al. (2012; see Di Luzio et al., 2009 for crustal model). \n",
    "\n",
    "Similar to Serpelloni et al. (2012), we invert for a planar fault with a homogenous slip distribution in order to obtain the fault planes location and orientation. This location and orientation is used for our planar fault with heterogeneous slip distribution. For the homogenous slip distribution inversion, we attempt to find the global minimum of the weighted residual sum of squares, $WRSS$, between the observed displacements, $d_o$, and modelled displacements, $d_m$, where\n",
    "$$WRSS = \\left( d_{o}−d_{m} \\right)^{T}\\cdot cov^{−1} \\cdot \\left( d_{o}−d_{m} \\right)$$\n",
    "and $cov$ is the covariance matrix of the GPS measurements. $cov$ is made assuming both that there is no correlation between measurements at different GPS stations, and there is no correlation between measurements in different directions at the same station. Thus, it is a diagonal matrix with only the variance of each GPS data. dm for this model and all planar models was calculated using homogeneous half-space cells (Okada, 1985; 1982) with the original Fortran code implemented as a Python wrapper (Thompson, 2017). We use the Basin-Hopping minimization algorithm from Scientific Python to find a global minimum WRSS for the non-linear problem (Jones et al., 2014). The parameters were simultaneously inverted: latitude, longitude, depth (positions are for the center of the fault), strike, dip, rake, slip, length in strike direction, and width in dip direction (Table 1). \n",
    "\n",
    "When inverting for the slip distribution on our 2D planar fault, it is helpful to increase the size of the fault to allow for slip to gently taper off toward the edges of the fault. The length of the fault in the strike direction was extended to 35 km, similar to the length of the inverted 3D fault surface. The width of the fault was left unmodified because the planes maximum depth (about 12 km) already extends beyond the maximum observed depth of hypocenters on the fault (about 11 km). \n",
    "\n",
    "To invert for slip distributions on both the planar and 3D fault models, the methods are very similar to each other except that the individual fault cells are dealt with differently (discussed later). We represent the slip distribution on faults using bi-cubic splines (Jones et al., 2014). A grid of nodes is distributed over the fault with variable slips, and the splines are used to interpolate for the slip between these nodes. Slip values at the base and sides of the fault are held at 0. Because the ground surface was co-seismically ruptured (Boncio et al., 2010; Roberts et al., 2010; Cinti et al., 2011), slip is allowed to vary at the top of the fault. There are 9 by 6 spline nodes in the strike and dip direction respectively. For the planar fault, 40 by 24 uniformly sized Okada (1985, 1992) type fault patches are distributed over the fault, and the slip on these patches is calculated based on the slip value of the splines in the centers of the fault patches. For the 3D fault, we test a similar 40 by 24 grid of Okada cells, but these rectangular cells cannot be meshed in 3D without creating gaps in the fault. Thus, we also use triangular dislocations (Nikkhoo and Walter, 2015) based on a Delaunay of 40 by 24 points (all results associated with the 3D fault model come from the triangular solution and not the Okada solution unless otherwise specified). The slip distributions inverted using Okada cells compared to triangular dislocations are only negligibly different. Stresses that are later found with these different methods (Figure 11) are also negligibly different with end results varying by ~1%. \n",
    "\n",
    "To prevent an unrealistically sharp slip distribution which overfits the data, some smoothing function is needed. For the fault models with variable slip distribution, instead of simply minimizing for $WRSS$, we minimize $WRSS+R$ where $R$ is a positive scalar that quantifies the roughness of the slip distribution. $R$ is defined here as:\n",
    "$$R = \\sum_{i=1}^{2} \\int \\left( \\nabla ^2 u_{i} \\right)^{2} dA$$\n",
    "where $u_i$ is the slip in the strike slip or dip slip direction and  is a smoothing factor that is manually chosen to determine how important the roughness of the slip distribution is relative to minimizing $WRSS$. There are many ways to quantify roughness. We chose a function based on Harris and Segal (1987). We chose  where there is a maximum trade-off between roughness and $WRSS$ (Figure S1). The choice of  is subjective here, but statistical methods can apply smoothing in a more objective way (Yabuki and Matsu'ura, 1992; Cambiotti et al., 2017). We used the BFGS minimization algorithm to find minimums of $WRSS+R$ (Jones et al., 2014)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Invert for mainshock model with homogenous slip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-05T20:38:49.087787Z",
     "start_time": "2019-05-05T20:38:49.044124Z"
    }
   },
   "outputs": [],
   "source": [
    "# Starting point for inverting fault with homogenous slip distribution\n",
    "# Starting parameters should be quite close to final ones, as the problem is \n",
    "#     non linear and difficult. This isn't an important point of our paper. \n",
    "\n",
    "Strike                   = np.array([ 2.259223])\n",
    "Dip                      = np.array([ 0.890402])\n",
    "Rake                     = np.array([-1.822806])\n",
    "Lon                      = np.array([ 13.42772])\n",
    "Lat                      = np.array([ 42.31982])\n",
    "Depth                    = np.array([-7072.831])\n",
    "Slip                     = np.array([ 0.601985])\n",
    "Strikeslip               = np.array([-0.150105])\n",
    "Dipslip                  = np.array([-0.582970])\n",
    "Tensile                  = np.array([ 0.      ])\n",
    "Strikelength             = np.array([ 13842   ])\n",
    "Diplength                = np.array([ 16098   ])\n",
    "Strikerectangles         = np.array([ 1       ])\n",
    "Verticalrectangles       = np.array([ 1       ])\n",
    "\n",
    "# make object for homogenous slip distribution inversion\n",
    "hom = inversionHom(iterations=1000, arraySaveName=variable_folder+'homogenousTests', rescale=True, method='CG')\n",
    "hom.getStartVals(Lon, Lat, Depth, Strike, Dip, Rake, Slip, \n",
    "           Strikelength, Diplength)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-05T20:38:49.235098Z",
     "start_time": "2019-05-05T20:38:49.093135Z"
    }
   },
   "outputs": [],
   "source": [
    "if True:\n",
    "    hom.loadResults()\n",
    "\n",
    "hom.iterations = 10\n",
    "if False:\n",
    "    hom.runOptimizeBH(ratioOfEpsilon=.2)\n",
    "\n",
    "if False:\n",
    "    hom.saveResults()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-05T20:38:50.480661Z",
     "start_time": "2019-05-05T20:38:49.243633Z"
    }
   },
   "outputs": [],
   "source": [
    "hom.testCS() # testCS here really just is used to find displacements at GPS stations\n",
    "hom.plotErrors()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Table 1. \n",
    "Table 1. The 9 parameters that were inverted to represent a rectangular fault model with homogenous slip for the L’Aquila mainshock. Parameters were inverted by matching modelled and observed co-seismic displacements at GPS stations. This position and orientation was then used to invert the slip distribution for our 2D fault model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-05T20:38:51.471544Z",
     "start_time": "2019-05-05T20:38:50.487750Z"
    }
   },
   "outputs": [],
   "source": [
    "# Table 1\n",
    "if True: \n",
    "    homLabels = ['Strike', 'Dip', 'Rake', 'Lon', \n",
    "                 'Lat', 'Depth (km)', 'Slip (cm)',\n",
    "                 'Strike Length (km)', 'Dip Length (km)']\n",
    "\n",
    "    homValsTable = [\n",
    "        np.around(hom.s*180/np.pi), \n",
    "        np.around(hom.d*180/np.pi), \n",
    "        np.around(hom.r*180/np.pi), \n",
    "        np.around(hom.lon, 3),     \n",
    "        np.around(hom.lat, 3), \n",
    "        np.around(-hom.depth/1000, 2), \n",
    "        np.around(hom.slip*100, 1),\n",
    "        np.around(hom.strikeLength/1000, 1), \n",
    "        np.around(hom.dipLength/1000, 1)\n",
    "                  ]\n",
    "\n",
    "    for i, thing in enumerate(homValsTable):\n",
    "        homValsTable[i] = float(thing)\n",
    "\n",
    "    values = [homLabels, homValsTable]\n",
    "    trace = go.Table(cells = dict(values = values))\n",
    "    data = [trace]\n",
    "    layout = dict(width=500)\n",
    "    fig = dict(data=data, layout=layout)\n",
    "    plot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-05T20:38:52.630141Z",
     "start_time": "2019-05-05T20:38:51.479307Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"This cell is just a test to make sure the code works.\n",
    "I test displacement and GPS station errors using a known fault \n",
    "model from Serpelloni et al. (2012).\n",
    "\n",
    "Within some range of error with varying geographic projects etc., \n",
    "our calculations seem to work.\"\"\"\n",
    "\n",
    "ser = inversionHom(\"Serpelloni's homogenously inverted parameters.\")\n",
    "\n",
    "ser.s = np.array([129.4 * np.pi / 180])\n",
    "ser.d = np.array([50.5 * np.pi / 180])\n",
    "ser.r = np.array([-105.0 * np.pi / 180.])\n",
    "\n",
    "ser.lon = np.array([13.387])\n",
    "ser.lat = np.array([42.285])\n",
    "ser.slip = np.array([.602]) \n",
    "\n",
    "ser.strikeLength = np.array([13900])\n",
    "ser.dipLength = np.array([16000]) \n",
    "ser.depth = - np.array([13200])\n",
    "\n",
    "ser.strikeRects = np.array([1])\n",
    "ser.verticalRects = np.array([1])\n",
    "\n",
    "ser.centerOrigin='bottomCenter'\n",
    "ser.slipComponents()\n",
    "ser.makeLists() # make list of these parameters for later code. \n",
    "\n",
    "ser.testCS()\n",
    "ser.plotErrors()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Cirella et al. (2012) model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-05T20:39:03.781270Z",
     "start_time": "2019-05-05T20:38:52.639183Z"
    }
   },
   "outputs": [],
   "source": [
    "cir = bilinearFaultImport()\n",
    "cir.loadModel(splineShape=(6, 9))\n",
    "cir.shiftKnots()\n",
    "cir.getMainshockPosition()\n",
    "cir.spGrid()\n",
    "\n",
    "cir.mechanicalConstants()\n",
    "if True:\n",
    "    cir.GPSConstants()\n",
    "    print('use GPSConstantsAlternate to use GPS data more similar to what was used in Cirella et al. (2012)')\n",
    "if False:\n",
    "    cir.GPSConstantsAlternate()\n",
    "cir.dummyPointSDR()\n",
    "\n",
    "cir.strikeRects = 40 \n",
    "cir.verticalRects = 24 \n",
    "\n",
    "cir.makeLists()\n",
    "cir.geogToMet()\n",
    "cir.coord2d()\n",
    "    \n",
    "cir.spGrid()\n",
    "cir.testCS()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-05T20:39:04.857891Z",
     "start_time": "2019-05-05T20:39:03.785323Z"
    }
   },
   "outputs": [],
   "source": [
    "cir.plotErrors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-05T20:39:06.915988Z",
     "start_time": "2019-05-05T20:39:04.862559Z"
    }
   },
   "outputs": [],
   "source": [
    "cir.spImageVector()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Heterogeneous slip planar fault"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-05T20:39:07.537702Z",
     "start_time": "2019-05-05T20:39:06.922726Z"
    }
   },
   "outputs": [],
   "source": [
    "# Multiple iterations with increasing number of nodes.\n",
    "heter = inversions()\n",
    "heter.importHomogenous(hom) # get geometry of previously inverted fault\n",
    "heter.getMainshockPosition() # project mainshock into a strike-dip coordinate system for plots\n",
    "\n",
    "# stretch strike length to accomadate for tapering fault\n",
    "heter.strikeLength = np.array([35e3])\n",
    "\n",
    "heter.mechanicalConstants()\n",
    "heter.GPSConstants()\n",
    "heter.dummyPointSDR()\n",
    "\n",
    "heter.strikeRects = 40\n",
    "heter.verticalRects = 24\n",
    "heter.setMinMax()\n",
    "\n",
    "heter.makeLists()\n",
    "heter.geogToMet()\n",
    "heter.ratio=2\n",
    "\n",
    "heter.coord2d()\n",
    "heter.sNodes = 9\n",
    "heter.dNodes = 6\n",
    "heter.spGrid()\n",
    "\n",
    "heter.spSlipsInit()\n",
    "heter.splineToCells()\n",
    "heter.prepLaplacian()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-05T20:39:08.522184Z",
     "start_time": "2019-05-05T20:39:07.552765Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load saved results, or run again. \n",
    "if False:\n",
    "    heter.constructLinearArrays()\n",
    "    \n",
    "heterLinearArrays = variable_folder+slip_dist_folder+'heterLinearArrays'\n",
    "\n",
    "if False:\n",
    "    heter.saveLinearArrays(heterLinearArrays)\n",
    "\n",
    "if True:\n",
    "    heter.loadLinearArrays(heterLinearArrays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-05T20:39:10.807177Z",
     "start_time": "2019-05-05T20:39:08.531482Z"
    }
   },
   "outputs": [],
   "source": [
    "if True:\n",
    "    iterationsEach = 100\n",
    "    \n",
    "    numOfb = 30\n",
    "    ball = 1e7 * 10**np.linspace(4, 0, numOfb) # for loading use 1e7 * 10**np.linspace(4, 0, numOfb) with numOfb = 30\n",
    "\n",
    "    heterBErr = [[],[],[]] # for comparing b to error\n",
    "\n",
    "\n",
    "    for i in np.arange(ball.size):\n",
    "        heter.b = ball[i]\n",
    "\n",
    "        heter.saveName = slip_dist_folder+'heter_slips_b_'+'%e' % heter.b # another folder is appended in class\n",
    "\n",
    "        if False:\n",
    "            heter.tol = .0024\n",
    "            heter.iterations=iterationsEach\n",
    "            heter.runErrHetBFGS()\n",
    "\n",
    "        if False:\n",
    "            heter.savesp() #8x4, 16x8\n",
    "\n",
    "        if True:\n",
    "            heter.loadsp()\n",
    "            \n",
    "        heter.splineToCells()\n",
    "        heter.getDisp()\n",
    "        heter.laplacianGrid()\n",
    "        heter.getWRSS()\n",
    "        heterBErr[0].append(heter.b); heterBErr[1].append(heter.WRSS); heterBErr[2].append(heter.smoothe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-05T20:39:11.348847Z",
     "start_time": "2019-05-05T20:39:10.812746Z"
    }
   },
   "outputs": [],
   "source": [
    "# part of figure s1\n",
    "\n",
    "plt.rcParams.update({'font.size':12})\n",
    "plt.figure(figsize=(7, 7))\n",
    "a = np.array(heterBErr)\n",
    "roughness =a[2]/a[0]*1e16 \n",
    "error = a[1]\n",
    "\n",
    "\n",
    "plt.scatter(roughness, error, c = 'k')\n",
    "\n",
    "indChoice = 15\n",
    "plt.annotate(s = r'$\\beta$ = '+'%e' % a[0, indChoice], xy = (roughness[indChoice], error[indChoice]))\n",
    "\n",
    "plt.scatter(roughness[indChoice], error[indChoice], c = 'red')\n",
    "\n",
    "plt.xlabel('Roughness')\n",
    "plt.ylabel('WRSS')\n",
    "\n",
    "plt.title('Tradeoff Curve Planar Fault')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-05T20:39:11.430878Z",
     "start_time": "2019-05-05T20:39:11.355348Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load the model with chosen b value\n",
    "heter.b = ball[indChoice] \n",
    "heter.saveName = slip_dist_folder+'heter_slips_b_'+'%e' % heter.b # another folder is appended in class\n",
    "heter.loadsp()\n",
    "heter.splineToCells()\n",
    "heter.getDisp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-05T20:39:12.538997Z",
     "start_time": "2019-05-05T20:39:11.437916Z"
    }
   },
   "outputs": [],
   "source": [
    "heter.plotErrors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-05T20:39:14.184328Z",
     "start_time": "2019-05-05T20:39:12.544165Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "heter.spImageVector(vmax=1.2, figsize = (15, 15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-05T20:39:14.202306Z",
     "start_time": "2019-05-05T20:39:14.190524Z"
    }
   },
   "outputs": [],
   "source": [
    "heterF = copy.deepcopy(heter) # make a copy of that object so we don't mess it up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Invert slip on 3D fault surface using OKADA\n",
    "This is just to compare to the triangular dislocation results. The Okada based results are not reused for the 3D surface results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-05T20:39:15.405074Z",
     "start_time": "2019-05-05T20:39:14.208927Z"
    }
   },
   "outputs": [],
   "source": [
    "surf = inversionCurved(fName = 'surfaceArraysCoarse')\n",
    "\n",
    "surf.saveName='surface_spline_slips'\n",
    "surf.savePath=variable_folder\n",
    "\n",
    "surf.temporarySplineBoundary()\n",
    "surf.spGrid(9, 6)\n",
    "surf.spSlipsInit(ss=-.122, ds=-.495) # Start with values of homogenous inversion\n",
    "\n",
    "surf.getMainshockPosition()\n",
    "surf.makeListInitial()\n",
    "surf.importShapeSplines(surface)\n",
    "surf.prepLaplacian()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-05T20:39:16.365692Z",
     "start_time": "2019-05-05T20:39:15.417157Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load saved results, or run again. \n",
    "if False:\n",
    "    surf.constructLinearArrays()\n",
    "    \n",
    "surfLinearArrays = variable_folder+slip_dist_folder+'surfLinearArrays'\n",
    "\n",
    "if False:\n",
    "    surf.saveLinearArrays(surfLinearArrays)\n",
    "\n",
    "if True:\n",
    "    surf.loadLinearArrays(surfLinearArrays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-05T20:39:18.434590Z",
     "start_time": "2019-05-05T20:39:16.370231Z"
    }
   },
   "outputs": [],
   "source": [
    "if True:\n",
    "    iterationsEach = 100\n",
    "    \n",
    "    numOfb = 30\n",
    "    ball = 1e7 * 10**np.linspace(4, 0, numOfb)\n",
    "    \n",
    "\n",
    "    surfBErr = [[],[],[]] # for comparing b to error\n",
    "\n",
    "\n",
    "    for i in np.arange(ball.size):\n",
    "        surf.b = ball[i]\n",
    "\n",
    "        surf.saveName = slip_dist_folder+'surf_slips_b_'+'%e' % surf.b # another folder is appended in class\n",
    "\n",
    "        if False:\n",
    "            surf.tol = .0024\n",
    "            surf.iterations=iterationsEach\n",
    "            surf.runErrHetBFGS()\n",
    "\n",
    "        if False:\n",
    "            surf.savesp() #8x4, 16x8\n",
    "\n",
    "        if True:\n",
    "            surf.loadsp()\n",
    "            \n",
    "        surf.splineToCells()\n",
    "        surf.getDisp()\n",
    "        surf.laplacianGrid()\n",
    "        surf.getWRSS()\n",
    "        surfBErr[0].append(surf.b); surfBErr[1].append(surf.WRSS); surfBErr[2].append(surf.smoothe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-05T20:39:18.974049Z",
     "start_time": "2019-05-05T20:39:18.438728Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.rcParams.update({'font.size':12})\n",
    "plt.figure(figsize=(7, 7))\n",
    "b = np.array(surfBErr)\n",
    "roughness =b[2]/b[0]*1e16 \n",
    "error = b[1]\n",
    "\n",
    "\n",
    "plt.scatter(roughness, error, c = 'k')\n",
    "\n",
    "indChoice = 15\n",
    "plt.annotate(s = r'$\\beta$ = '+'%e' % b[0, indChoice], xy = (roughness[indChoice], error[indChoice]))\n",
    "plt.scatter(roughness[indChoice], error[indChoice], c = 'red')\n",
    "\n",
    "\n",
    "plt.xlabel('Roughness')\n",
    "plt.ylabel('WRSS')\n",
    "\n",
    "plt.title('Tradeoff Curve 3D Fault')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-05T20:39:19.050577Z",
     "start_time": "2019-05-05T20:39:18.981366Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load the model with chosen b value\n",
    "surf.b = ball[indChoice]\n",
    "surf.saveName = slip_dist_folder+'surf_slips_b_'+'%e' % surf.b # another folder is appended in class\n",
    "surf.loadsp()\n",
    "surf.splineToCells()\n",
    "surf.getDisp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-05T20:39:20.126479Z",
     "start_time": "2019-05-05T20:39:19.055772Z"
    }
   },
   "outputs": [],
   "source": [
    "surf.plotErrors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-05T20:39:21.844577Z",
     "start_time": "2019-05-05T20:39:20.132392Z"
    }
   },
   "outputs": [],
   "source": [
    "surf.spImageVector(vmax=1.2, figsize = (15, 15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-05T20:39:21.880633Z",
     "start_time": "2019-05-05T20:39:21.850484Z"
    }
   },
   "outputs": [],
   "source": [
    "surfF = copy.deepcopy(surf) # Make copy of object so we don't mess it up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Invert slip on 3D fault surface using Triangular Dislocations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-05T20:39:21.964299Z",
     "start_time": "2019-05-05T20:39:21.885834Z"
    }
   },
   "outputs": [],
   "source": [
    "surfTD = inversionCurvedTD(fName = 'surfaceArraysCoarseTD', cellType = 'TD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-05T20:39:22.038571Z",
     "start_time": "2019-05-05T20:39:21.968522Z"
    }
   },
   "outputs": [],
   "source": [
    "surfTD.saveName='surface_spline_slipsTD'\n",
    "surfTD.savePath=variable_folder\n",
    "\n",
    "surfTD.temporarySplineBoundary()\n",
    "surfTD.spGrid(9, 6)\n",
    "surfTD.spSlipsInit(ss=-.122, ds=-.495) # Start with values of homogenous inversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-05T20:39:23.248604Z",
     "start_time": "2019-05-05T20:39:22.044236Z"
    }
   },
   "outputs": [],
   "source": [
    "surfTD.getMainshockPosition()\n",
    "surfTD.makeListInitial()\n",
    "surfTD.importShapeSplines(surface)\n",
    "surfTD.prepLaplacian()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-05T20:39:24.159394Z",
     "start_time": "2019-05-05T20:39:23.252483Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load saved results, or run again. \n",
    "surfTDLinearArrays = variable_folder+slip_dist_folder+'surfTDLinearArrays'\n",
    "if False: \n",
    "    surfTD.saveSimplicesPermanent()\n",
    "    surfTD.constructLinearArraysTD()\n",
    "    \n",
    "if False: \n",
    "    surfTD.saveLinearArrays(surfTDLinearArrays)\n",
    "\n",
    "if True:\n",
    "    surfTD.loadLinearArrays(surfTDLinearArrays)\n",
    "    surfTD.loadSimplices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-05T20:39:26.353152Z",
     "start_time": "2019-05-05T20:39:24.163520Z"
    }
   },
   "outputs": [],
   "source": [
    "if True:\n",
    "    iterationsEach = 100\n",
    "    \n",
    "    numOfb = 30\n",
    "    ball = 1e7 * 10**np.linspace(4, 0, numOfb)\n",
    "    \n",
    "\n",
    "    surfTDBErr = [[],[],[]] # for comparing b to error\n",
    "\n",
    "\n",
    "    for i in np.arange(ball.size):\n",
    "        surfTD.b = ball[i]\n",
    "\n",
    "        surfTD.saveName = slip_dist_folder+'surfTD_slips_b_'+'%e' % surfTD.b # another folder is appended in class\n",
    "\n",
    "        if False: \n",
    "            surfTD.tol = .0024\n",
    "            surfTD.iterations=iterationsEach\n",
    "            surfTD.runErrHetBFGS()\n",
    "\n",
    "        if False: \n",
    "            surfTD.savesp()\n",
    "\n",
    "        if True: \n",
    "            surfTD.loadsp()\n",
    "            \n",
    "        surfTD.splineToCells()\n",
    "        surfTD.getDisp()\n",
    "        surfTD.laplacianGrid()\n",
    "        surfTD.getWRSS()\n",
    "        surfTDBErr[0].append(surfTD.b); surfTDBErr[1].append(surfTD.WRSS); surfTDBErr[2].append(surfTD.smoothe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-05T20:39:27.254331Z",
     "start_time": "2019-05-05T20:39:26.373160Z"
    }
   },
   "outputs": [],
   "source": [
    "# Part of figure s1\n",
    "\n",
    "plt.rcParams.update({'font.size':12})\n",
    "plt.figure(figsize=(7, 7))\n",
    "b = np.array(surfTDBErr)\n",
    "roughness =b[2]/b[0]*1e16 \n",
    "error = b[1]\n",
    "\n",
    "\n",
    "plt.scatter(roughness, error, c = 'k')\n",
    "\n",
    "indChoice = 15\n",
    "plt.annotate(s = r'$\\beta$ = '+'%e' % b[0, indChoice], xy = (roughness[indChoice], error[indChoice]))\n",
    "plt.scatter(roughness[indChoice], error[indChoice], c = 'red')\n",
    "\n",
    "\n",
    "plt.xlabel('Roughness')\n",
    "plt.ylabel('WRSS')\n",
    "\n",
    "plt.title('Tradeoff Curve 3D Fault')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-05T20:39:27.321357Z",
     "start_time": "2019-05-05T20:39:27.259464Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load the model with chosen b value\n",
    "surfTD.b = ball[indChoice]\n",
    "surfTD.saveName = slip_dist_folder+'surfTD_slips_b_'+'%e' % surfTD.b # another folder is appended in class\n",
    "surfTD.loadsp()\n",
    "surfTD.splineToCells()\n",
    "surfTD.getDisp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-05T20:39:30.096768Z",
     "start_time": "2019-05-05T20:39:27.325289Z"
    }
   },
   "outputs": [],
   "source": [
    "surfTD.plotErrors()\n",
    "\n",
    "surfTD.spImageVector(vmax=1.2, figsize = (15, 15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-05T20:39:30.119259Z",
     "start_time": "2019-05-05T20:39:30.101658Z"
    }
   },
   "outputs": [],
   "source": [
    "surfTDF = copy.deepcopy(surfTD) # Make copy of object so we don't mess it up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Note that Figure 6 and 7 are repeated here with parts a-c combined for easy comparison while these sub-parts were seperated in the methods section)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final results for the fault model inversion with a homogenous slip distribution can be seen in Table 1. For a comparison to other planar models, see Serpelloni et al. (2012) and references therein. Our parameters are similar to those obtained from other studies. \n",
    "\n",
    "Misfit between modelled and GPS measured co-seismic displacements can be seen in Figure 7 and Table 2. Inverted slip distributions can be seen in Figure 8. For comparison, we include the jointly inverted planar model of Cirella et al. (2012) and approximate their model using a grid of homogenous half-spaces (Okada, 1985, 1992). Because the joint model was originally inverted using a 3D mechanical model, our half-space approximation may become misrepresentative for stations close to the fault. Further, because of errors in GPS measurements, the inclusion of strong-motion and InSAR data decreases displacement fit at GPS stations even though the model becomes more realistic. The jointly inverted planar model thus has a slightly higher misfit than the purely GPS inverted models. For the purely GPS based inversions, we find that our 3D mainshock model fits worse for horizontal data and better for vertical data compared to our planar model. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Figure 7\n",
    "Figure 7. Modelled (blue) and observed (red) displacements at GPS stations produced from the jointly inverted model by Cirella et al. (2012), our 2D planar model with GPS based slip distribution inversion, and our 3D model with GPS based slip distribution inversion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-05T20:39:30.197410Z",
     "start_time": "2019-05-05T20:39:30.136519Z"
    }
   },
   "outputs": [],
   "source": [
    "def plotDxyz(ax, lat, lon, dxO, dxM, dyO, dyM):\n",
    "    \"\"\"plot vectors dx and dy (observed and modeled). \n",
    "    \n",
    "    lat and lon are position of vector tails.\n",
    "    dx and dy are x and y displacements.\n",
    "    O or M signifies observed and modelled values.\"\"\"\n",
    "    ones = np.ones(lat.shape)\n",
    "    lat = lat * ones\n",
    "    lon = lon * ones\n",
    "    dxO = dxO * ones\n",
    "    dxM = dxM * ones\n",
    "    dyO = dyO * ones\n",
    "    dyM = dyM * ones\n",
    "    \n",
    "    scalePlot = .65\n",
    "    \n",
    "    edgecolor='black'\n",
    "    linewidth=.5\n",
    "    ax.set_xlim(13.1, 13.8)\n",
    "    ax.set_ylim(42.1, 42.6)\n",
    "    \n",
    "    #scale bar\n",
    "    xbar = np.array([13.2])\n",
    "    ybar = np.array([42.15])\n",
    "    barup = np.array([.05])\n",
    "    barside = np.array([0])\n",
    "    co = np.ones(lon.size, dtype = str)\n",
    "    cm = np.ones(lon.size, dtype = str)\n",
    "    cb = np.ones(1, dtype = str)\n",
    "    co[:] = 'r'\n",
    "    cm[:] = 'b'\n",
    "    cb[:] = 'k'\n",
    "    \n",
    "    xall = np.append(np.append(lon, lon), xbar)\n",
    "    yall = np.append(np.append(lat, lat), ybar)\n",
    "    dxall= np.append(np.append(dxO, dxM), barside)\n",
    "    dyall= np.append(np.append(dyO, dyM), barup)\n",
    "\n",
    "    colors = np.append(np.append(co, cm), cb)\n",
    "    \n",
    "    ax.quiver(xall, yall, dxall, dyall, color = colors,\n",
    "             scale_units = 'x', scale = scalePlot,\n",
    "             angles = 'uv', edgecolor=edgecolor, linewidth=linewidth)\n",
    "    \n",
    "    ax.annotate('  5 cm', (xbar, ybar))\n",
    "    \n",
    "    # A scatter plot to show vector tail locations\n",
    "    ax.scatter(lon, lat, c='k', s=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-05T20:39:30.309303Z",
     "start_time": "2019-05-05T20:39:30.203822Z"
    }
   },
   "outputs": [],
   "source": [
    "def plotErrors(self, label=None):\n",
    "#     fig = plt.figure(figsize=(15, 15))\n",
    "#     axs = fig.add_subplot(122, aspect='equal')\n",
    "    fig, axs = plt.subplots(1, 2)#, sharex=True)\n",
    "    fig.set\n",
    "    fig.subplots_adjust(left=0, right=2, bottom=0, top = 1)\n",
    "#     fig.subplots\n",
    "#     axs.reshape()\n",
    "    if label is not None:\n",
    "        axs[0].set_ylabel(label)\n",
    "    axs[0].set_title('Horizontal (mm)')\n",
    "    axs[1].set_title('Vertical (mm)')\n",
    "    plotDxyz(axs[0], self.latP, self.lonP, self.dxMeas, self.dx, self.dyMeas,self.dy)\n",
    "    plotDxyz(axs[1], self.latP,self.lonP, 0, 0, self.dzMeas, self.dz)\n",
    "\n",
    "    for i in range(2):\n",
    "        axs[i].set_yticklabels(['{:.1f}$\\degree$'.format(x) for x in plt.gca().get_yticks()])\n",
    "        axs[i].set_xticklabels(['{:.1f}$\\degree$'.format(x) for x in plt.gca().get_xticks()])\n",
    "\n",
    "    plt.show()\n",
    "    print('rms residual for dx, dy, dz = ',rmsResiduals(self.dxMeas, self.dyMeas, self.dzMeas, \n",
    "                 self.dx, self.dy, self.dz) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-05T20:39:33.627168Z",
     "start_time": "2019-05-05T20:39:30.315813Z"
    }
   },
   "outputs": [],
   "source": [
    "# Figure 7\n",
    "\n",
    "if False or runAllFigures:\n",
    "    matplotlib.rcParams.update({'font.size': 12})\n",
    "    mods = [cir, heterF, surfTDF]\n",
    "    titles = ['Planar Joint', 'Planar GPS', '3D GPS']\n",
    "\n",
    "    for i, model in enumerate(mods):\n",
    "    #     fig, ax = fig, axs = plt.subplots(4, 2, sharex=True)\n",
    "#         if titles[i] != '3D GPS':\n",
    "#             model.testCS()\n",
    "#         else:\n",
    "#             model.testCSTD()\n",
    "        model.plotErrors = plotErrors\n",
    "        model.plotErrors(model, label = titles[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Table 2\n",
    "Table 2. Same error as in Figure 7. Values are RMS residual mismatch between observed and modelled co-seismic displacements (mm)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-05T20:42:40.497248Z",
     "start_time": "2019-05-05T20:39:33.633862Z"
    }
   },
   "outputs": [],
   "source": [
    "if False or runAllFigures:\n",
    "\n",
    "    # Table 2\n",
    "    models = [cir, heterF, surfTDF]\n",
    "    labels = ['Planar Joint', 'Planar GPS', '3D GPS']\n",
    "    errors = np.zeros((len(models), 3))\n",
    "    for i, mod in enumerate(models):\n",
    "        if labels[i] != '3D GPS':\n",
    "            model.testCS()\n",
    "        else:\n",
    "            model.testCSTD()\n",
    "        xerror, yerror, zerror = rmsResiduals(\n",
    "            mod.dx, mod.dy, mod.dz, mod.dxMeas, mod.dyMeas, mod.dzMeas)\n",
    "        errors[i,:] = np.array([xerror, yerror, zerror])\n",
    "\n",
    "\n",
    "        errorsplot = np.around(errors.T*1000, decimals=2).tolist()\n",
    "        errorsplot.insert(0, labels)\n",
    "\n",
    "        # layout = go.Layout(title='Title')\n",
    "    trace = go.Table(\n",
    "        header=dict(values=['Model', 'East Error (mm)', 'North Error (mm)', 'Vertical Error (mm)']),\n",
    "        cells=dict(values= errorsplot ) )\n",
    "    layout = dict(width=800, height=300)\n",
    "    data = [trace] \n",
    "    fig = dict(data=data, layout=layout)\n",
    "    plot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Figure 8\n",
    "Figure 8. These are the slip distributions we consider for the 2009 L’Aquila mainshock. Because each rupture model is in a different location, the mainshocks hypocenter should be used as a reference when comparing slip distributions. The location of this hypocenter is represented with a star. Vectors show co-seismic displacement on the hanging wall relative to the footwall. (a) Slip distribution jointly inverted by Cirella et al. (2012), (b) slip distribution on our inverted 2D fault plane (c) slip distribution inverted with our 3D fault."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-05T20:42:45.771675Z",
     "start_time": "2019-05-05T20:42:40.504910Z"
    }
   },
   "outputs": [],
   "source": [
    "# Figure 8\n",
    "\n",
    "matplotlib.rcParams.update({'font.size': 16})\n",
    "mods = [cir, heterF, surfTDF]\n",
    "titles = ['Planar Joint', 'Planar GPS', '3D GPS']\n",
    "for i, mod in enumerate(mods):\n",
    "    def spImageVector(self, xVectors=12, yVectors = 6, xPoints=200, \n",
    "                      yPoints=100, showMainshock=True, vmin=0, vmax = 1.2, \n",
    "                      shiftLabels=True, figsize = (15, 15), title = None ):\n",
    "        \"\"\" replacing old spline to cells, using interp2d\"\"\"\n",
    "        fig = plt.figure(figsize=figsize)\n",
    "        self.splineToCells()\n",
    "\n",
    "        xLin = np.linspace( self.minS, self.maxS, \n",
    "                                 num=xPoints)\n",
    "        xLoc = np.outer( xLin, np.ones(yPoints) )\n",
    "\n",
    "        yLin = np.linspace( self.minD, self.maxD, \n",
    "                                 num=yPoints)\n",
    "        yLoc = np.outer( np.ones(xPoints), yLin )\n",
    "\n",
    "\n",
    "\n",
    "        xLinV = np.linspace( self.minS, self.maxS, \n",
    "                                 num=xVectors)\n",
    "        xLocV = np.outer( xLinV, np.ones(yVectors) )\n",
    "\n",
    "        yLinV = np.linspace( self.minD, self.maxD, \n",
    "                                 num=yVectors)\n",
    "        yLocV = np.outer( np.ones(xVectors), yLinV )\n",
    "\n",
    "\n",
    "        ssIm = self.rbfss(xLin, yLin)\n",
    "        dsIm = self.rbfds(xLin, yLin)\n",
    "\n",
    "        ssImV = self.rbfss(xLinV, yLinV)\n",
    "        dsImV = self.rbfds(xLinV, yLinV)\n",
    "\n",
    "        extent = np.array([np.amin(xLin), np.amax(xLin), np.amin(yLin), np.amax(yLin)])\n",
    "        if shiftLabels:\n",
    "            extentShift = np.array([-self.mainS, -self.mainS, -self.mainD, -self.mainD]).ravel()\n",
    "            extent+=extentShift\n",
    "            xLocV-=self.mainS\n",
    "            yLocV-=self.mainD\n",
    "            \n",
    "            extent*=1/1000\n",
    "            xLocV*=1/1000\n",
    "            yLocV*=1/1000\n",
    "\n",
    "        im = plt.imshow(np.sqrt(ssIm.T**2+dsIm.T**2), cmap='YlOrBr', \n",
    "                   origin='lower', extent=extent, vmin = vmin, vmax = vmax,\n",
    "                   interpolation='bicubic')\n",
    "        if showMainshock:\n",
    "            if shiftLabels:\n",
    "                mainS=0\n",
    "                mainD=0\n",
    "            elif not shiftLabels:\n",
    "                mainS = self.mainS\n",
    "                mainD = self.mainD\n",
    "            try:\n",
    "                plt.scatter(mainS, mainD, marker=\"*\", s=500, c='yellow', linewidths=1, edgecolors='k')\n",
    "            except: print('No mainshock position')\n",
    "        plt.quiver(xLocV, yLocV, ssImV, dsImV, scale=10)\n",
    "        \n",
    "        cb = plt.colorbar(im, orientation='horizontal', fraction=0.026, pad=0.06)\n",
    "        \n",
    "        cb.set_label('Slip (m)')\n",
    "        plt.xlabel('Strike Distance (km)')\n",
    "        plt.ylabel('Dip Distance (km)')\n",
    "        yshift = -.75\n",
    "        plt.text(np.amin(xLocV), np.amin(yLocV)+yshift, 'NW Base', \n",
    "                 verticalalignment = 'top',\n",
    "                 horizontalalignment = 'center')\n",
    "        plt.text(np.amax(xLocV), np.amin(yLocV)+yshift, 'SE Base', \n",
    "                 verticalalignment = 'top',\n",
    "                 horizontalalignment = 'center')\n",
    "        \n",
    "        if not title is None:\n",
    "            plt.title(title)#, loc = 'right')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    mod.spImageVector = spImageVector\n",
    "    mod.spImageVector(mod, title = titles[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 Coulomb Stress"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Static stress change is believed to be one of the main causes of earthquake interaction, and ∆CFF is commonly used to quantify the promotion of fault failure due to static stress change (e.g. Stein, 1999 and references therein). To understand the significance of incorporating an earthquake 3D morphology into stress calculations, and to better understand earthquake interaction and the source process for the L’Aquila seismic sequence and earthquakes in general, we compare the stresses that result from each mainshock model. We use a dataset with 3422 focal mechanisms (Aldersons et al., 2009) in order to find the ∆CFF that the mainshock generated. These focal mechanisms were calculated using the FPFIT method (USGS Open-File Report 85-739), which finds a source model which minimizes first-motion polarity discrepancies. \n",
    "\n",
    "Coulomb stress is defined as\n",
    "$$CFF=\\tau+\\mu \\left(\\sigma_n+P\\right)$$ \n",
    "where $CFF$ is the Coulomb stress, $\\tau$ is the shear stress across the fault in the direction of slip, $\\mu$ is the coefficient of friction across the fault, $\\sigma_n$ is the normal stress on the fault (we use positive as tension), and P is the pore pressure. $CFF$ can be used for quantitative analysis showing how much the stress on a fault contributes to rupture. This compares the maximum sustainable shear stress on a fault to the actual shear stress where positive $CFF$ suggests fault failure.  Because we only look at how the earthquake contributes to failure, we find\n",
    "$$\\Delta CFF = \\Delta \\tau + \\mu \\left( \\Delta \\sigma_n + \\Delta P \\right)$$\n",
    "\n",
    "To account for pore pressures response to volumetric change, the change in pore pressure is treated as proportional to the volumetric stress, \n",
    "$$ \\Delta P = − B \\frac{tr\\left(\\Delta \\sigma\\right)}{3} $$\n",
    ", where $B$ is Skempton’s coefficient (Cocco and Rice, 2002; Makhnenko and Labuz, 2013). This formula is sufficiently accurate while not introducing many new, unknown constants. We assumed $B=0.45$ and $\\mu = 0.6$. Within reasonable variation, stress results are only minimally sensitive to the values of $B$ and $\\mu$. Various sets of constants have been used for the L’Aquila sequence. For instance, Terakawa et al. (2010) used $\\mu = 0.6$ and $B=0.6$, Serpelloni et al. (2012) used $\\mu= 0.75$ and $B = 0.47$, Walters et al. (2009) used an effective coefficient of friction $\\mu ‘ = 0.6$ (which takes both $\\mu$ and $B$ into account), but had similar results with $\\mu ‘ = 0.4$. \n",
    "\n",
    "In order to find ∆CFF, the strike, dip, and rake of a fault must be known. Because it is generally not possible to distinguish between the fault plane and auxiliary plane in an aftershock’s focal mechanism, we calculate ∆CFF on both planes as in Nostro et al. (2005). We analyze whether ∆CFF results are positive on neither, only one, or both planes. This prevents the need to assume that one plane in a focal mechanism is the true fault plane; such a choice could easily be wrong. However, care must be taken to interpret our results when ∆CFF is calculated to be positive on only one plane in a focal mechanism as that plane may not be the actual fault plane. \n",
    "\n",
    "We test ∆CFF both on the actual focal mechanisms of aftershocks (Aldersons et al., 2009) and on planes that are optimally oriented for failure (OOPs). There are several advantages to analyzing OOPs: (i) the actual focal mechanism inversion process has errors which could be systematic, (ii) there are many well located events (47,917, about 93% of the catalog) which do not have focal mechanisms which can be analyzed only if we assume OOPs, (iii) the spatial distribution of events, and thus sampled ∆CFF, is greater if more events are used, and (iv) smaller events which tend not to have focal mechanisms are generally better located than large events due to the complexities in cross-correlating the waveforms of large events. The orientations of OOPs are found by maximizing ∆CFF given the combination of regional stress and mainshock induced stress (King, Stein, and Lin, 1994). We adopt the regional stress given by Chiaraluce et al. (2003). We also find ∆CFF on a grid of OOPs for visualization. The orientations of these OOPs can be seen in Figure S2. \n",
    "\n",
    "There are also advantages to using only the actual focal mechanisms. (i) Inaccuracies in regional stress knowledge affect OOP orientations. This can be orientation error, but both the magnitude of the deviatoric stress and the depth dependence of the lithostatic stress are poorly known. (ii) Failure orientation is not only controlled by stress, but also pre-existing weaknesses and rheology. For instance, thrust faults that exist from the complex evolution of stress in the Apennines (Malinverno and Ryan, 1986; Mariucci et al., 1999; Jolivet and Faccenna, 2000) may activate at orientations much different than OOPs. The combination of testing ∆CFF on OOPs and actual focal mechanisms provides the most complete picture of ∆CFF development.  \n",
    "\n",
    "Although induced stress is not constant in time (e.g. Verdecchia et al., 2018) we assumed that induced stress does not change after it is generated. We test this assumption by modelling stresses generated not only from the mainshock, but also from several large aftershocks (Figure S3). We find that aftershock slip distributions are not constrained enough to find reliable results. Thus, incorporating their stresses decreases modelled ∆CFF before rupture. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-05T20:42:46.315653Z",
     "start_time": "2019-05-05T20:42:45.776847Z"
    }
   },
   "outputs": [],
   "source": [
    "# first, define some functions for finding OOP orientations\n",
    "def plungeTrendToUnitVec(plunge, trend):\n",
    "    \"\"\"Radians ALWAYS!\"\"\"\n",
    "    \n",
    "    x = np.cos(plunge) * np.sin(trend)\n",
    "    y = np.cos(plunge) * np.cos(trend)\n",
    "    z = np.sin(plunge)\n",
    "    \n",
    "    return np.array([x, y, z]).T\n",
    "\n",
    "def hydroStaticStress(row, g, z):\n",
    "    points = z.size\n",
    "    \n",
    "    meanStress = row * g * z\n",
    "    if (meanStress > 0).any():\n",
    "        print(\"shouldn't all mean stress be < 0?\")\n",
    "        \n",
    "    idents = np.zeros((points, 3, 3))  \n",
    "    ident = np.identity(3)\n",
    "    \n",
    "    for i in np.arange(points):\n",
    "        idents[i] += ident * meanStress[i]\n",
    "    \n",
    "    return idents\n",
    "\n",
    "def rotateTensFull(ten, Q):\n",
    "    \"\"\"Q is array of orthogonal unit vectors of new coordinate system\"\"\"\n",
    "    if ten.shape == (3, 3):\n",
    "        ten.reshape((1, 3, 3))\n",
    "    \n",
    "    points = ten.shape[0]\n",
    "                    \n",
    "\n",
    "    if Q.shape == (3, 3):\n",
    "        a = np.zeros(ten.shape)\n",
    "        a[:] = Q       \n",
    "        Q = a\n",
    "        \n",
    "    tenPrime = np.zeros(ten.shape)    \n",
    "    for i in np.arange(points):\n",
    "        tenPrime[i] = np.dot(Q[i], np.dot(ten[i], Q[i].T))\n",
    "    \n",
    "    return tenPrime\n",
    "\n",
    "\n",
    "\n",
    "def findOOP(deltaStress, dev, densityRock, z):\n",
    "    QPrime = plungeTrendToUnitVec(plunge, trend)\n",
    "    QPrimeInv = QPrime.T\n",
    "\n",
    "    g = 9.81\n",
    "    hydroStress = hydroStaticStress(densityRock, g, z)\n",
    "\n",
    "    regStressPrime = hydroStress + dev\n",
    "    regStress = rotateTensFull(regStressPrime, QPrimeInv)\n",
    "\n",
    "    totalStress = regStress + deltaStress\n",
    "\n",
    "    eigVal, eigVec = np.linalg.eig(totalStress)\n",
    "    for i in np.arange(eigVec.shape[0]): # it gives eigVec in the wrong order for me. \n",
    "        eigVec[i] = eigVec[i].T\n",
    "\n",
    "    # Sort, from least to greatest (because most compression gives most negative number, this is actually sorting principal stresses from least to greatest)\n",
    "    for i in np.arange(eigVec.shape[0]):\n",
    "        sort = np.argsort(eigVal[i])\n",
    "        eigVal[i] = eigVal[i][sort]\n",
    "        eigVec[i] = eigVec[i][sort]\n",
    "\n",
    "    failAng = np.arctan(1/coefFrict) / 2 # from king\n",
    "    if len(failAng.shape) != 2:\n",
    "        failAng = np.array(failAng * np.ones(eigVec.shape[0]))\n",
    "\n",
    "    # I designate normal in direction failAng + 90. Don't change! Later, I check if this actually faces hanging wall and then correct it\n",
    "\n",
    "    oop1primen = np.array(\n",
    "        [np.cos( failAng + np.pi/2), np.zeros(failAng.shape), np.sin( failAng + np.pi/2)] ).T\n",
    "    oop2primen = np.array(\n",
    "        [np.cos(-failAng - np.pi/2), np.zeros(failAng.shape), np.sin(-failAng - np.pi/2)] ).T\n",
    "\n",
    "    oop1primes = np.array([np.cos( failAng), np.zeros(failAng.shape), np.sin( failAng)]).T\n",
    "    oop2primes = np.array([np.cos(-failAng), np.zeros(failAng.shape), np.sin(-failAng)]).T\n",
    "\n",
    "    oop1n = np.zeros(oop1primen.shape)\n",
    "    oop2n = np.zeros(oop2primen.shape)\n",
    "    oop1s = np.zeros(oop1primes.shape)\n",
    "    oop2s = np.zeros(oop2primes.shape)\n",
    "\n",
    "    for i in np.arange(oop1n.shape[0]):\n",
    "        oop1n[i] = np.dot(eigVec[i].T, oop1primen[i])\n",
    "        oop2n[i] = np.dot(eigVec[i].T, oop2primen[i])\n",
    "        oop1s[i] = np.dot(eigVec[i].T, oop1primes[i])\n",
    "        oop2s[i] = np.dot(eigVec[i].T, oop2primes[i])\n",
    "\n",
    "    upPrime = np.zeros( (eigVec.shape[0], 3) )\n",
    "    wall1 = np.zeros(oop1primen.shape[0], dtype = 'bool')\n",
    "    wall2 = np.zeros(oop2primen.shape[0], dtype = 'bool')\n",
    "    for i in np.arange(upPrime.shape[0]):\n",
    "        upPrime[i] = np.dot(eigVec[i], np.array([0, 0, 1]))\n",
    "        wall1[i] = np.dot(oop1primen[i], upPrime[i]) > 0\n",
    "        wall2[i] = np.dot(oop2primen[i], upPrime[i]) > 0    \n",
    "\n",
    "    oop1n[~wall1] *= -1   \n",
    "    oop1s[~wall1] *= -1\n",
    "    oop2n[~wall2] *= -1\n",
    "    oop2s[~wall2] *= -1\n",
    "\n",
    "    s1, d1 = normalToStrikeDip(oop1n)\n",
    "    s2, d2 = normalToStrikeDip(oop2n)\n",
    "\n",
    "    strikeVec1 = np.array([np.sin(s1), np.cos(s1), np.zeros(s1.shape)]).T\n",
    "    strikeVec2 = np.array([np.sin(s2), np.cos(s2), np.zeros(s2.shape)]).T\n",
    "\n",
    "    r1 = np.zeros(s1.shape)\n",
    "    r2 = np.zeros(s2.shape)\n",
    "\n",
    "    for i in np.arange(r1.shape[0]):\n",
    "        r1[i] = np.arccos(np.dot(strikeVec1[i], oop1s[i]))\n",
    "        r2[i] = np.arccos(np.dot(strikeVec2[i], oop2s[i]))\n",
    "\n",
    "    r1[oop1s[:,2]<0]*=-1\n",
    "    r2[oop2s[:,2]<0]*=-1\n",
    "    \n",
    "    return s1, d1, r1, s2, d2, r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-05T20:42:46.461464Z",
     "start_time": "2019-05-05T20:42:46.320365Z"
    }
   },
   "outputs": [],
   "source": [
    "# Make grid of points for visualizing coulomb stress\n",
    "\n",
    "xmin = 13\n",
    "xmax = 13.9\n",
    "ymin = 42.05\n",
    "ymax = 42.7\n",
    "aspect = (xmin-xmax)/(ymin-ymax)\n",
    "\n",
    "# 2d Mesh values. Can be made 3d by changing zz and nz\n",
    "nx=50;ny=50; nz=1\n",
    "\n",
    "# nx=int(nx * cutGrid); ny=int(ny * cutGrid)\n",
    "\n",
    "xx = np.linspace(xmin,xmax,nx)\n",
    "yy = np.linspace(ymin,ymax,ny)\n",
    "zz = -np.array([5000])\n",
    "xGrid, yGrid, zGrid = np.meshgrid(xx,yy,zz)\n",
    "totalPoints=nx*ny*nz\n",
    "lonGrid=xGrid.reshape(totalPoints)\n",
    "latGrid=yGrid.reshape(totalPoints)\n",
    "depthGrid=zGrid.reshape(totalPoints)\n",
    "\n",
    "stressDeviator = 2e6 * np.array([\n",
    "    [-1, 0, 0], \n",
    "    [0, 0, 0],\n",
    "    [0, 0, 1]\n",
    "    ])\n",
    "densityRock = 2700\n",
    "plunge = np.array([75.29, 13.89, 4.55]) * np.pi / 180 # Principal stress directions\n",
    "trend = np.array([160.77, -37.96, 53.1]) * np.pi / 180 # From Chiaraluce 2003? read from Nostro\n",
    "coefFrict = hom.coefFrict\n",
    "coefSkempt = hom.coefSkempt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-05T20:42:47.386552Z",
     "start_time": "2019-05-05T20:42:46.467725Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load hypocenter info. These are points to be strained, not source quakes. \n",
    "# Also create booleans to include/disclude points from analysis. \n",
    "magLimitTesting = None\n",
    "if magLimitTesting:\n",
    "    print('change that!')\n",
    "\n",
    "[latP, lonP, depthP, mLP, radP, ST1P, DIP1P, RK1P, ST2P, DIP2P, RK2P, ttP, slipP\n",
    "    ] = selectedINGV(magLimitTesting, False, reRun=False,path=variable_folder)\n",
    "\n",
    "s1P, s2P, d1P, d2P, r1P, r2P = sortRupture(ST1P, ST2P, DIP1P, DIP2P, RK1P, RK2P, \n",
    "                                                    130*np.pi/180)\n",
    "\n",
    "FA = ~ ((s1P== 0) * (s2P==0) * (d1P==0) * (d2P==0) * (r1P==0) * (r2P==0) ) # focal available\n",
    "\n",
    "mainshockInd = np.where(allPoints.mL==np.amax(allPoints.mL) )[0]\n",
    "aftershock = np.ones(allPoints.mL.size, dtype = bool)\n",
    "for i in range(aftershock.size):\n",
    "    if i <= mainshockInd:\n",
    "        aftershock[i] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-05T20:42:54.251645Z",
     "start_time": "2019-05-05T20:42:47.391358Z"
    }
   },
   "outputs": [],
   "source": [
    "# options. 0: run and save, 1: load and don't save, 2: run and don't save, 3: don't run but save, 4: nothing?\n",
    "rssurf  = 1\n",
    "rssurfTD= 1\n",
    "rsheter = 1\n",
    "rscir   = 1\n",
    "\n",
    "simulate = aftershock.copy()\n",
    "\n",
    "# Find stress tensors: (s: stress tensor, g or p: grid or point, cir or het or surf: model)\n",
    "# Below, I Load, run, save, or whatever was decided. \n",
    "\n",
    "#3d gps okada\n",
    "if (rssurf==0) or (rssurf==2):\n",
    "    spsurf = np.zeros( (latP.size, 3, 3) )\n",
    "    spsurf[simulate] = surfF.findStressTen(latP[simulate], lonP[simulate], depthP[simulate])\n",
    "    sgsurf = surfF.findStressTen(latGrid, lonGrid, depthGrid)\n",
    "    \n",
    "if (rssurf==0) or (rssurf==3):\n",
    "    saveArrays([spsurf, sgsurf],   'stressTenSurf',  variable_folder)\n",
    "    \n",
    "if (rssurf==1):\n",
    "    spsurf, sgsurf   =loadArrays('stressTenSurf',  variable_folder)\n",
    "\n",
    "#planar gps\n",
    "if (rsheter==0) or (rsheter==2):\n",
    "    spHeter = np.zeros( (latP.size, 3, 3) )\n",
    "    spHeter[simulate] = heterF.findStressTen(latP[simulate], lonP[simulate], depthP[simulate])\n",
    "    sgHeter = heterF.findStressTen(latGrid, lonGrid, depthGrid)\n",
    "\n",
    "if (rsheter==0) or (rsheter==3):\n",
    "    saveArrays([spHeter, sgHeter], 'stressTenHeter', variable_folder)\n",
    "    \n",
    "if (rsheter==1):\n",
    "    spHeter, sgHeter =loadArrays('stressTenHeter', variable_folder)\n",
    "\n",
    "#cirella\n",
    "if (rscir==0) or (rscir==2):\n",
    "    spcir = np.zeros( (latP.size, 3, 3) )\n",
    "    spcir[simulate] = cir.findStressTen(latP[simulate], lonP[simulate], depthP[simulate])\n",
    "    sgcir = cir.findStressTen(latGrid, lonGrid, depthGrid)\n",
    "\n",
    "if (rscir==0) or (rscir==3):\n",
    "    saveArrays([spcir, sgcir],     'stressTenCir',   variable_folder)\n",
    "    \n",
    "if (rscir==1):\n",
    "    spcir, sgcir     =loadArrays('stressTenCir',   variable_folder)\n",
    "\n",
    "#3d gps TD\n",
    "if (rssurfTD==0) or (rssurfTD==2):\n",
    "    spsurfTD = np.zeros( (latP.size, 3, 3) )\n",
    "    spsurfTD[simulate] = surfTDF.findStressTenTD(latP[simulate], lonP[simulate], depthP[simulate])\n",
    "    sgsurfTD = surfTDF.findStressTen(latGrid, lonGrid, depthGrid)\n",
    "    \n",
    "if (rssurfTD==0) or (rssurfTD==3):\n",
    "    saveArrays([spsurfTD, sgsurfTD],   'stressTenSurfTD',  variable_folder)\n",
    "    \n",
    "if (rssurfTD==1):\n",
    "    spsurfTD, sgsurfTD   =loadArrays('stressTenSurfTD',  variable_folder)\n",
    "\n",
    "    \n",
    "# Below, I CAN combine stress made from each mainshock model\n",
    "# with stress made from smaller earthquakes. \n",
    "# by default, this isn't done. \n",
    "# It should only be done to make Figure S3, or to play around. \n",
    "IncludeStressFromSmallQuakes = False # this is all that has to be changed to include small earthquakes as sources\n",
    "if IncludeStressFromSmallQuakes:\n",
    "    print('If this cell is ran more than once, we have to reload each models stress tensors!')\n",
    "\n",
    "    citarealeQuake = allPoints.mL > 3\n",
    "    citarealeQuake *= allPoints.time > np.datetime64('2009-06-24T00:00:00')\n",
    "    citarealeQuake *= allPoints.time < np.datetime64('2009-06-26T00:00:00')\n",
    "    print(allPoints.time[citarealeQuake], '\\n',\n",
    "    allPoints.mL[citarealeQuake])\n",
    "\n",
    "    minMagSource = 4 # saved with minMagSource = 4 on 04/04/2019\n",
    "    sources = np.where( # find indecies of events that are sources. Used in naming files.\n",
    "        np.logical_and(\n",
    "            np.logical_or(allPoints.mL>=minMagSource, \n",
    "                      citarealeQuake), # citraelle quake?\n",
    "        allPoints.mL!=np.amax(allPoints.mL)\n",
    "                        )\n",
    "        )[0]\n",
    "    smallDCSFolder = 'dcs_small_sources/'\n",
    "    plt.scatter(allPoints.x[sources], allPoints.y[sources])\n",
    "    plt.show()\n",
    "\n",
    "    print(np.sum(allHypo.mL>minMagSource), 'sources \\n')\n",
    "    print('slip of sources', allPoints.slip[sources])\n",
    "\n",
    "    # options. 0: run and save, 1: load and don't save, 2: run and don't save, 3: don't run but save\n",
    "    rssmallSource  = 1\n",
    "    ####### move this ^\n",
    "\n",
    "    stressTenPsmall = np.zeros( (sources.size, latP.size, 3, 3) )\n",
    "    stressTenGsmall = np.zeros( (sources.size, latGrid.size, 3, 3))\n",
    "\n",
    "    for index in sources:\n",
    "    # Make INGV instance of mainshock model\n",
    "        smallSource = mainModels('A smallish source quake') # replaced in each loop iteration\n",
    "        smallSource.mechanicalConstants()\n",
    "\n",
    "        # Load INGV inverted mainshock: IM\n",
    "        [smallSource.lat, smallSource.lon, smallSource.depth, smallSource.mL, \n",
    "             smallSource.rad, \n",
    "             s1, d1, r1, s2, d2, r2, \n",
    "             smallSource.tt, smallSource.slip\n",
    "            ] = [allPoints.lat [[index]], \n",
    "                 allPoints.lon [[index]], \n",
    "                 allPoints.z   [[index]], \n",
    "                 allPoints.mL  [[index]], \n",
    "                 allPoints.rad [[index]], \n",
    "                 allPoints.st1 [[index]],\n",
    "                 allPoints.dp1 [[index]],\n",
    "                 allPoints.rk1 [[index]],\n",
    "                 allPoints.st2 [[index]],\n",
    "                 allPoints.dp2 [[index]],\n",
    "                 allPoints.rk2 [[index]],\n",
    "                 allPoints.time[[index]], \n",
    "                 allPoints.slip[[index]]\n",
    "            ]\n",
    "\n",
    "        smallSource.s, __, smallSource.d, __, smallSource.r, __ = sortRupture(\n",
    "            s1, s2, d1, d2, r1, r2, 130*np.pi/180 )\n",
    "\n",
    "        smallSource.strikeLength = 2*smallSource.rad\n",
    "        smallSource.dipLength = 2*smallSource.rad\n",
    "\n",
    "        smallSource.ss = smallSource.slip * np.cos(smallSource.r)\n",
    "        smallSource.ds = smallSource.slip * np.sin(smallSource.r)\n",
    "\n",
    "        # 2.5 cells per 1000 m, but no less than 3 cells, no more than 9 cells\n",
    "        cellsLine = int(np.ceil(smallSource.rad * 2.5/1000) )\n",
    "        if cellsLine <3:\n",
    "            cellsLine = 3\n",
    "        if cellsLine >9:\n",
    "            cellsLine = 9\n",
    "\n",
    "        smallSource.strikeRects =  np.array([cellsLine])\n",
    "        smallSource.verticalRects = np.array([cellsLine])\n",
    "        smallSource.makeLists()\n",
    "\n",
    "        plt.scatter(smallSource.lonL, smallSource.latL, \n",
    "                    c = smallSource.dsL,\n",
    "                    vmin = -.5, vmax = 0)\n",
    "\n",
    "        simulate = ttP > smallSource.tt\n",
    "\n",
    "        if (rssmallSource==0) or (rssmallSource==2):\n",
    "            spsmallSource1 = np.zeros( (latP.size, 3, 3) )\n",
    "            spsmallSource1[simulate] = smallSource.findStressTen(latP[simulate], lonP[simulate], depthP[simulate])\n",
    "            sgsmallSource1 = smallSource.findStressTen(latGrid, lonGrid, depthGrid) \n",
    "\n",
    "            ni = np.where(sources==index)\n",
    "            stressTenPsmall[ni] = spsmallSource1\n",
    "            stressTenGsmall[ni] = sgsmallSource1\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    spsmallSource = np.zeros((latP.size, 3, 3))\n",
    "    sgsmallSource = np.zeros((latGrid.size, 3, 3))\n",
    "    for i in np.arange(stressTenPsmall.shape[0]):\n",
    "        spsmallSource[:] += stressTenPsmall[i]\n",
    "        sgsmallSource[:] += stressTenGsmall[i]\n",
    "\n",
    "\n",
    "\n",
    "    if (rssmallSource==0) or (rssmallSource==3):\n",
    "        saveArrays([spsmallSource, sgsmallSource], 'stressTensmallSource',   variable_folder)\n",
    "\n",
    "    if (rssmallSource==1):\n",
    "        spsmallSource, sgsmallSource =loadArrays('stressTensmallSource',   variable_folder)\n",
    "\n",
    "\n",
    "    \n",
    "    spsurfTD += spsmallSource\n",
    "    sgsurfTD += sgsmallSource\n",
    "    \n",
    "    spsurf += spsmallSource\n",
    "    sgsurf += sgsmallSource\n",
    "    \n",
    "    spHeter += spsmallSource\n",
    "    sgHeter += sgsmallSource\n",
    "    \n",
    "    spcir += spsmallSource\n",
    "    sgcir += sgsmallSource"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-05T20:43:58.379321Z",
     "start_time": "2019-05-05T20:42:54.255877Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get OOPs\n",
    "sOOPstdp, dOOPstdp, rOOPstdp, __, __, __ = findOOP(spsurfTD, stressDeviator, densityRock, depthP)\n",
    "sOOPstdg, dOOPstdg, rOOPstdg, __, __, __ = findOOP(sgsurfTD, stressDeviator, densityRock, depthGrid)\n",
    "\n",
    "sOOPsp, dOOPsp, rOOPsp, __, __, __ = findOOP(spsurf, stressDeviator, densityRock, depthP)\n",
    "sOOPsg, dOOPsg, rOOPsg, __, __, __ = findOOP(sgsurf, stressDeviator, densityRock, depthGrid)\n",
    "\n",
    "sOOPhp, dOOPhp, rOOPhp, __, __, __ = findOOP(spHeter , stressDeviator, densityRock, depthP)\n",
    "sOOPhg, dOOPhg, rOOPhg, __, __, __ = findOOP(sgHeter , stressDeviator, densityRock, depthGrid)\n",
    "\n",
    "sOOPcp, dOOPcp, rOOPcp, __, __, __ = findOOP(spcir , stressDeviator, densityRock, depthP)\n",
    "sOOPcg, dOOPcg, rOOPcg, __, __, __ = findOOP(sgcir , stressDeviator, densityRock, depthGrid)\n",
    "    \n",
    "\n",
    "# cs\n",
    "dCS1surfTD = coulombStress(spsurfTD[FA], \n",
    "                         coefFrict, coefSkempt, s=s1P[FA],d=d1P[FA],r=r1P[FA])[2]\n",
    "dCS2surfTD = coulombStress(spsurfTD[FA], \n",
    "                         coefFrict, coefSkempt, s=s2P[FA],d=d2P[FA],r=r2P[FA])[2]\n",
    "dCSMsurfTD = coulombStress(spsurfTD, coefFrict, coefSkempt, s=sOOPstdp,d=dOOPstdp,r=rOOPstdp)[2]\n",
    "dCSGsurfTD = coulombStress(sgsurfTD, coefFrict, coefSkempt, s=sOOPstdg,d=dOOPstdg,r=rOOPstdg)[2]\n",
    "\n",
    "\n",
    "dCS1surf = coulombStress(spsurf[FA], \n",
    "                         coefFrict, coefSkempt, s=s1P[FA],d=d1P[FA],r=r1P[FA])[2]\n",
    "dCS2surf = coulombStress(spsurf[FA], \n",
    "                         coefFrict, coefSkempt, s=s2P[FA],d=d2P[FA],r=r2P[FA])[2]\n",
    "dCSMsurf = coulombStress(spsurf, coefFrict, coefSkempt, s=sOOPsp,d=dOOPsp,r=rOOPsp)[2]\n",
    "dCSGsurf = coulombStress(sgsurf, coefFrict, coefSkempt, s=sOOPsg,d=dOOPsg,r=rOOPsg)[2]\n",
    "\n",
    "\n",
    "dCS1Heter  = coulombStress(spHeter[FA], \n",
    "                         coefFrict, coefSkempt, s=s1P[FA],d=d1P[FA],r=r1P[FA])[2]\n",
    "dCS2Heter  = coulombStress(spHeter[FA], \n",
    "                         coefFrict, coefSkempt, s=s2P[FA],d=d2P[FA],r=r2P[FA])[2]\n",
    "dCSMHeter  = coulombStress(spHeter , coefFrict, coefSkempt, s=sOOPhp,d=dOOPhp,r=rOOPhp)[2]\n",
    "dCSGHeter  = coulombStress(sgHeter , coefFrict, coefSkempt, s=sOOPhg,d=dOOPhg,r=rOOPhg)[2]\n",
    "\n",
    "\n",
    "dCS1cir  = coulombStress(spcir[FA], \n",
    "                         coefFrict, coefSkempt, s=s1P[FA],d=d1P[FA],r=r1P[FA])[2]\n",
    "dCS2cir  = coulombStress(spcir[FA], \n",
    "                         coefFrict, coefSkempt, s=s2P[FA],d=d2P[FA],r=r2P[FA])[2]\n",
    "dCSMcir  = coulombStress(spcir , coefFrict, coefSkempt, s=sOOPcp,d=dOOPcp,r=rOOPcp)[2]\n",
    "dCSGcir  = coulombStress(sgcir , coefFrict, coefSkempt, s=sOOPcg,d=dOOPcg,r=rOOPcg)[2]\n",
    "\n",
    "dCSGsurfTD  = dCSGsurfTD.reshape((nx, ny))\n",
    "dCSGsurf  = dCSGsurf.reshape((nx, ny))\n",
    "dCSGHeter = dCSGHeter.reshape((nx, ny))\n",
    "dCSGcir   = dCSGcir.reshape((nx, ny))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figure 9 shows the distribution of ∆CFF in map view projected on both OOPs and focal mechanism data which is modelled from our 2D and 3D GPS based mainshock models as well as the planar joint model (Cirella et al., 2013). Figure 10 shows the relationship between slip distributions, aftershocks locations, and ∆CFF. We observe that the distribution of ∆CFF is very complex near the mainshock and is very smooth at large distances from the mainshock. Far from the mainshock, ∆CFF varies minimally between mainshock models, while ∆CFF near the mainshock is strongly dependent on mainshock model parameters. This is also shown in Figure 11. When aftershocks that are part of the L’Aquila fault cluster are not counted, meaning that we are looking only at aftershocks distant from the mainshock, there is increased similarity between models regarding the percent of aftershocks with positive ∆CFF.\n",
    "\n",
    "When accounting for all aftershocks which had focal mechanisms, the jointly inverted planar mainshock model predicts that more events have positive ∆CFF than do our GPS inverted mainshock models (Figure 11). This is true regardless of whether we used the actual focal mechanism data or OOPs. Our 3D mainshock model predicts the least events to have positive ∆CFF on focal mechanism data, but our planar GPS inverted model finds the least events to have positive ∆CFF on OOPs. For each model, the percent of events with positive ∆CFF increases when we exclude events which were part of the L’Aquila fault cluster.\n",
    "\n",
    "Figure 12a shows ∆CFF results at different time intervals. This overlays a scatter plot of the magnitude of aftershocks through time. After the mainshock, the frequency of events as well as their magnitudes generally decreases through time until shortly after the MW 4.4 Campotosto earthquake in June which appears to initiate more aftershocks. ∆CFF results are fairly constant through time, but when weighted by $10^M$, the trend emerges that there is very positive ∆CFF immediately after the mainshock. ∆CFF then decays through time until August-September when the percent of events experiencing positive ∆CFF increases again. Figure 12b excludes events that are not part of the L’Aquila cluster. Temporal ∆CFF patterns are similar as in Figure 12a, but these patterns are not as unanimously demonstrated by the three mainshock models, and these patterns are less consistent. This is likely due to the difficulties in modelling near-field stresses.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Figure S2\n",
    "Figure S2. This shows the strike and dip of OOPs. Orientations resulting from our 3D surface using Okada cells and triangular dislocations are both shown and are very similar, suggesting that Okada cells are generally sufficient for modelling relatively simple 3D surfaces. For all models, the orientation of OOPs is dominated by regional stress at great distances from the mainshock. Close to the mainshock, orientations are influenced more by the mainshock."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-05T20:44:01.108345Z",
     "start_time": "2019-05-05T20:43:58.383473Z"
    }
   },
   "outputs": [],
   "source": [
    "# directions of OOPs at 5km depth\n",
    "yOOPFig, xOOPFig = ChangeAxis(np.amin(latGrid), np.amin(lonGrid), latGrid, lonGrid)\n",
    "xyvalid = (xOOPFig < 60e3) * (xOOPFig > 20e3) * (yOOPFig < 50e3) * (yOOPFig > 10e3)\n",
    "xOOPFig = xOOPFig[xyvalid]\n",
    "yOOPFig = yOOPFig[xyvalid]\n",
    "xOOPFig -= xOOPFig.min()\n",
    "yOOPFig -= yOOPFig.min()\n",
    "\n",
    "OOPList = [sOOPstdg, sOOPsg,sOOPhg,sOOPcg]\n",
    "OOPStresses = [dCSGsurfTD, dCSGsurf, dCSGHeter, dCSGcir]\n",
    "OOPLabelList = ['3D GPS TD', '3D GPS Okada', '2D GPS', '2D Joint']\n",
    "fig, ax = plt.subplots(4)\n",
    "fig.set_size_inches(7, 7*4)\n",
    "for i, sMod in enumerate(OOPList):\n",
    "\n",
    "    dx = np.sin(sMod)[xyvalid]\n",
    "    dy = np.cos(sMod)[xyvalid]\n",
    "#     ax.set_xlim(np.amin(xGrid), np.amax(xGrid))\n",
    "#     ax.set_ylim(np.amin(yGrid), np.amax(yGrid))\n",
    "#     ax.imshow(OOPStresses[i], zorder=2, origin = 'lower')\n",
    "    ax[i].quiver(\n",
    "        xOOPFig/1e3, yOOPFig/1e3,\n",
    "        dx, dy,\n",
    "        headwidth = 0, scale = 30, pivot = 'middle')\n",
    "    dxd = np.sin(sMod+np.pi/2)[xyvalid]\n",
    "    dyd = np.cos(sMod+np.pi/2)[xyvalid]\n",
    "    ax[i].quiver(\n",
    "        xOOPFig/1e3, yOOPFig/1e3,\n",
    "        dxd, dyd, pivot = 'tail',\n",
    "        headwidth = 0, scale = 50)\n",
    "    ax[i].set_title(OOPLabelList[i])\n",
    "    plt.xlabel('East (km)')\n",
    "    plt.ylabel('North (km)')\n",
    "    plt.grid()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "    # sOOPsg, dOOPsg, rOOPsg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Figure 9\n",
    "Figure 9. ∆CFF on aftershocks resulting from different mainshock models. On the left, all 51011 aftershocks were used assuming OOPs. On the right, we used 3415 actual focal mechanisms of aftershocks. We determined whether both, one, or neither plane experienced positive ∆CFF induced from the mainshock. The color of aftershocks is such that red means positive ∆CFF on both focal planes and blue means negative ∆CFF on both focal planes. Yellow means positive ∆CFF on one but not the other focal plane. For the stress image in the background of these figures, ∆CFF is calculated on a grid of OOPs at 5 km depth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-05T20:44:02.319768Z",
     "start_time": "2019-05-05T20:44:01.113017Z"
    }
   },
   "outputs": [],
   "source": [
    "# Make array that shows us which are aftershocks\n",
    "# Used for all CFF analysis\n",
    "# mainshockInd = np.where(allPoints.mL==np.amax(allPoints.mL) )[0]\n",
    "# aftershock = np.ones(allPoints.mL.size, dtype = bool)\n",
    "# for i in range(aftershock.size):\n",
    "#     if i <= mainshockInd:\n",
    "#         aftershock[i] = False\n",
    "aftershockFocal = np.logical_and(aftershock, FA)\n",
    "\n",
    "boo = db2.labels==0 # this IS part of main cluster. \n",
    "\n",
    "allPoints = hypoData()\n",
    "allPoints.minMag = None\n",
    "allPoints.getHypocenters()\n",
    "focalBool = (FA)[boo] # sub of focal available\n",
    "\n",
    "# Make boolan arrays which show which hypocenters to consider\n",
    "includeSurf = np.logical_and(\n",
    "    ~boo, aftershock)[FA] # NOT part of surface, is an aftershock\n",
    "includeAll = aftershock[FA] # subset with focal mechanisms and aftershock\n",
    "\n",
    "includeSurfFull = np.logical_and( # Full means matrix is size of all points, not just ones with focal mechanisms\n",
    "    ~boo, aftershock*FA ) \n",
    "includeAllFull = aftershock*FA \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-05T20:44:02.335492Z",
     "start_time": "2019-05-05T20:44:02.323833Z"
    }
   },
   "outputs": [],
   "source": [
    "partsurf = db2.labels==0 # part of surface\n",
    "aft = aftershock # aftershock\n",
    "foc = FA # focal available\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-05T20:44:30.532113Z",
     "start_time": "2019-05-05T20:44:02.345279Z"
    }
   },
   "outputs": [],
   "source": [
    "# Figure 9\n",
    "\n",
    "# Make individual scatter and image plots for figure below\n",
    "def dCS_sca_im(ax, high, low, lat, lon, dCS1, dCS2=None,\n",
    "                 dCSG = None, strainers=None, maxIm = .1, minIm = -.1,\n",
    "              aspect = 4/3, alpha = .9, errBarWidth = 10000):\n",
    "    if dCSG is not None:\n",
    "        im = ax.imshow(dCSG*1e-6, extent = (xmin, xmax, ymin, ymax), \n",
    "               origin = 'lower',interpolation = 'bilinear', zorder=0,\n",
    "               cmap = 'seismic', vmax = maxIm, vmin = minIm, \n",
    "                       aspect = aspect, \n",
    "                       alpha=alpha)\n",
    "        \n",
    "        barX = 1/7*(xmax-xmin)+xmin\n",
    "        barY = 1/10*(ymax-ymin)+ymin\n",
    "        __, lonNew = xyToLonLat(barY, barX, 10000, 0)\n",
    "        dxErrBar = (lonNew-barX)*.5 # Because err bar goes both directions\n",
    "        bar = ax.errorbar(barX, barY, xerr = dxErrBar,\n",
    "                         elinewidth=2, color = 'white', capsize=4)\n",
    "        ax.text(barX, barY+.5*dxErrBar, '10 km', color = 'white', \n",
    "                horizontalalignment = 'center')\n",
    "\n",
    "    if dCS2 is None:\n",
    "        dCS2 = dCS1.copy()\n",
    "    \n",
    "    color, legend_markers, labels = greenRed(dCS1, dCS2, high, low)\n",
    "    \n",
    "    scatterP = ax.scatter(lon, lat, c = color, s = 10, zorder=1,\n",
    "        linewidths=.5,edgecolors='black' )\n",
    "    \n",
    "    if strainers is not None:\n",
    "        scatterF = ax.scatter(lon[strainers], lat[strainers], c = color, s = 500, \n",
    "            marker = '*', zorder = 2)\n",
    "\n",
    "    return im, scatterP, bar\n",
    "\n",
    "\n",
    "minStress=-.1 # MPa, for background stress image\n",
    "maxStress=.1\n",
    "\n",
    "# mainshock plane, plane1+2, grid\n",
    "plots = 3\n",
    "\n",
    "# Put all stresses into one array for a for loop\n",
    "dCSCombine = np.array([\n",
    "    [dCSMcir, dCS1cir, dCS2cir, dCSGcir], \n",
    "    [dCSMHeter, dCS1Heter, dCS2Heter, dCSGHeter],\n",
    "    [dCSMsurfTD, dCS1surfTD, dCS2surfTD, dCSGsurfTD]\n",
    "#    ,[dCSMsurfFine, dCS1surfFine, dCS2surfFine, dCSGsurfFine]\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "# Silly way of also putting these booleans in an array\n",
    "inclusion = np.array([\n",
    "    aftershock[foc],\n",
    "    aftershock[foc],\n",
    "    aftershock[foc]\n",
    "#    ,aftershock[foc]\n",
    "])\n",
    "\n",
    "inBounds = ~((latP > ymax) + (latP < ymin) + (lonP > xmax) + (lonP<xmin)).astype('bool')\n",
    "inclusionFull = np.array([\n",
    "    aftershock * inBounds,\n",
    "    aftershock * inBounds,\n",
    "    aftershock * inBounds\n",
    "#    ,aftershock * inBounds\n",
    "])\n",
    "\n",
    "fig, axes = plt.subplots(nrows=dCSCombine.shape[0], ncols=2, figsize=(12, 24))\n",
    "matplotlib.rcParams.update({'font.size': 12})\n",
    "\n",
    "# These aren't relevant but are stuck as part of my code\n",
    "high = 0\n",
    "low = 0\n",
    "\n",
    "for i in np.arange(plots):\n",
    "    im, scatter1, bar1 = dCS_sca_im(axes[i,0], high, low, latP[inclusionFull[i]], lonP[inclusionFull[i]], \n",
    "               dCSCombine[i][0][inclusionFull[i]], dCSCombine[i][0][inclusionFull[i]], dCSCombine[i, 3],\n",
    "                             minIm = minStress, maxIm = maxStress, aspect = aspect)\n",
    "    \n",
    "    im, scatter2, bar2 = dCS_sca_im(axes[i,1], high, low, latP[FA][inclusion[i]], lonP[FA][inclusion[i]], \n",
    "               dCSCombine[i][1][inclusion[i]], dCSCombine[i][2][inclusion[i]], dCSCombine[i, 3],\n",
    "                             minIm = minStress, maxIm = maxStress, aspect = aspect)\n",
    "    \n",
    "# Label slip distributions\n",
    "axes[0,0].set_ylabel('Planar Joint')\n",
    "axes[1,0].set_ylabel('Planar GPS')\n",
    "axes[2,0].set_ylabel('3D GPS')\n",
    "\n",
    "for i in range(axes.shape[0]):\n",
    "    for j in range(axes.shape[1]):\n",
    "        axes[i, j].set_yticklabels(['{:.1f}$\\degree$'.format(x) for x in plt.gca().get_yticks()])\n",
    "        axes[i, j].set_xticklabels(['{:.1f}$\\degree$'.format(x) for x in plt.gca().get_xticks()])\n",
    "\n",
    "\n",
    "# Label planes\n",
    "axes[0,0].set_title('OOPs')\n",
    "axes[0,1].set_title('Actual Focal Mechanisms')\n",
    "plt.suptitle('$\\Delta$CFF From Models', y = .885)\n",
    "\n",
    "# Colorbar\n",
    "widthC = .35\n",
    "tallC = 0.01\n",
    "cbar_ax = fig.add_axes([.5 - .5 * widthC, \n",
    "                        .1 - .5 * tallC, \n",
    "                        widthC, tallC])\n",
    "cbar = fig.colorbar(im, cax=cbar_ax, orientation = 'horizontal')\n",
    "cbar.set_label('$\\Delta$CFF (MPa)')\n",
    "cticks = np.linspace(minStress, maxStress, 5)\n",
    "cbar.set_ticks(cticks)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Figure 10\n",
    "Figure 10. This shows the slip distribution of models compared to the location of aftershocks and their calculated ∆CFF. The slip distributions are the same as in Figure 8, while the color scheme for aftershocks is the same as in Figure 9. (a) Slip distribution jointly inverted by Cirella et al. (2012), (b) slip distribution on our inverted 2D fault plane (c) slip distribution inverted with our 3D fault.\f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-05T20:44:30.578778Z",
     "start_time": "2019-05-05T20:44:30.537827Z"
    }
   },
   "outputs": [],
   "source": [
    "def plotly_trisurf2(x, y, z, simplices, colormap=cm.RdBu, plot_edges=None, colorsSurf=None):\n",
    "    #x, y, z are lists of coordinates of the triangle vertices \n",
    "    #simplices are the simplices that define the triangularization;\n",
    "    #simplices  is a numpy array of shape (no_triangles, 3)\n",
    "    #insert here the  type check for input data\n",
    "\n",
    "    points3D=np.vstack((x,y,z)).T\n",
    "    tri_vertices=map(lambda index: points3D[index], simplices)# vertices of the surface triangles  \n",
    "    zmean=[np.mean(tri[:,2]) for tri in tri_vertices ]# mean values of z-coordinates of \n",
    "                                                      #triangle vertices\n",
    "    min_zmean=np.min(zmean)\n",
    "    max_zmean=np.max(zmean)\n",
    "    facecolor=[map_z2color(zz,  colormap, min_zmean, max_zmean) for  zz in zmean]\n",
    "    \n",
    "\n",
    "    I,J,K=tri_indices(simplices)\n",
    "    \n",
    "    slpmean = 1/3 * (colorsSurf[I]+colorsSurf[J]+colorsSurf[K])\n",
    "    min_slpmean = np.amin(slpmean)\n",
    "    max_slpmean = np.amax(slpmean)\n",
    "    facecolor=[map_z2color(slp,  colormap, min_slpmean, max_slpmean) for  slp in slpmean]\n",
    "\n",
    "    triangles=go.Mesh3d(x=x,\n",
    "                     y=y,\n",
    "                     z=z,\n",
    "                     facecolor=facecolor,\n",
    "                     opacity=.7,\n",
    "                     i=I,\n",
    "                     j=J,\n",
    "                     k=K\n",
    "                    )\n",
    "\n",
    "    if plot_edges is None:# the triangle sides are not plotted \n",
    "        return [triangles]\n",
    "    else:\n",
    "        #define the lists Xe, Ye, Ze, of x, y, resp z coordinates of edge end points for each triangle\n",
    "        #None separates data corresponding to two consecutive triangles\n",
    "        lists_coord=[[[T[k%3][c] for k in range(4)]+[ None]   for T in tri_vertices]  for c in range(3)]\n",
    "        Xe, Ye, Ze=[reduce(lambda x,y: x+y, lists_coord[k]) for k in range(3)]\n",
    "\n",
    "        #define the lines to be plotted\n",
    "        lines=go.Scatter3d(x=Xe,\n",
    "                        y=Ye,\n",
    "                        z=Ze,\n",
    "                        mode='lines',\n",
    "                        line=dict(color= 'rgb(50,50,50)', width=1.5)\n",
    "               )\n",
    "        return [triangles, lines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-05T20:44:30.792241Z",
     "start_time": "2019-05-05T20:44:30.584529Z"
    }
   },
   "outputs": [],
   "source": [
    "def stresses3D(xp, yp, zp, xs=None, ys=None, zs=None, colors=None, colorsSurf=None,\n",
    "              pointSize=.5, pLineWidthR=1/4, save = False, saveName = 'delete'):\n",
    "    plots = []\n",
    "    ###### scatter plot\n",
    "    pointSize=pointSize\n",
    "    scatterPlot = [go.Scatter3d(\n",
    "        x=xp, y=yp, z=zp,\n",
    "        mode='markers',\n",
    "        marker=dict(\n",
    "            size=pointSize,\n",
    "            color=colors, # change\n",
    "            line=dict(\n",
    "                    color='rgb(0,0,0)',\n",
    "                    width=pLineWidthR * pointSize\n",
    "                ),\n",
    "            opacity=1) )]\n",
    "    plots = plots + scatterPlot\n",
    "    ######    \n",
    "\n",
    "    if xs is not None:\n",
    "        ###### Triangulation surface\n",
    "        points2D=np.vstack([xs,ys]).T\n",
    "        tri=Delaunay(points2D)\n",
    "\n",
    "        surfacePlot=plotly_trisurf2(xs, ys, zs,\n",
    "                                   tri.simplices, colormap=cm.cubehelix, plot_edges=None,\n",
    "                                   colorsSurf=colorsSurf)\n",
    "#         print(surfacePlot)\n",
    "        plots = plots + surfacePlot\n",
    "        ######\n",
    "\n",
    "\n",
    "    ###############\n",
    "    ############################ \n",
    "    textFontDict = {'color':'red', 'size':18}\n",
    "    ### north arrow\n",
    "    xR = 10e3\n",
    "    xNA = np.amax(xR-3e3)\n",
    "    yNA = 28e3#np.amax(self.y)\n",
    "    zNA = -14e3#np.amin(self.z)\n",
    "    yBase = yNA - 5e3\n",
    "\n",
    "    northLine = go.Scatter3d(\n",
    "        x=[xNA, xNA], y=[yBase, yNA], z=[zNA, zNA],\n",
    "        text = np.array(['', 'N']),\n",
    "        mode='lines+text',#Change?\n",
    "        line  =dict(width = 5, color = '#7f7f7f'),\n",
    "        textposition = 'middle left',\n",
    "        textfont = textFontDict\n",
    "    )\n",
    "\n",
    "    northCone = dict(\n",
    "        type = 'cone',\n",
    "        colorscale = 'Greys',\n",
    "        showscale = False,\n",
    "        x = [xNA], y = [yNA], z = [zNA],\n",
    "        u = [0], v = [2e3], w = [0]\n",
    "        )\n",
    "    ###\n",
    "\n",
    "    ###  scale bar      \n",
    "    xN = np.array([xR, xR, xR])\n",
    "    yN = np.array([-10e3, -5e3, 0])+yNA\n",
    "    zN = np.array([zNA, zNA, zNA])\n",
    "    scaleBar = go.Scatter3d(\n",
    "        x=xN,\n",
    "        y=yN,\n",
    "        z=zN,\n",
    "        text = np.array(['', '10 km', '']),\n",
    "        mode='lines+text',\n",
    "        line  =dict(width = 15, color = '#7f7f7f'),\n",
    "        textposition='middle right',\n",
    "        textfont = textFontDict\n",
    "    )\n",
    "\n",
    "    upLine = go.Scatter3d(\n",
    "        x=[xNA, xNA], y=[yBase, yBase], z=[zNA, zNA+5e3],\n",
    "        text = np.array(['', 'UP']),\n",
    "        mode='lines+text',#Change?\n",
    "        line  =dict(width = 5, color = '#7f7f7f'),\n",
    "        textposition = 'middle left',\n",
    "        textfont = textFontDict\n",
    "    )\n",
    "\n",
    "    upCone = dict(\n",
    "        type = 'cone',\n",
    "        colorscale = 'Greys',\n",
    "        showscale = False,\n",
    "        x = [xNA], y = [yBase], z = [zNA+5e3],\n",
    "        u = [0], v = [0], w = [2e3]\n",
    "        )\n",
    "    ###\n",
    "    plots.append(scaleBar)\n",
    "    plots.append(northLine)\n",
    "    plots.append(northCone)\n",
    "    plots.append(upLine)\n",
    "    plots.append(upCone)\n",
    "    #########################\n",
    "    ###############\n",
    "\n",
    "    layout = go.Layout(\n",
    "        showlegend=False,\n",
    "        autosize=False,\n",
    "        width=1000,\n",
    "        height=1000,\n",
    "        scene=dict(\n",
    "            camera=dict(eye=dict(x=1.75, y=-0.7, z= 0.75) ),\n",
    "            xaxis = dict(showticklabels=False, title=''),\n",
    "            yaxis = dict(showticklabels=False, title=''),\n",
    "            zaxis = dict(showticklabels=False, title='')\n",
    "                  )  \n",
    "        )\n",
    "\n",
    "    fig2 = go.Figure(data=plots, layout=layout)\n",
    "    if save:\n",
    "        py.iplot(fig2, filename = saveName)\n",
    "    else:\n",
    "        plot(fig2)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-05T20:44:37.580745Z",
     "start_time": "2019-05-05T20:44:30.804452Z"
    }
   },
   "outputs": [],
   "source": [
    "model3Dplot = [cir, heterF, surfF, \n",
    "               cir, heterF, surfF]\n",
    "dCS3DFig = np.array([\n",
    "    [dCS1cir,     dCS2cir    ],\n",
    "    [dCS1Heter,   dCS2Heter  ],\n",
    "    [dCS1surfTD,  dCS2surfTD ],\n",
    "    [dCSMcir,     dCSMcir    ],\n",
    "    [dCSMHeter,   dCSMHeter  ],\n",
    "    [dCSMsurfTD,  dCSMsurfTD ]\n",
    "])\n",
    "inclAll = np.ones(FA.shape, dtype = 'bool')\n",
    "incl = np.array([\n",
    "    FA, FA, FA, inclAll, inclAll, inclAll\n",
    "])\n",
    "saveNames = ['Joint Focal',\n",
    "            '2D GPS Focal',\n",
    "            '3D GPS focal',\n",
    "            'Joint OOP',\n",
    "            '2D GPS OOP',\n",
    "            '3D GPS OOP']\n",
    "\n",
    "include3DPlot = [0, 0, 0, 0, 0, 0]\n",
    "pointsizes = [2, 2, 2, 1, 1, 1]\n",
    "if runAllFigures:\n",
    "    for i in range(3):\n",
    "        include3DPlot[i] = 1\n",
    "print('to see OOP relationship, change include3DPlot[3:] to 1')\n",
    "\n",
    "putOnline = False\n",
    "\n",
    "for i, mod in enumerate(model3Dplot):\n",
    "    if include3DPlot[i]:\n",
    "        y3D, x3D = ChangeAxis(allPoints.latCent, allPoints.lonCent, mod.latL[0], mod.lonL[0])\n",
    "\n",
    "        colors = np.zeros(np.sum(incl[i]), dtype = 'object')\n",
    "        stressFun1 = dCS3DFig[i][0]\n",
    "        stressFun2 = dCS3DFig[i][1]\n",
    "\n",
    "        both = (stressFun1>0)*(stressFun2>0)\n",
    "        justone    = np.logical_or((stressFun1>0),(stressFun2>0))\n",
    "        neither = ~ ( np.logical_or(justone, both) )\n",
    "\n",
    "        colors[justone]='yellow'\n",
    "        colors[both] = 'red'\n",
    "        colors[neither]='blue'\n",
    "\n",
    "        y3D, x3D = ChangeAxis(allPoints.latCent, allPoints.lonCent, mod.latL[0], mod.lonL[0])\n",
    "        slp = np.sqrt( mod.ssL[0]**2 + mod.dsL[0]**2 )\n",
    "\n",
    "        stresses3D(allPoints.x[incl[i]], allPoints.y[incl[i]], allPoints.z[incl[i]], \n",
    "                   x3D, y3D, mod.depthL[0],\n",
    "                   colors=colors,\n",
    "                   colorsSurf = slp,\n",
    "                  pointSize=pointsizes[i], saveName = saveNames[i], save=putOnline)\n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Figure 11\n",
    "Figure 11. ∆CFF resulting from different mainshock models. Red shows the percent of aftershocks which experienced +∆CFF on both focal mechanism planes, yellow for one and not the other focal mechanism plane, and blue for neither focal mechanism plane. The OOP results (left) include all 51011 aftershocks. The other results (center and right) use the real focal mechanism solutions on 3415 aftershocks. (a) Shows ∆CFF results on all aftershocks, regardless of their location. Comparing our GPS based planar and 3D models, the 3D surface finds more OOPs with +∆CFF, while the planar GPS based model find more actual focal mechanisms with +∆CFF. (b) Shows results when aftershocks that are part of the mainshock fault cluster are excluded. (c) Shows results when aftershocks which are not part of the mainshock fault cluster are excluded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-05T20:44:37.667898Z",
     "start_time": "2019-05-05T20:44:37.587532Z"
    }
   },
   "outputs": [],
   "source": [
    "def histograms(title, otherBool=None):\n",
    "    subplots = 3\n",
    "    fig, ax = plt.subplots(1, subplots)\n",
    "    fig.set_size_inches(11, 5)\n",
    "\n",
    "    labelsbar = ('Planar\\nJoint','Planar\\nGPS','3D\\nGPS'\n",
    "                 )\n",
    "    labelsaxes = ('OOPs', 'Actual Focal Mechanisms', 'Weighted $10^{M}$')\n",
    "\n",
    "    dCSHist = np.array([\n",
    "        [dCS1cir,   dCS2cir],\n",
    "        [dCS1Heter, dCS2Heter], \n",
    "#         [dCS1surf,  dCS2surf]\n",
    "        [dCS1surfTD, dCS2surfTD] # Checking fineness of surface\n",
    "    ])\n",
    "    dCSMHist = np.array([\n",
    "        [dCSMcir,   dCSMcir],\n",
    "        [dCSMHeter, dCSMHeter], \n",
    "#         [dCSMsurf,  dCSMsurf]\n",
    "        [dCSMsurfTD, dCSMsurfTD] # Checking fineness of surface\n",
    "    ])\n",
    "    dCSHistAll = [dCSMHist, dCSHist, dCSHist\n",
    "                 ,dCSMHist # Checking fineness of surface\n",
    "                 ]\n",
    "\n",
    "    if otherBool is None:\n",
    "        otherBool = np.ones(aftershock.size, dtype = 'bool')\n",
    "    \n",
    "    inclusionSub = aftershock * otherBool\n",
    "        \n",
    "    inclusion = np.array([\n",
    "        (inclusionSub)[foc],\n",
    "        (inclusionSub)[foc],\n",
    "#         (inclusionSub)[foc]\n",
    "        (inclusionSub)[foc] # Checking fineness of surface\n",
    "    ])\n",
    "    inclusionM = np.array([\n",
    "        (inclusionSub),\n",
    "        (inclusionSub),\n",
    "#         (inclusionSub)\n",
    "        (inclusionSub) # Checking fineness of surface\n",
    "    ])\n",
    "    inclusionAll = [inclusionM, inclusion, inclusion]\n",
    "\n",
    "    weights = [None, None, 10**allPoints.mL[foc] ]\n",
    "\n",
    "    ###\n",
    "    for axes in range(subplots):\n",
    "        bars = dCSHistAll[axes].shape[0]\n",
    "        p0 = np.zeros(bars); p1 = np.zeros(bars); p2 = np.zeros(bars)\n",
    "        ind = np.arange(bars)\n",
    "        width = 0.45\n",
    "\n",
    "        for i in np.arange(bars):\n",
    "            stress1 = dCSHistAll[axes][i][0]\n",
    "            stress2 = dCSHistAll[axes][i][1] \n",
    "            inc = inclusionAll[axes][i]\n",
    "            wet = weights[axes]\n",
    "            p0[i], p1[i], p2[i] = p012(stress1, stress2, inclusion = inc, weights = wet)\n",
    "\n",
    "            shiftUp = 1\n",
    "            horizAlign = 'center'\n",
    "            ax[axes].text(ind[i], p2[i]+shiftUp, '{:04.1f}'.format(p2[i]),\n",
    "                          horizontalalignment = horizAlign, color = 'black')\n",
    "            ax[axes].text(ind[i], p1[i]+p2[i]+shiftUp, '{:04.1f}'.format(p2[i]+p1[i]),\n",
    "                          horizontalalignment = horizAlign, color = 'black')\n",
    "\n",
    "        im2 = ax[axes].bar(ind, p2, width, color = 'red')\n",
    "        im1 = ax[axes].bar(ind, p1, width, bottom=p2, color = 'yellow')\n",
    "        im0 = ax[axes].bar(ind, p0, width, bottom = p2+p1, color='blue')\n",
    "        ax[axes].set_xticks(ind)\n",
    "        ax[axes].set_xticklabels(labelsbar)\n",
    "        ax[axes].set_xlabel(labelsaxes[axes], labelpad = 10)\n",
    "\n",
    "\n",
    "        ###\n",
    "\n",
    "    ax[0].set_ylabel('Percent psitive')\n",
    "    plt.tight_layout(pad = 1.1)\n",
    "    for i in range(subplots):\n",
    "        ax[i].set_yticklabels(['{:.0f}%'.format(x) for x in plt.gca().get_yticks()])\n",
    "\n",
    "\n",
    "\n",
    "    plt.suptitle(title, y = 1.05)\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-05T20:44:43.581051Z",
     "start_time": "2019-05-05T20:44:37.673155Z"
    }
   },
   "outputs": [],
   "source": [
    "matplotlib.rcParams.update({'font.size': 15})\n",
    "histograms('$\\Delta CFF$ from Models on Aftershocks')\n",
    "histograms('$\\Delta CFF$ only where Focal Mechanisms are Available', otherBool = FA)\n",
    "histograms('$\\Delta CFF$ in L\\'Aquila cluster', otherBool = partsurf)\n",
    "histograms('$\\Delta CFF$ not in L\\'Aquila cluster', otherBool = ~partsurf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-05T20:44:43.654267Z",
     "start_time": "2019-05-05T20:44:43.587708Z"
    }
   },
   "outputs": [],
   "source": [
    "def histograms2(title, otherBool=None):\n",
    "    subplots = 3\n",
    "    fig, ax = plt.subplots(1, subplots)\n",
    "    fig.set_size_inches(12, 5.5)\n",
    "\n",
    "    labelsbar = ('Okada','TD'\n",
    "                 )\n",
    "    labelsaxes = ('OOPs', 'Actual Focal Mechanisms', 'Weighted $10^{M}$')\n",
    "\n",
    "    dCSHist = np.array([\n",
    "#         [dCS1cir,   dCS2cir],\n",
    "#         [dCS1Heter, dCS2Heter], \n",
    "        [dCS1surf,  dCS2surf],\n",
    "        [dCS1surfTD, dCS2surfTD] # Checking fineness of surface\n",
    "    ])\n",
    "    dCSMHist = np.array([\n",
    "#         [dCSMcir,   dCSMcir],\n",
    "#         [dCSMHeter, dCSMHeter], \n",
    "        [dCSMsurf,  dCSMsurf],\n",
    "        [dCSMsurfTD, dCSMsurfTD] # Checking fineness of surface\n",
    "    ])\n",
    "    dCSHistAll = [dCSMHist, dCSHist, dCSHist\n",
    "                 ,dCSMHist # Checking fineness of surface\n",
    "                 ]\n",
    "\n",
    "    if otherBool is None:\n",
    "        otherBool = np.ones(aftershock.size, dtype = 'bool')\n",
    "    \n",
    "    inclusionSub = aftershock * otherBool\n",
    "        \n",
    "    inclusion = np.array([\n",
    "        (inclusionSub)[foc],\n",
    "#         (inclusionSub)[foc],\n",
    "#         (inclusionSub)[foc]\n",
    "        (inclusionSub)[foc] # Checking fineness of surface\n",
    "    ])\n",
    "    inclusionM = np.array([\n",
    "        (inclusionSub),\n",
    "#         (inclusionSub),\n",
    "#         (inclusionSub)\n",
    "        (inclusionSub) # Checking fineness of surface\n",
    "    ])\n",
    "    inclusionAll = [inclusionM, inclusion, inclusion]\n",
    "\n",
    "    weights = [None, None, 10**allPoints.mL[foc] ]\n",
    "\n",
    "    ###\n",
    "    for axes in range(subplots):\n",
    "        bars = dCSHistAll[axes].shape[0]\n",
    "        p0 = np.zeros(bars); p1 = np.zeros(bars); p2 = np.zeros(bars)\n",
    "        ind = np.arange(bars)\n",
    "        width = 0.45\n",
    "\n",
    "        for i in np.arange(bars):\n",
    "            stress1 = dCSHistAll[axes][i][0]\n",
    "            stress2 = dCSHistAll[axes][i][1] \n",
    "            inc = inclusionAll[axes][i]\n",
    "            wet = weights[axes]\n",
    "            p0[i], p1[i], p2[i] = p012(stress1, stress2, inclusion = inc, weights = wet)\n",
    "\n",
    "            shiftUp = 1\n",
    "            horizAlign = 'center'\n",
    "            ax[axes].text(ind[i], p2[i]+shiftUp, '{:04.1f}'.format(p2[i]),\n",
    "                          horizontalalignment = horizAlign, color = 'black')\n",
    "            ax[axes].text(ind[i], p1[i]+p2[i]+shiftUp, '{:04.1f}'.format(p2[i]+p1[i]),\n",
    "                          horizontalalignment = horizAlign, color = 'black')\n",
    "\n",
    "        im2 = ax[axes].bar(ind, p2, width, color = 'red')\n",
    "        im1 = ax[axes].bar(ind, p1, width, bottom=p2, color = 'yellow')\n",
    "        im0 = ax[axes].bar(ind, p0, width, bottom = p2+p1, color='blue')\n",
    "        ax[axes].set_xticks(ind)\n",
    "        ax[axes].set_xticklabels(labelsbar)\n",
    "        ax[axes].set_xlabel(labelsaxes[axes], labelpad = 10)\n",
    "\n",
    "\n",
    "        ###\n",
    "\n",
    "    ax[0].set_ylabel('Percent psitive')\n",
    "    plt.tight_layout(pad = 1.1)\n",
    "    for i in range(subplots):\n",
    "        ax[i].set_yticklabels(['{:.0f}%'.format(x) for x in plt.gca().get_yticks()])\n",
    "\n",
    "\n",
    "\n",
    "    plt.suptitle(title, y = 1.05)\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-05T20:44:44.991719Z",
     "start_time": "2019-05-05T20:44:43.660199Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"this plot is only in the Jupyter Notebook. \n",
    "It simply shows that using Okada vs Triangular Dislocations\n",
    "does not produce importantly different results\n",
    "for our 3D fault surface\"\"\"\n",
    "histograms2('$\\Delta CFF$ Okada vs. Triangular dislocation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Figure 12\n",
    "Figure 12. This shows how ∆CFF changes through time and how it relates to the magnitude and frequency of earthquakes. Black dots represent ML of all earthquakes. For (a), in order to determine the number of planes from actual focal mechanisms with positive ∆CFF, we used all 3415 events with available focal mechanisms. To determine whether OOPs has positive ∆CFF, all 51011 aftershocks were used. For (b), we used the subset of aftershocks which were part of the L’Aquila cluster. The horizontal axis is time. The number of events with positive ∆CFF is shown unweighted and weighted by $10^M$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-05T20:45:03.054791Z",
     "start_time": "2019-05-05T20:44:44.996801Z"
    }
   },
   "outputs": [],
   "source": [
    "matplotlib.rcParams.update({'font.size': 17})\n",
    "\n",
    "# Figure 12\n",
    "intervals = 9\n",
    "time = allPoints.time\n",
    "\n",
    "dcscomb2 = np.array([\n",
    "    [dCS1cir,     dCS2cir,    dCSMcir   ],  \n",
    "    [dCS1Heter,   dCS2Heter,  dCSMHeter ],\n",
    "    [dCS1surfTD,  dCS2surfTD, dCSMsurfTD]\n",
    "])\n",
    "\n",
    "linestyles = [':', '--', '-']\n",
    "\n",
    "wholePlotTitles = np.array(['Unweighted', 'Weighted $10^{M}$'])\n",
    "\n",
    "fig, axes = plt.subplots(nrows=2, ncols=1, figsize=(14, 14))\n",
    "matplotlib.rcParams.update({'font.size': 12})\n",
    "\n",
    "timeMin = time[allPoints.mL==np.amax(allPoints.mL)]\n",
    "timeMax = np.amax(time)\n",
    "dTime = (timeMax-timeMin)/intervals\n",
    "timeSlices = timeMin + np.arange(intervals+1) * dTime\n",
    "timeSlicesPlot = timeSlices[:-1] + .5 * dTime\n",
    "noFoc = np.datetime64('2009-09-01T00:00:00')\n",
    "startTime = np.datetime64('2009-03-15T00:00:00')\n",
    "toEarly = allPoints.time<startTime\n",
    "\n",
    "p0 = np.zeros(intervals)\n",
    "p1 = np.zeros(intervals)\n",
    "p2 = np.zeros(intervals)\n",
    "p0M= np.zeros(intervals)\n",
    "p1M= np.zeros(intervals)\n",
    "\n",
    "for w in [0, 1]:\n",
    "    for j in range(dcscomb2.shape[0]):\n",
    "        for i in np.arange(intervals):\n",
    "            timeBool = (time >= timeSlices[i]) * (time <= timeSlices[i+1])\n",
    "            timeBool = timeBool * aftershock# * exclmp\n",
    "\n",
    "            if w == 0:\n",
    "                weights = np.ones(timeBool.shape)\n",
    "            elif w == 1:\n",
    "                weights = 10**allPoints.mL\n",
    "            stress1 = dcscomb2[j][0]\n",
    "            stress2 = dcscomb2[j][1]\n",
    "            stressM = dcscomb2[j][2]\n",
    "            p0[i], p1[i], p2[i] = p012(stress1, stress2, (timeBool)[FA],\n",
    "                                       weights = weights[FA])\n",
    "            p0M[i], __, p1M[i] = p012(stressM, stressM, (timeBool),\n",
    "                                       weights = weights)\n",
    "        \n",
    "        shiftAmt = .045\n",
    "        num = np.sum(~toEarly)\n",
    "        shift = (np.random.random(num)-.5) * 2 * shiftAmt\n",
    "        axes[w].scatter(allPoints.time[~toEarly], allPoints.mL[~toEarly] +shift,#\n",
    "                    s = 3.5, linewidths=.02, edgecolors = 'k', c = 'k')\n",
    "        axes[w].scatter(timeMin, 0, s = 0)\n",
    "        axes[w].set_ylabel('$M_L$', fontsize = 16)\n",
    "\n",
    "        ax2 = axes[w].twinx()\n",
    "        incl = timeSlicesPlot<noFoc\n",
    "        ax2.plot(   timeSlicesPlot, p1M        \n",
    "                 , c = 'green'  , linestyle = linestyles[j], linewidth=5 ) \n",
    "        ax2.plot(   timeSlicesPlot[incl], (p1+p2)[incl] \n",
    "                 , c = 'yellow', linestyle = linestyles[j], linewidth=5 )\n",
    "        ax2.plot(   timeSlicesPlot[incl], (p2)[incl]\n",
    "                 , c = 'red'   , linestyle = linestyles[j], linewidth=5 )\n",
    "        ax2.set_ylim(0, 100)\n",
    "        plt.gca().set_yticklabels(['{:.0f}%'.format(x) for x in plt.gca().get_yticks()])\n",
    "        ax2.set_title(wholePlotTitles[w])\n",
    "        ax2.set_ylabel('Percent Positive', fontsize=16)\n",
    " \n",
    "\n",
    "legendFract = (timeSlices[0]-startTime)/3\n",
    "legendLeft = startTime - legendFract\n",
    "legendRight = legendLeft\n",
    "legendTime = np.array([legendLeft, legendRight])\n",
    "legendMag = np.zeros(2)\n",
    "legendbar = [axes[0].plot(legendTime, legendMag,\n",
    "    c = 'k', linestyle = linestyles[i], linewidth=5)[0] for i in range(dcscomb2.shape[0])]\n",
    "labels = np.array(['Planar Joint', 'Planar GPS', '3D GPS',\n",
    "                   '+$\\Delta$CFF on OOP', \n",
    "                   '+$\\Delta$CFF one plane', \n",
    "                   '+$\\Delta$CFF two planes'])\n",
    "colors = ['green', 'yellow', 'red']\n",
    "for i in range(3):\n",
    "    legendbar.append(\n",
    "    axes[0].plot(legendTime, legendMag,\n",
    "                c = colors[i], linewidth=5)[0]\n",
    "    )\n",
    "# for j in range(dcscomb2.shape[0]):\n",
    "#     legendbar.append()\n",
    "# axes[0].legend()\n",
    "axes[0].legend(legendbar, labels, loc = 7, fontsize=15)\n",
    "        \n",
    "plt.suptitle('$\\Delta$CFF Through Time', fontsize=20)\n",
    "fig.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "# axes[1].plot(0, 0, c='k', linestyle = linestyles[0], linewidth=5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-05T20:45:15.436909Z",
     "start_time": "2019-05-05T20:45:03.059748Z"
    }
   },
   "outputs": [],
   "source": [
    "#For only aftershocks in main cluster\n",
    "\n",
    "matplotlib.rcParams.update({'font.size': 17})\n",
    "\n",
    "intervals = 9\n",
    "time = allPoints.time # only where focal mechanisms available\n",
    "\n",
    "dcscomb2 = np.array([\n",
    "    [dCS1cir,     dCS2cir,    dCSMcir   ],  \n",
    "    [dCS1Heter,   dCS2Heter,  dCSMHeter ],\n",
    "    [dCS1surfTD,  dCS2surfTD, dCSMsurfTD]\n",
    "])\n",
    "\n",
    "linestyles = [':', '--', '-']\n",
    "\n",
    "wholePlotTitles = np.array(['Unweighted', 'Weighted $10^{M}$'])\n",
    "\n",
    "fig, axes = plt.subplots(nrows=2, ncols=1, figsize=(14, 14))\n",
    "matplotlib.rcParams.update({'font.size': 12})\n",
    "\n",
    "timeMin = time[allPoints.mL==np.amax(allPoints.mL)]\n",
    "timeMax = np.amax(time)\n",
    "dTime = (timeMax-timeMin)/intervals\n",
    "timeSlices = timeMin + np.arange(intervals+1) * dTime\n",
    "timeSlicesPlot = timeSlices[:-1] + .5 * dTime\n",
    "noFoc = np.datetime64('2009-09-01T00:00:00')\n",
    "startTime = np.datetime64('2009-03-15T00:00:00')\n",
    "toEarly = allPoints.time<startTime\n",
    "\n",
    "p0 = np.zeros(intervals)\n",
    "p1 = np.zeros(intervals)\n",
    "p2 = np.zeros(intervals)\n",
    "p0M= np.zeros(intervals)\n",
    "p1M= np.zeros(intervals)\n",
    "\n",
    "for w in [0, 1]:\n",
    "    for j in range(dcscomb2.shape[0]):\n",
    "        for i in np.arange(intervals):\n",
    "            timeBool = (time >= timeSlices[i]) * (time <= timeSlices[i+1])\n",
    "            timeBool = timeBool * aftershock\n",
    "\n",
    "            if w == 0:\n",
    "                weights = np.ones(timeBool.shape)\n",
    "            elif w == 1:\n",
    "                weights = 10**allPoints.mL\n",
    "            stress1 = dcscomb2[j][0]\n",
    "            stress2 = dcscomb2[j][1]\n",
    "            stressM = dcscomb2[j][2]\n",
    "            p0[i], p1[i], p2[i] = p012(stress1, stress2, (partsurf*timeBool)[FA],\n",
    "                                       weights = (weights)[FA])\n",
    "            p0M[i], __, p1M[i] = p012(stressM, stressM, partsurf*timeBool,\n",
    "                                       weights = weights)\n",
    "        shiftAmt = .045\n",
    "        num = np.sum((~toEarly)*partsurf)\n",
    "        shift = (np.random.random(num)-.5) * 2 * shiftAmt\n",
    "        axes[w].scatter(allPoints.time[(~toEarly)*partsurf],\n",
    "                        allPoints.mL[(~toEarly)*partsurf] +shift,#\n",
    "                    s = 3.5, linewidths=.02, edgecolors = 'k', c = 'k')\n",
    "        axes[w].scatter(timeMin, 0, s = 0)\n",
    "        axes[w].set_ylabel('$M_L$', fontsize = 16)\n",
    "\n",
    "        ax2 = axes[w].twinx()\n",
    "        incl = timeSlicesPlot<noFoc\n",
    "        ax2.plot(   timeSlicesPlot, p1M        \n",
    "                 , c = 'green'  , linestyle = linestyles[j], linewidth=5 ) \n",
    "        ax2.plot(   timeSlicesPlot[incl], (p1+p2)[incl] \n",
    "                 , c = 'yellow', linestyle = linestyles[j], linewidth=5 )\n",
    "        ax2.plot(   timeSlicesPlot[incl], (p2)[incl]\n",
    "                 , c = 'red'   , linestyle = linestyles[j], linewidth=5 )\n",
    "        ax2.set_ylim(0, 100)\n",
    "        plt.gca().set_yticklabels(['{:.0f}%'.format(x) for x in plt.gca().get_yticks()])\n",
    "        ax2.set_title(wholePlotTitles[w])\n",
    "        ax2.set_ylabel('Percent Positive', fontsize=16)\n",
    " \n",
    "\n",
    "legendFract = (timeSlices[0]-startTime)/3\n",
    "legendLeft = startTime - legendFract\n",
    "legendRight = legendLeft\n",
    "legendTime = np.array([legendLeft, legendRight])\n",
    "legendMag = np.zeros(2)\n",
    "legendbar = [axes[0].plot(legendTime, legendMag,\n",
    "    c = 'k', linestyle = linestyles[i], linewidth=5)[0] for i in range(dcscomb2.shape[0])]\n",
    "labels = np.array(['Planar Joint', 'Planar GPS', '3D GPS',\n",
    "                   '+$\\Delta$CFF on OOP', \n",
    "                   '+$\\Delta$CFF one plane', \n",
    "                   '+$\\Delta$CFF two planes'])\n",
    "colors = ['green', 'yellow', 'red']\n",
    "for i in range(3):\n",
    "    legendbar.append(\n",
    "    axes[0].plot(legendTime, legendMag,\n",
    "                c = colors[i], linewidth=5)[0]\n",
    "    )\n",
    "# for j in range(dcscomb2.shape[0]):\n",
    "#     legendbar.append()\n",
    "# axes[0].legend()\n",
    "axes[0].legend(legendbar, labels, loc = 7, fontsize=15)\n",
    "        \n",
    "plt.suptitle('$\\Delta$CFF on Aftershocks in L\\'Aquila Cluster', fontsize=20)\n",
    "fig.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "# axes[1].plot(0, 0, c='k', linestyle = linestyles[0], linewidth=5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Figure 13\n",
    "Figure 13. This compares ∆CFF to the hypocenters minimum distance from the mainshock. For each event, the plotted ∆CFF was chosen from the focal plane where ∆CFF was highest. Colors are the same as in Figure 9. Vertical lines are drawn at integer intervals of a fault cells size (the width of a triangle or Okada cell). Ideally, aftershocks should always be at least one cell size away from the fault.\f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-05T20:45:19.932971Z",
     "start_time": "2019-05-05T20:45:15.445620Z"
    }
   },
   "outputs": [],
   "source": [
    "matplotlib.rcParams.update({'font.size': 12})\n",
    "\n",
    "\n",
    "dcscomb1 = np.array([\n",
    "    [dCS1cir,   dCS2cir, dCSMcir],  \n",
    "    [dCS1Heter, dCS2Heter, dCSMHeter],\n",
    "#     [dCS1surf,  dCS2surf, dCSMsurf]\n",
    "    [dCS1surfTD,dCS2surfTD,dCSMsurfTD]\n",
    "])\n",
    "\n",
    "titles = ['Planar Joint', 'Planar GPS', '3D GPS']\n",
    "\n",
    "otherBool = partsurf\n",
    "otherBool = np.ones(partsurf.size, dtype = 'bool')\n",
    "includeHere =  (aftershock*otherBool)[FA]\n",
    "includeHereFull = (aftershock*otherBool)\n",
    "\n",
    "\n",
    "stressLim = 1\n",
    "\n",
    "yPoints, xPoints = ChangeAxis(0, 0, allHypo.lat[FA*aftershock*otherBool], \n",
    "                                    allHypo.lon[aftershock*FA*otherBool])\n",
    "zPoints = allHypo.z[aftershock*FA*otherBool]\n",
    "\n",
    "fig, ax = plt.subplots(3)\n",
    "fig.set_size_inches(14, 20)\n",
    "\n",
    "for i, mod in enumerate([cir, heterF, surfTDF]):\n",
    "#     plt.figure(figsize=(12,8))\n",
    "    stress1 = dcscomb1[i][0]\n",
    "    stress2 = dcscomb1[i][1]\n",
    "    stressM = dcscomb1[i][2]\n",
    "    \n",
    "    s1big = stress1>stress2\n",
    "    \n",
    "    stressGre = np.zeros(stress1.shape)\n",
    "    stressMin = np.zeros(stress1.shape)\n",
    "    \n",
    "    stressGre[s1big] = stress1[s1big] \n",
    "    stressGre[~s1big]= stress2[~s1big]\n",
    "    stressMin[~s1big]= stress1[~s1big]\n",
    "    stressMin[s1big] = stress2[s1big] \n",
    "    \n",
    "    stressMin *= 1e-6\n",
    "    stressGre *= 1e-6\n",
    "    \n",
    "    yFaults, xFaults = ChangeAxis(0, 0, mod.latL[0], mod.lonL[0])\n",
    "    zFaults = mod.depthL[0]\n",
    "    modHypDist = distances(xPoints, yPoints, zPoints, \n",
    "              xFaults, yFaults, zFaults)\n",
    "    distPlot = modHypDist.min(axis = 1)\n",
    "    \n",
    "    # Decide colors\n",
    "    pNum = (stressGre>0).astype('int') + (stressMin>0).astype('int')\n",
    "    colorsMagStr = np.zeros(pNum.shape, dtype = 'str')\n",
    "    colorsMagStr[pNum == 0] = 'blue'\n",
    "    colorsMagStr[pNum == 1] = 'yellow'\n",
    "    colorsMagStr[pNum == 2] = 'red'\n",
    "    \n",
    "    ax[i].scatter(distPlot, stressGre[includeHereFull[FA]], \n",
    "                c = colorsMagStr[includeHereFull[FA]],\n",
    "                s = 8, edgecolors = 'k', linewidths = .4)\n",
    "    ax[i].set_ylim((-stressLim, stressLim))\n",
    "    \n",
    "    cellLength = np.mean([mod.dipLengthL[0][0], mod.strikeLengthL[0][0] ])\n",
    "    \n",
    "    for j in range(5):\n",
    "        ax[i].plot([cellLength * j, cellLength * j],\n",
    "                 [-stressLim, stressLim], c = 'black', linewidth = 1)\n",
    "        \n",
    "    ax[i].plot([np.amin(distPlot), np.amax(distPlot)], [0, 0], c = 'black', linewidth = 1)\n",
    "    \n",
    "    ax[i].set_ylabel('Greater $\\Delta CFF$ (MPa)')    \n",
    "    ax[i].set_title(titles[i])\n",
    "    \n",
    "ax[i].set_xlabel('Shortest distance to mainshock (m)')\n",
    "\n",
    "        \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discussion and Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 Complexities of the Mainshock Surface\n",
    "We tested the quality of the interpolated 3D fault surface through several approaches. One approach was to find how well the interpolated fault surface matches the orientation of the earthquakes that it interpolates (Figure 6). There are several aspects of this analysis to consider. When comparing the match of aftershock orientations to the mainshock models, there does not appear to be an improvement moving from the planar model to our 3D surface. This could have implications regarding the accuracy of our 3D fault surface, but it is also important that the L’Aquila mainshock ruptured on a relatively planar fault segment. It may be that this fault is planar enough that a 2D model is sufficient. Yet, this analysis is limited by inaccuracies from the focal mechanism inversion as well as aftershock locations. Faults with more curvature, such as the Campotosto fault, will likely show more certain improvement by incorporating 3D morphology. \n",
    "\n",
    "While ideally the aftershock orientations should have matched the 3D fault surface model well, when comparing the mismatch with each aftershock weighted equally, the aftershocks only mildly agree with any of the fault models (Figure 6). However, by weighting this mismatch with the energy of aftershocks $\\left( \\sim 10^M \\right)$, the match between aftershocks and the 3D surface increases. This suggests that larger earthquakes are more closely aligned with the surface and/or the focal mechanisms of smaller events are less well identified. Oddly, a spatial pattern for agreement or disagreement between the focal mechanisms and fault surface does not appear to exist (Figure 6a), suggesting that disagreement is primarily from focal mechanism inversion error or from aftershocks truly rupturing at angles that are inconsistent with our inverted fault surface. \n",
    "\n",
    "The misalignment of small aftershocks with the main fault might be due to several reasons. One clear possibility is that the inversion of the orientation of low magnitude events might be less constrained than in the case of higher magnitude events. Another reason for such misalignment is that some aftershocks may be associated with conjugate faults instead of the main fault surface due to imperfect clustering (Figure 5b). There may be more unnoticed, smaller conjugate faults as well. These conjugate faults are responsible for part of the focal mechanism orientation misalignment as well as some positional mismatch between hypocenters and our 3D surface. Because these conjugate faults do not appear to constitute a major portion of the L’Aquila fault cluster, it appears that this is not a major problem. However, improved clustering should be done in the future to avoid this problem. \n",
    "\n",
    "However, this focal mechanism to fault model misalignment may have implications for the structure of the fault. Faults can become very sophisticated with damage zones at various scales and patterns (Peacock and Sanderson, 1991), and aftershock clusters tend to occur at the most complex regions of a fault surface where damage zones are the most extreme (Sibson 1989; Kim and Sanderson, 2008). Not only damage zones, but also the complexity of fault connectivity provides a means for aftershocks to rupture in directions and locations not in agreement with the main fault surface. One model for fault growth requires several smaller faults to link together, for instance through the breaching of relay ramps, in order to form larger faults (Childs et al., 2009). This results in complex rupture geometries. Relay ramps allow for a fault to be connected by many discontinuous faults at small and large scales. Relays have indeed been observed on the ground in the Paganica fault (Roberts et al., 2010). Faults can also be connected by extensional steps and contractional steps (Kim, peacock, and Sanderson, 2004). The tips of a fault can be particularly complex as well. Where fault displacement dies out at the tips of faults, displacement tends to end in splays (Kim, Peacock, and Sanderson, 2004). Splaying has been observed in the Paganica fault geologically (Cinti et al., 2011) and seismically (Chiaraluce et al., 2011). Given these issues, we are actually modelling a complex fault zone as a single 3D surface. Thus, we expect discontinuous sections of the fault zone to contain aftershocks which align poorly to the main fault surface. \n",
    "\n",
    "Given that we model the fault as simple and continuous 2D or 3D surfaces, we cannot model stresses induced by the fine scale fault zone characteristics discussed above. In general, these characteristics are probably too small to be detected. This poses a problem as such fault complexities can cause local stresses that greatly deviate from those modelled by smooth fault surfaces. Thus, using modern fault models to calculate ∆CFF, aftershocks associated stress perturbations originating from fault morphology discontinuities will appear erroneous. This, in part, is likely to explain why many events close to the mainshock are modelled to have negative ∆CFF while events far from the mainshock tend to have positive ∆CFF (Figures 9, 10, 11, and 13).\n",
    "\n",
    "## 6.2 Slip Distribution Inversion and GPS Fit\n",
    "\n",
    "Fit between modelled and observed co-seismic GPS displacements for our 2D and 3D GPS based mainshock models is very similar (Figure 7). Moving from the 2D to 3D model, horizontal displacement fit worsens while vertical fit improves (Table 2). The joint model finds the poorest fit to GPS data, though it should be much more realistic than at least our 2D GPS based model. This illustrates several problems. Our exclusion of a three-dimensional crustal model could be important. Also, GPS measurement errors can be very important and are often comparable in magnitude to measured displacement. This contributes to the important problem that slip distribution inversion results are non-unique. That is, for any model that reproduces the data decently, there are many different models that can also sufficiently reproduce the data. To help choose an appropriate model, complimentary data to the original dataset should be incorporated. The joint inversion included strong motion which is very complimentary to co-seismic geodetic data. Including strong motion and InSAR data within the joint inversion decreased the model’s ability to fit GPS data by ~15% (Cirella et al., 2012), yet ultimately resulted in a more realistic model. By using aftershock distribution to find fault morphology as we have done, information that is not present in co-seismic data is incorporated into the inversion. Incorporating other complementary data in our fault models would help to reduce the non-unique problem and extract realistic models which could further illuminate the roll of fault morphology in controlling stress and displacement.\n",
    "\n",
    "## 6.3 Coulomb Stress\n",
    "\n",
    "Generally, the joint inversion (Cirella et al. 2012) finds the highest number of events with positive ∆CFF. This suggests that incorporating the most realistic slip distribution by using complementary data sets is crucial. When moving from our GPS based planar inversion to our 3D inversion, we find a decrease in the number of actual focal mechanisms predicting positive ∆CFF (Figures 9 and 11). In part, this is likely a consequence of (i) the problem of obtaining the correct slip distribution from non-unique GPS based inversions, (ii) the assumed homogeneity of the crustal model, and (iii) lack of knowledge of the discontinuous 3D nature of the fault. The complex geometric relationship between aftershocks and a mainshock slip surface results in some error in the 3D rupture surface location and thus stress calculations. Future work could explore this effect. Although the planar GPS based mainshock model finds more focal mechanisms with +∆CFF, the 3D GPS based model finds more OOPs with +∆CFF. The OOP analysis has ~15 times the hypocenters and thus samples the spatial distribution of ∆CFF more thoroughly. In other words, the 3D model seems to better match the spatial distribution of aftershocks, while our planar model better matches the orientation of aftershocks. \n",
    "\n",
    "We note the correlation between slip distributions and aftershocks (Figure 10). As a general phenomenon, aftershocks are concentrated where slip distributions taper off or in portions of a fault that remain locked after an earthquake (Das and Henry, 2003). Stress tends to concentrate where slip distributions taper off, triggering aftershocks which relieve this stress. The high-resolution slip distribution of the joint inversion shows the strongest correlation between aftershock positions and slip distribution, though the more generalized and smooth slip distributions of the GPS based models still shows some correlation. Although aftershocks tend to concentrate where slip tapers off, ∆CFF results turn out negative most often in these regions particularly for our GPS based mainshock models. However, stress clearly must concentrate in these regions in order for so much seismicity to occur. This illustrates the unfortunate difficulty in modelling stresses most near the mainshock where aftershocks are most likely to occur.\n",
    "\n",
    "The relationship between timing of aftershocks, their magnitudes, and their mainshock induced ∆CFF is shown in Figure 12a. Immediately after the mainshock, only for energy weighted aftershocks one observes a peak of positive ∆CFF, as seen by the increase in proportion of positive ∆CFF events when $10^M$ is used as a weight. Predicted ∆CFF decreases through time approximately until the MW 4.4 Campotosto earthquake in June. This is consistent with the idea that ∆CFF has the most control on aftershocks that follow shortly after the mainshock, and the ∆CFF from the mainshock decays and becomes less relevant with time. Yet, the trend becomes less clear close to and after August when ∆CFF is generally high again. When we exclude events which were not part of the L’Aquila cluster (Figure 12b), the temporal pattern for ∆CFF becomes very complicated. This is another illustration that stress transfer modelling is less reliable over short distances. \n",
    "\n",
    "Mechanisms other than static stress transfer are important in controlling seismic sequences as well (e.g. Freed, 2005 and references therein). Some examples include dynamic stress from seismic waves, fluid migration and pore pressure changes, and temporal variation in rock properties. These other mechanisms account for some of the aftershocks which according to mainshock models experienced negative ∆CFF from the mainshock. In some sense, ∆CFF should explain aftershocks better when they do not occur in proximity to the mainshock surface. This would be expected by the consideration that a brittle event is expected to weaken a fault, so it is not necessary to expect an increase in ∆CFF to trigger an aftershock very nearby the mainshock. We tested this hypothesis by calculating the percentage of events with positive ∆CFF, excluding the aftershocks that were part of the main fault surface cluster (Figure 11b). With this assumption, every model predicts a greater number of events with positive ∆CFF. Other studies have suggested that the role of fluid movement in driving aftershocks is important (Miller et al., 2004), particularly for the L’Aquila seismic sequence (Di Luccio et al., 2010; Lucente et al., 2010; Malagnini et al., 2012). Our results are consistent with this hypothesis because the intrusion of high-pressure fluids could have induced rupture where the mainshock did not generate positive ∆CFF. \n",
    "\n",
    "However, we have observed that ∆CFF is more sensitive to complexities in fault models at locations near the rupture (Figure 9). If ∆CFF is very sensitive to fault model parameters at locations very near the fault, then ∆CFF modelling becomes more error prone near the fault. Not only is the data used for inversions too low of resolution to reliably model stresses very near the mainshock, but discontinuities in the Okada and triangular dislocation functions make them unreliable for nearby events. In general, the distance between a fault patch and the point it calculates stress or displacement on should be greater than the size of the fault patch. This presents a particularly challenging problem in regard to our 3D surface. Because it is built to fit the location of aftershocks, the aftershocks are necessarily very close to the surface. Figure 13 shows that many aftershocks are excessively close to each fault model, resulting in unreliable stress results. The aftershocks in the L’Aquila cluster are almost all within a cell length from the 3D fault surface. This is undoubtedly one reason that the 3D surface finds a small number of aftershocks with +∆CFF. Note that the cell sizes are already impractically small, and to make the cells small enough that most points lie more than one cell size away from the fault would take an extremely fine triangular mesh. \n",
    "\n",
    "Given also the errors in mainshock and aftershock location, rupture models, and the lack of a 3D elastic model, it is most practical to use ∆CFF modelling at great distances. This is problematic as other large earthquakes, particularly on the Campotosto fault, likely had important control on such distant aftershocks. The mainshock should have had the most control on earthquakes that occurred on the L’Aquila fault where modelling is not fine enough to produce reliable ∆CFF calculations. This illustrates the importance of improving stress transfer models.\n",
    "\n",
    "## 6.4 Limitations and Future Directions\n",
    "\n",
    "The relationship between hypocenters and faults is complex, so extrapolating fault configuration and morphology through clustering presents several challenges: (i) some faults, especially conjugate faults, intersect each other and thus some hypocenters can ambiguously belong to either fault, (ii) several seismic events are located outside any fault, either due to earthquakes occurring far from a primary fault plane or due to the aftershocks being erroneously located, and (iii) hypocenters that belong to the same fault may be separated by large areas where there were no aftershocks, for instance where the mainshock relieved much stress (Wetzler et al., 2018). \n",
    "\n",
    "To overcome these problems, clustering was done in multiple iterations with the first iteration finding general clusters and the second iteration finding clusters in an appropriately spatially modified axis system. However, this does not completely solve the clustering issues. For instance, the technique of stretching hypocenters away from the best fit plane of a cluster would work poorly on a listric fault where the fault surface is oriented much differently than the best fit plane. A new or modified algorithm could be better suited. For instance, the k-planes algorithm (Bradley and Mangasarian, 2000) works by finding clusters based on each point’s least square distance from planes. By incorporating a way for planes to bend or to connect to each other appropriately, it will be possible to create a more effective algorithm for fault clustering. The method used to generate the 3D surface from hypocenters can also be improved upon. We used a grid of 7x7 knots, but by applying some smoothing function (analogous to how our slip distribution was smoothed), a finer grid of knots could be used. This could reduce the problem of strange spline behavior in locations where hypocenter distribution was greatly heterogeneous or sparse. Particularly with improvements to the method, obtained fault morphologies can be used for many things besides ∆CFF modelling such as modelling fluid flow, seismic waves, or understanding slip on low angle normal faults (such as in the Amatrice sequence). \n",
    "\n",
    "The location of maximum slip does not necessarily coincide with the hypocenter for any earthquake. This can be seen for instance in each of the slip distributions we have used (Figure 8). It may not be most meaningful to find ∆CFF at the hypocenter of aftershocks as is done here as well as in most other studies (King, Stein, and Lin, 1994; Nostro et al., 2005, Freed, 2005 and references therein). Rather, ∆CFF could be calculated at several locations on the fault surface that an earthquake might occur on, similar to Walters et al. (2009). Possibly, the distribution of this ∆CFF can give a better estimate of the slip distribution of the subsequent series of earthquakes. Using fault surface imaging such as done here offers the option to find induced ∆CFF over an entire rupture area of a fault (which can be many km long).\n",
    "\n",
    "Some model-based limitations include the exclusion of topography, geological heterogeneities, fluid interaction, and the modification of rock properties as the sequence developed. Some of these limitations suggest considering the range of possible values of ∆CFF on each event. Whether aftershocks experienced positive or negative ∆CFF can be very sensitive to model parameters, so understanding the error margins of ∆CFF on aftershocks would help to better understand whether they truly experienced negative or positive ∆CFF. Statistical inversion of slip distributions can allow for propagation of slip error into calculated slip and stress (Yabuki and Matsu'ura, 1992; Cambiotti et al., 2017). Because stress can be represented as a linear function of slip on a fault patch, slip distribution error could be propagated to stress to give a probability that ∆CFF is positive for any event.\n",
    "\n",
    "Many of the error inducing assumptions of this model can be mitigated using more sophisticated numerical modelling (Trasatti, Kyriakopoulos, and Chini, 2011, inverted for the source model of the L’Aquila mainshock using a finite element model, but the mainshock was assumed to lie on a plane). Using numerical models, one could incorporate topography, 3D mechanical heterogeneities, and potentially synthesize temporally variable fluid presence with static stress modelling. Combining a numerical approach with co-seismic data other than GPS which are available for the L’Aquila sequence, such as InSAR and strong motion, within this more general framework on a curved fault surface would allow for a very complete rupture model. This could be used to gain further insight into the roles of various mechanisms involved in the earthquake source process. \n",
    "\n",
    "Future work might involve simulating the entire time dependent evolution of the faulting surface. Previous work has suggested that seismicity moved towards north through time. Therefore, by reconstructing the surface morphology with partial sets of hypocenters, for example every week, this might give visual and quantitative information on the propagation of fluids in the crust and within the fault or other important insights such as the evolution of the 3D stress field in time.\n",
    "\n",
    "A primary inspiration for this work is that when these methods are more fully developed, they can then be applied to more complicated earthquake sequences. In particular, the 2016/2017 Amatrice/Visso/Norcia sequence is an important target (Chiaraluce et al., 2017). The MW 6.5 event was preceded two months prior by a MW 6.0 event, and again only four days prior by a MW 5.9 event. Given the spatiotemporal correlation of these deadly events, they could provide a crucial view into earthquake triggering and hazard assessment. However, relocated seismicity has not provided a clear view of the associated faults, so extracting fault morphology in this sequence would facilitate a great deal of research. For instance, low angle normal fault seismicity is likely present in the aftershocks. Techniques such as the one developed here could reconstruct the low angle normal fault so that it can be studied in the context of triggering. This could greatly contribute to understanding how enigmatic low angle normal fault seismicity can occur. Further, the Amatrice sequence reactivated the Campotosto fault. This sequence and the L’Aquila sequence are thus intertwined, and it would be valuable to understand how the L’Aquila sequence contributed to the development of the Amatrice sequence.\f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "Please see main text."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
