{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-25T18:09:04.585281Z",
     "start_time": "2019-02-25T18:09:04.558604Z"
    }
   },
   "outputs": [],
   "source": [
    "def loadINGV(minMagnitude=None, onlyFocal = False, \n",
    "#              fname = 'LaqAll.final', # on 04/02/2019, adding 49 missing events 'Brennan_Aquila2009_51k_FocMec_CMT.dat.final',\n",
    "             fname = 'LaqAlldf.final', # on 04/03/2019. Added slip and radius calculated from \"creating_final_catalog\"\n",
    "             normalRet = True):\n",
    "    if minMagnitude is not None:\n",
    "        print(\"Not ready for minMagnitude in loadINGV (after getting new dataset)\")\n",
    "        raise NotImplementedError()\n",
    "    if onlyFocal != False:\n",
    "        print(\"Not ready for onlyFocal in loadINGV (after getting new dataset)\")\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    \n",
    "    # load data from file into panda dataframe. Extract data from there. \n",
    "    panDat = pd.read_csv(fname, delimiter= '\\t', skipinitialspace=True)\n",
    "    values = panDat.values.T\n",
    "    \n",
    "    tt        = values[0 ].astype('datetime64')\n",
    "    latitude  = values[1 ].astype('float64')\n",
    "    longitude = values[2 ].astype('float64')\n",
    "    depth     = values[3 ].astype('float64') * 1000 # this becomes negative only in selectedINGV function\n",
    "    ML        = values[4 ].astype('float64')                        #calling ml for now, not sure if mw\n",
    "    ID        = values[5 ].astype('int'    )\n",
    "    ST1       = values[6 ].astype('float64') * np.pi / 180\n",
    "    DIP1      = values[7 ].astype('float64') * np.pi / 180\n",
    "    RK1       = values[8 ].astype('float64') * np.pi / 180\n",
    "    ST2       = values[9 ].astype('float64') * np.pi / 180\n",
    "    DIP2      = values[10].astype('float64') * np.pi / 180\n",
    "    RK2       = values[11].astype('float64') * np.pi / 180\n",
    "    fty       = values[12].astype('str'    )\n",
    "    td        = values[13].astype('float64')\n",
    "    tla       = values[14].astype('float64')\n",
    "    tlo       = values[15].astype('float64')\n",
    "    radius    = values[16].astype('float64')\n",
    "    SLIP      = values[17].astype('float64')\n",
    "    \n",
    "    # strike of plane 1 is actually dip direction when fty=='MP'. This is FPFIT convention?\n",
    "    changeStrike = fty == 'MP'\n",
    "    ST1[changeStrike] += - np.pi / 2\n",
    "    ST2[changeStrike] += - np.pi / 2\n",
    "    \n",
    "#     can run the following test to show that if and only if\n",
    "#         all focal data == 0, there is no focal mechanism\n",
    "#     Foc1 = fty!='0'\n",
    "#     Foc2 = ~( (ST1==0) * (ST2==0) * (RK1==0) * (RK2==0) * (DIP1==0) * (DIP2==0) )\n",
    "#     print(np.sum(Foc1!=Foc2)) \n",
    "    \n",
    "    if normalRet:\n",
    "        return [latitude, longitude, depth, ML, radius, ST1, \n",
    "            DIP1, RK1, ST2, DIP2, RK2, tt, SLIP]\n",
    "    else:\n",
    "        return [latitude, longitude, depth, ML, radius, ST1, \n",
    "            DIP1, RK1, ST2, DIP2, RK2, tt, SLIP, fty]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-30T21:19:11.674264Z",
     "start_time": "2019-01-30T21:19:11.651147Z"
    }
   },
   "outputs": [],
   "source": [
    "# # This both loads the data from the INGV file,\n",
    "# # and it calculates slip from the provided radius and mL\n",
    "\n",
    "# def loadINGV(minMagnitude=0, onlyFocal = False): \n",
    "#     \"\"\"Returns: (latitude, longitude, depth,\n",
    "#     mL, radius, ST1, DIP1, RK1, ST2, DIP2, RK2, time, SLIP)\n",
    "    \n",
    "#     Provide maxMagnitude to load only earthquake with higher than that magnitude\n",
    "#     Only focal is boolean to indicate if earthquake with no focal mechanisms are desired\n",
    "    \n",
    "#     Angles are returned in radians\n",
    "#     slip and depth are in meters\n",
    "#     \"\"\"\n",
    "\n",
    "#     # We've had some problems with the address of the file. This should fix it.\n",
    "#     try:\n",
    "#         file=open('LAquila_2009_ALLinONE_unpub.out')\n",
    "#     except:\n",
    "#         url = '/work/Course'#= os.getcwd()\n",
    "#         file=open(url+'/LAquila_2009_ALLinONE_unpub.out')\n",
    "        \n",
    "#     maxLines = sum(1 for lines in file)\n",
    "#     file.seek(0)\n",
    "#     splitlines = []\n",
    "#     for i in range(maxLines):\n",
    "#         if i == 0:\n",
    "#             next(file)\n",
    "#         else:\n",
    "#             splitlines.append(file.readline().split())\n",
    "            \n",
    "#     dataSize=len(splitlines)\n",
    "#     year=np.zeros(dataSize,np.int)\n",
    "#     month=np.zeros(dataSize,np.int)    \n",
    "#     day=np.zeros(dataSize,np.int)  \n",
    "#     hours=np.zeros(dataSize,np.int)      \n",
    "#     minutes=np.zeros(dataSize,np.int)    \n",
    "#     seconds=np.zeros(dataSize,np.int)    \n",
    "#     latitude=np.zeros(dataSize,np.float)    \n",
    "#     longitude=np.zeros(dataSize,np.float)  \n",
    "#     depth=np.zeros(dataSize,np.float)      \n",
    "#     ML=np.zeros(dataSize,np.float)    \n",
    "#     err=np.zeros(dataSize,np.float)    \n",
    "#     radius=np.zeros(dataSize,np.float)    \n",
    "#     ID=np.zeros(dataSize,np.int)    \n",
    "#     ST1=np.zeros(dataSize,np.float)   \n",
    "#     DIP1=np.zeros(dataSize,np.float)   \n",
    "#     RK1=np.zeros(dataSize,np.float)   \n",
    "#     ST2=np.zeros(dataSize,np.float)     \n",
    "#     DIP2=np.zeros(dataSize,np.float) \n",
    "#     RK2=np.zeros(dataSize,np.float)  \n",
    "#     DS=np.zeros(dataSize,np.float)\n",
    "#     tt=np.zeros(dataSize,dtype='datetime64[s]')\n",
    "    \n",
    "#     for data in np.arange(dataSize):\n",
    "#         latitude[data]=float(splitlines[data][6])\n",
    "#         longitude[data]=float(splitlines[data][7])    \n",
    "#         depth[data]=float(splitlines[data][8]) * 1000 # meters\n",
    "#         ML[data]=float(splitlines[data][9])\n",
    "#         radius[data]=float(splitlines[data][11]) # meters\n",
    "#         ST1[data]=float(splitlines[data][13]) * np.pi / 180 \n",
    "#         DIP1[data]=float(splitlines[data][14]) * np.pi / 180 \n",
    "#         RK1[data]=float(splitlines[data][15]) * np.pi / 180 \n",
    "#         ST2[data]=float(splitlines[data][16]) * np.pi / 180 \n",
    "#         DIP2[data]=float(splitlines[data][17]) * np.pi / 180 \n",
    "#         RK2[data]=float(splitlines[data][18])  * np.pi / 180 \n",
    "#         year[data]=float(splitlines[data][0])\n",
    "#         month[data]=float(splitlines[data][1])\n",
    "#         day[data]=float(splitlines[data][2])\n",
    "#         hours[data]=float(splitlines[data][3])\n",
    "#         minutes[data]=float(splitlines[data][4])\n",
    "#         seconds[data]=float(splitlines[data][5])\n",
    "#         d = date(year[data], month[data], day[data])\n",
    "#         t = time(hours[data], minutes[data])\n",
    "#         dt = datetime.combine(d, t)\n",
    "#         tt[data] = dt\n",
    "\n",
    "#     #Here we calculate displacement based on magnitude and radius. \n",
    "#     #I think we calculated to go from a circular fault to a rectangular fault?\n",
    "#     Mo=10**(1.5*ML+16.1)/1e7\n",
    "#     SLIP = Mo/(2/3*3e10*radius**2*3.14) #slip is displacement in m\n",
    "#     #\n",
    "    \n",
    "#     #boolean array that selects only the quakes with a rupture mechanism\n",
    "#     if onlyFocal:\n",
    "#         selectQuakesST1 = ~( (ST1 == 0) * (ST2==0) * (DIP1==0) * (DIP2==0) * (RK1==0) * (RK2==0) )\n",
    "#     else:\n",
    "#         selectQuakesST1 = True\n",
    "        \n",
    "#     selectQuakesML = ML>minMagnitude#max magnitude is the highest magnitude we want to load\n",
    "#     selectQuakes = selectQuakesST1 * selectQuakesML # selectQuakes is boolean for events we want to load\n",
    "        \n",
    "#     return[ latitude[selectQuakes].copy(), longitude[selectQuakes].copy(), depth[selectQuakes].copy(),\n",
    "#             ML[selectQuakes].copy(), radius[selectQuakes].copy(), ST1[selectQuakes].copy(),\n",
    "#             DIP1[selectQuakes].copy(), RK1[selectQuakes].copy(), ST2[selectQuakes].copy(), DIP2[selectQuakes].copy(),\n",
    "#             RK2[selectQuakes].copy(), tt[selectQuakes].copy(), SLIP[selectQuakes].copy() ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-26T18:42:35.444579Z",
     "start_time": "2018-07-26T18:42:35.300583Z"
    }
   },
   "outputs": [],
   "source": [
    "# The point here is to load or save INGV data as .npy for quick data loading and manimuplating\n",
    "\n",
    "def selectedINGV(minMag = None, onlyFocal = False, reRun = False, path=None):\n",
    "    \"\"\"returns (latFault, lonFault, depthFault, mL, rad, ST1, DIP1, RK1, ST2, DIP2, RK2, tt, slip)\n",
    "    \n",
    "    minMag: minimum magnitude that is loaded\n",
    "    onlyFocal: Return only earthquakes with a known focal mechanism\n",
    "    \n",
    "    Will access the loadINGV function if .npy file is not already saved, and it will create the .npy file.\n",
    "    Otherwise, it simply loads the data from .npy file. This should save time.  \"\"\"\n",
    "\n",
    "    if not reRun:\n",
    "        try: \n",
    "            mostINGV = loadArrays('INGV_data', path)\n",
    "            time = loadSerialized('INGV_event_times', path=path, dtype='datetime64[s]')\n",
    "        except: \n",
    "           print(\"\"\"INGV_data and INGV_event_times were not found in specified path.\n",
    "               Loading data directly from LAquila_2009_ALLinONE_unpub. \"\"\")\n",
    "           reRun = True\n",
    "        \n",
    "    if reRun: \n",
    "        mostINGV = loadINGV()\n",
    "        time = mostINGV.pop(-2)\n",
    "        saveArrays(mostINGV, 'INGV_data', path)\n",
    "        saveSerialized(time, 'INGV_event_times', path)\n",
    "        \n",
    "    latFault, lonFault, depthFault, mL, rad, ST1, DIP1, RK1, ST2, DIP2, RK2, slip = mostINGV\n",
    "    depthFault = - depthFault\n",
    "    \n",
    "    if minMag is None:\n",
    "        selectBool = np.ones(latFault.size, dtype = bool)\n",
    "    else:\n",
    "        selectBool = mL >= minMag\n",
    "    if onlyFocal:\n",
    "        selectBool = selectBool * ~( (ST1==0) * (ST2==0) * (DIP1==0) * (DIP2==0) * (RK1==0) * (RK2==0) )\n",
    "        \n",
    "    return( latFault[selectBool], lonFault[selectBool], depthFault[selectBool],\n",
    "        mL[selectBool], rad[selectBool], ST1[selectBool], DIP1[selectBool],\n",
    "        RK1[selectBool], ST2[selectBool], DIP2[selectBool], RK2[selectBool],\n",
    "        time[selectBool], slip[selectBool] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadSecondSequence(minMag = None, onlyFocal = False,\n",
    "                       file = 'focmec-gmt-utm-medi.reloc.dat'):\n",
    "    table = pd.read_csv(file, delimiter = ' ', skipinitialspace=True)\n",
    "    latFault = table.LATITUDE.values\n",
    "    lonFault = table.LONGITUDE.values\n",
    "    depthFault = table.DEPTH.values * - 1000\n",
    "    mL =   table.ML.values  \n",
    "    rad =  np.zeros(latFault.size) * np.nan\n",
    "    ST1 =  table.STRIKE1.values * np.pi / 180\n",
    "    ST2 =  table.STRIKE2.values * np.pi / 180\n",
    "    DIP1 = table.DIP1.values    * np.pi / 180\n",
    "    DIP2 = table.DIP2.values    * np.pi / 180\n",
    "    RK1 =  table.RAKE1.values   * np.pi / 180\n",
    "    RK2 =  table.RAKE2.values   * np.pi / 180\n",
    "    slip = np.zeros(latFault.size) * np.nan\n",
    "    \n",
    "    year = table.YEAR.values\n",
    "    month = table.MONTH.values\n",
    "    day = table.DAY.values\n",
    "    hour = table.HOUR.values\n",
    "    minute = table.MINUTE.values\n",
    "    seconds = table.SECONDS.values\n",
    "    \n",
    "    datestr = []\n",
    "    for i in range(year.size):\n",
    "        if seconds[i] == 60:\n",
    "            seconds[i] = 0\n",
    "            minute[i] += 1\n",
    "        datestr.append('{:04.0f}-'.format(year[i]   ) +\n",
    "                       '{:02.0f}-'.format(month[i]  ) +\n",
    "                       '{:02.0f}T'.format(day[i]    ) +\n",
    "                       '{:02.0f}:'.format(hour[i]   ) +\n",
    "                       '{:02.0f}:'.format(minute[i] ) +\n",
    "                       '{:06.3f}' .format(seconds[i])\n",
    "                      )\n",
    "                        #aimed format'2009-01-12T20:53:38'\n",
    "            \n",
    "    time = np.array(datestr, dtype = 'datetime64')\n",
    "    \n",
    "    keep = np.ones(time.shape, dtype = 'bool')\n",
    "    \n",
    "    if onlyFocal:\n",
    "        FA = ~ ((ST1==0) * (ST2==0) * (DIP1==0) * (DIP2==0) * (RK1==0) * (RK2==0) )\n",
    "        keep = keep * FA\n",
    "        \n",
    "    if minMag is not None:\n",
    "        magPass = mL >= minMag\n",
    "        keep = keep * magPass\n",
    "\n",
    "    \n",
    "    return (latFault[keep], lonFault[keep], depthFault[keep], mL[keep], rad[keep], \n",
    "            ST1[keep], ST2[keep], DIP1[keep], DIP2[keep], RK1[keep], RK2[keep],\n",
    "            time[keep], slip[keep])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-14T20:27:09.741091Z",
     "start_time": "2018-09-14T20:27:09.727448Z"
    }
   },
   "outputs": [],
   "source": [
    "# Loads the GPS displacements and positions that were in Serpelloni's paper.\n",
    "\n",
    "def loadGPS():\n",
    "    \"\"\"return (lon, lat, dx, dy, du, ede, edn, edu) in SI units\n",
    "    \n",
    "    Make sure that GPSdata.csv is in the same folder as the jupyter notebook file\"\"\"\n",
    "    \n",
    "    # There has been inconsistency regarding both encoding and skiprows\n",
    "    # Some time should be devoted to fixing this in a solid way. \n",
    "    try:\n",
    "        atr = np.loadtxt(fname='GPSdata.csv',delimiter=',',skiprows=1,usecols=(1,2,3,4,7,8,10,11),unpack=True)#, encoding='latin-1') \n",
    "    except:\n",
    "        atr = np.loadtxt(fname='GPSdata.csv',delimiter=',',skiprows=0,usecols=(1,2,3,4,7,8,10,11),unpack=True)#, encoding='latin-1') \n",
    "#     except SecondException:\n",
    "#         atr = np.loadtxt(fname='GPSdata.csv',delimiter=',',skiprows=1,usecols=(1,2,3,4,7,8,10,11),unpack=True) \n",
    "#     except ThirdException:\n",
    "#         atr = np.loadtxt(fname='GPSdata.csv',delimiter=',',skiprows=0,usecols=(1,2,3,4,7,8,10,11),unpack=True) \n",
    "\n",
    "        \n",
    "#     atr=atr.reshape(8,77) \n",
    "#     atr = atr[:, (atr[7]!=0)] # remove station with unknown vertical properties\n",
    "        \n",
    "    lon = atr[0,]; lat = atr[1,]; dx = atr[2,]; dy = atr[3,]; du = atr[6,] \n",
    "    ede = atr[4,]; edn = atr[5,]; edu = atr[7,]\n",
    "    \n",
    "    # Error can't be zero. That is null data. I set the error arbitrarily high so that covariance inverse is not affected.\n",
    "    ede[ede == 0] = 1e6\n",
    "    edn[edn == 0] = 1e6\n",
    "    edu[edu == 0] = 1e6\n",
    "    \n",
    "        \n",
    "    #return displacements in meters\n",
    "    return lon, lat, dx/1000, dy/1000, du/1000, ede/1000, edn/1000, edu/1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPS data that I think was used in Cirella et al., from Chelloni.\n",
    "# I used this to make sure our data was similar. It is. \n",
    "def loadGPSChelloni(fileName):\n",
    "    with open(fileName) as file:\n",
    "        loaded = pd.read_csv(file, header = None)\n",
    "        \n",
    "    values = loaded.values.T\n",
    "    \n",
    "    lon = values[1].astype('float64')\n",
    "    lat = values[2].astype('float64') \n",
    "    dx = values[3].astype('float64') / 1000\n",
    "    edx = values[4].astype('float64') / 1000\n",
    "    dy = values[5].astype('float64') / 1000\n",
    "    edy = values[6].astype('float64') / 1000\n",
    "    dz = values[7].astype('float64') / 1000\n",
    "    edz = values[8].astype('float64') / 1000\n",
    "    \n",
    "    return (lon, lat, dx, dy, dz, edx, edy, edz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-13T21:40:34.267820Z",
     "start_time": "2018-09-13T21:40:34.245805Z"
    }
   },
   "outputs": [],
   "source": [
    "# def loadGPS():\n",
    "    \n",
    "#     data = pd.read_csv( open('GPSdata.csv', encoding='latin-1') ).values\n",
    "    \n",
    "#     boo = np.zeros( data[0,:].size, dtype=bool)\n",
    "    \n",
    "#     for ind in [1,2,3,4,7,8,10,11]:\n",
    "#         boo[ind] = True\n",
    "\n",
    "#     (lon, lat, dx, dy, du, ede, edn, edu) = data.T[boo, :]\n",
    "    \n",
    "#     dx=dx/1000\n",
    "#     dy=dy/1000\n",
    "#     du=du/1000\n",
    "#     ede=ede/1000\n",
    "#     edn=edn/1000\n",
    "#     edu=edu/1000\n",
    "    \n",
    "#     return lon, lat, dx, dy, du, ede, edn, edu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and save Generated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveArrays(arrays, fName, path=None):\n",
    "    \"\"\"Saves numpy arrays as json files. Pickle is not used.\n",
    "    \n",
    "    arrays should be a list or tupple containing all individual arrays. \n",
    "    path is a string placed before fName to choose folder.\"\"\"\n",
    "    \n",
    "    if path is not None:\n",
    "        fName = str(path)+str(fName)\n",
    "    elif path is None:\n",
    "        fName = str(fName)\n",
    "    \n",
    "    lists = [arr.tolist() for arr in arrays]\n",
    "    with open(fName, 'w') as outFile:\n",
    "        json.dump( lists, outFile )\n",
    "\n",
    "def loadArrays(fName, path=None):\n",
    "    \"\"\"Returns what was saved by saveArrays(). \n",
    "    \n",
    "    Returns arrays bundled in a list or tupple. \n",
    "    path is a string placed before fName to choose folder\"\"\"\n",
    "    \n",
    "    if path is not None:\n",
    "        fName = str(path)+str(fName)\n",
    "    elif path is None:\n",
    "        fName = str(fName)  \n",
    "    \n",
    "    with open(fName, 'r') as inFile:\n",
    "        lists = json.load( inFile )\n",
    "    listArrs = [np.array(arr) for arr in lists]\n",
    "    \n",
    "    return listArrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions used for storing arrays that are not json saveable, mainly dates.\n",
    "\n",
    "# Must first convert to string to storage, and later convert back to array.\n",
    "def toStrings(times):\n",
    "    \"\"\"Converts an array or list into a list of strings.\"\"\"\n",
    "    stringRep = []\n",
    "    for data in times:\n",
    "        stringRep.append(str(data))\n",
    "    return stringRep\n",
    "\n",
    "def stringsToArray(strings, dtype = None):\n",
    "    \"\"\"Converts a list of strings into an array.\"\"\"\n",
    "    Arr=np.zeros( len(strings), dtype=dtype)\n",
    "    for i in np.arange(len(strings)):\n",
    "        Arr[i] = strings[i]\n",
    "    return Arr \n",
    "\n",
    "# Functions for saving and loading array of datetime. \n",
    "def saveSerialized(array, fName, path=None):\n",
    "    \"\"\"Saves array or list of as serialized string.\"\"\"\n",
    "    if path is not None:\n",
    "        fName = str(path)+str(fName)\n",
    "    elif path is None:\n",
    "        fName = str(fName)\n",
    "        \n",
    "    strings = toStrings(array)\n",
    "    \n",
    "    with open(fName, 'w') as outFile:\n",
    "        json.dump( strings, outFile )\n",
    "        \n",
    "def loadSerialized(fName, path=None, retString=False, dtype = None):\n",
    "    \"\"\"Loads a string representation of list, returns as array or list.\"\"\"\n",
    "    if path is not None:\n",
    "        fName = str(path)+str(fName)\n",
    "    elif path is None:\n",
    "        fName = str(fName)\n",
    "    \n",
    "    with open(fName, 'r') as inFile:\n",
    "        strings = json.loads(inFile.read())\n",
    "\n",
    "    if retString:\n",
    "        return strings\n",
    "    else:\n",
    "        array = stringsToArray(strings, dtype=dtype)\n",
    "        return array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "341px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "475px",
    "left": "1071.46px",
    "right": "20px",
    "top": "133.977px",
    "width": "515px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
